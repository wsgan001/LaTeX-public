

%	options include 12pt or 11pt or 10pt
%	classes include article, report, book, letter, thesis

\title{}



\author{Brendan Whitaker}


\documentclass[12pt,oneside,reqno]{amsart}

%-------------------------------------
%--------PREAMBLE---------------------

%    Include referenced packages here.
\usepackage{}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bbm}
\usepackage{cancel}
\usepackage{verbatim}
\usepackage{amsrefs}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{tikz-cd}
%\usepackage[pdf]{pstricks}
\usepackage{braket}
\usetikzlibrary{cd}
\hypersetup{
     colorlinks   = true,
     citecolor    = red
}
%\usepackage{adjustbox}
\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{adjustbox}
\usepackage{changepage}
\usepackage{booktabs}
\usepackage{float}
\usepackage{listings}
\usepackage{framed}


\let\oldemptyset\emptyset
\let\emptyset\varnothing

\theoremstyle{plain}
\newtheorem{Thm}{Theorem}
\newtheorem{Prob}[Thm]{Problem}
%\theoremstyle{definition}
\newtheorem{Remark}[Thm]{Remark}
\newtheorem{Tech}[Thm]{Technical Remark}
\newtheorem*{Claim}{Claim}
%----------------------------------------
%CHAPTER STUFF

\newtheorem{theorem}{Theorem}%[chapter]
%\numberwithin{section}{chapter}
%\numberwithin{equation}{chapter}

%CHAPTER STUFF
%----------------------------------------
\newtheorem{lem}[theorem]{Lemma}
%\newtheorem{Q}[theorem]{Question}
\newtheorem{Prop}[theorem]{Proposition}
\newtheorem{Cor}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{e}{Exercise}
\newtheorem{Def}[theorem]{Definition}
\newtheorem{Ex}[theorem]{Example}
\newtheorem{xca}[theorem]{Exercise}



\theoremstyle{remark}
\newtheorem{rem}[theorem]{Remark}


\newcommand{\Mod}[1]{\ (\mathrm{mod}\ #1)}
\newcommand{\norm}{\trianglelefteq}
\newcommand{\propnorm}{\triangleleft}
\newcommand{\semi}{\rtimes}
\newcommand{\sub}{\subseteq}
\newcommand{\fa}{\forall}
\newcommand{\R}{\mathbb{R}}
\newcommand{\z}{\mathbb{Z}}
\newcommand{\n}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\renewcommand{\c}{\mathbb{C}}
\newcommand{\bb}{\vspace{3mm}}

\newcommand{\bee}{\begin{equation}\begin{aligned}}
\newcommand{\eee}{\end{aligned}\end{equation}}
\newcommand{\nequiv}{\not\equiv}
\newcommand{\lc}[2]{#1_1 + \cdots + #1_{#2}}
\newcommand{\lcc}[3]{#1_1 #2_1 + \cdots + #1_{#3} #2_{#3}}
\newcommand{\ten}{\otimes} %tensor product
\newcommand{\fracc}{\frac}
\newcommand{\tens}{\otimes}
\newcommand{\lpar}{\left(}
\newcommand{\rpar}{\right)}
\newcommand{\floor}{\lfloor}
\newcommand{\inlinecode}{\texttt}
\newcommand{\ra}[1]{\renewcommand{\arraystretch}{#1}}

\renewcommand{\rm}{\normalshape}%text inside math
\renewcommand{\Re}{\operatorname{Re}}%real part
\renewcommand{\Im}{\operatorname{Im}}%imaginary part
\renewcommand{\bar}{\overline}%bar (wide version often looks better)

\makeatletter
\newenvironment{restoretext}%
    {\@parboxrestore%
     \begin{adjustwidth}{}{\leftmargin}%
    }{\end{adjustwidth}
     }
\makeatother

%---------END-OF-PREAMBLE---------
%---------------------------------





\begin{document}



\title{CSE 5522 Homework 1}
\date{SP18}
\author[Brendan Whitaker]{Brendan Whitaker}
\maketitle
%\tableofcontents



\begin{enumerate}[label=\arabic*.]

\item \begin{enumerate}



\item We compute $P(x) = \int_\infty^\infty P(x,y) dy$. From the definition of the multivariate Gaussian distribution, we have:
$$
P(x,y) = \fracc{1}{(2\pi)|C|^{1/2}}e^{\fracc{-1}{2}(x - \mu)^TC^{-1}\left(
\lpar 
\begin{matrix}
x\\
y
\end{matrix} \rpar  - \mu\right)},
$$
where $C$ the covariance matrix is given by:
$$
C = \lpar \begin{matrix}
\sigma_1^2 & \rho \sigma_1 \sigma_2\\
\rho \sigma_1 \sigma_2 & \sigma_2^2
\end{matrix} \rpar.
$$
We compute:
$$
|C| = \det(C) = \sigma_1^2\sigma_2^2 - \rho^2\sigma_1^2\sigma_2^2 = \sigma_1^2\sigma_2^2(1 - \rho^2).
$$
And:
\bee
C^{-1} &= \fracc{1}{|C|}\lpar 
\begin{matrix}
\sigma_2^2 & -\rho \sigma_1 \sigma_2\\
-\rho \sigma_1 \sigma_2 & \sigma_1^2
\end{matrix} \rpar\\
&= \fracc{1}{\sigma_1^2\sigma_2^2(1 - \rho^2)}\lpar 
\begin{matrix}
\sigma_2^2 & -\rho \sigma_1 \sigma_2\\
-\rho \sigma_1 \sigma_2 & \sigma_1^2
\end{matrix} \rpar.
\eee
And:
$$
\left(
\lpar 
\begin{matrix}
x\\
y
\end{matrix} \rpar  - \mu\right) = \lpar 
\begin{matrix}
x - \mu_1\\
y - \mu_2
\end{matrix} \rpar.
$$
So we have:
\bee
&(x - \mu)^TC^{-1}\left(
\lpar 
\begin{matrix}
x\\
y
\end{matrix} \rpar  - \mu\right)\\
 = &(x - \mu_1,y - \mu_2)\fracc{1}{\sigma_1^2\sigma_2^2(1 - \rho^2)}\lpar 
\begin{matrix}
\sigma_2^2 & -\rho \sigma_1 \sigma_2\\
-\rho \sigma_1 \sigma_2 & \sigma_1^2
\end{matrix} \rpar\lpar 
\begin{matrix}
x - \mu_1\\
y - \mu_2
\end{matrix} \rpar\\
= &(x - \mu_1,y - \mu_2)\fracc{1}{\sigma_1^2\sigma_2^2(1 - \rho^2)}\lpar 
\begin{matrix}
\sigma_2^2(x - \mu_1) - \rho \sigma_1 \sigma_2(y - \mu_2)\\
-\rho \sigma_1 \sigma_2(x - \mu_1) + \sigma_1^2(y - \mu_2)
\end{matrix} \rpar\\
= &\fracc{1}{\sigma_1^2\sigma_2^2(1 - \rho^2)}\cdot
\sigma_2^2(x - \mu_1)^2 - \rho \sigma_1 \sigma_2(x - \mu_1)(y - \mu_2)\\
-&\fracc{1}{\sigma_1^2\sigma_2^2(1 - \rho^2)}\cdot\rho \sigma_1 \sigma_2(x - \mu_1)(y - \mu_2) + \sigma_1^2(y - \mu_2)^2\\
= &\fracc{(x - \mu_1)^2}{\sigma_1^2(1 - \rho^2)} - \fracc{2\rho(x - \mu_1)(y - \mu_2)}{\sigma_1\sigma_2(1 - \rho^2)} + \fracc{(y - \mu_2)^2}{\sigma_2^2(1 - \rho^2)}\\
= &\fracc{1}{1 - \rho^2}\left(\fracc{(x - \mu_1)^2}{\sigma_1^2} - \fracc{2\rho(x - \mu_1)(y - \mu_2)}{\sigma_1\sigma_2} + \fracc{(y - \mu_2)^2}{\sigma_2^2}\right). 
\eee
So all together:
\bee
P(x,y) &= \fracc{1}{2\pi\sigma_1\sigma_2\sqrt{1 - \rho^2}}e^{-\fracc{1}{2}Z}\\
&= Ae^{-\fracc{1}{2}Z}.
\eee
for $A =  \fracc{1}{2\pi\sigma_1\sigma_2\sqrt{1 - \rho^2}}$,
where:
\bee
Z &= \fracc{1}{1 - \rho^2}\left(\fracc{(x - \mu_1)^2}{\sigma_1^2} - 2\rho\fracc{(x - \mu_1)(y - \mu_2)}{\sigma_1\sigma_2} + \fracc{(y - \mu_2)^2}{\sigma_2^2}\right)\\
&= \fracc{1}{1 - \rho^2}\lpar\lpar 
\fracc{y - \mu_2}{\sigma_2} - \rho\fracc{x - \mu_1}{\sigma_1}
 \rpar^2 + (1 - \rho^2) \lpar \fracc{x - \mu_1}{\sigma_1}\rpar^2\rpar\\
 &= \fracc{1}{1 - \rho^2}\lpar 
\fracc{y - \mu_2}{\sigma_2} - \rho\fracc{x - \mu_1}{\sigma_1}
 \rpar^2 + \lpar \fracc{x - \mu_1}{\sigma_1}\rpar^2\\
 &= B +  \lpar \fracc{x - \mu_1}{\sigma_1}\rpar^2.
\eee
So we have: 
\bee 
B &= \fracc{1}{1 - \rho^2}\lpar 
\fracc{y - \mu_2}{\sigma_2} - \rho\fracc{x - \mu_1}{\sigma_1}
 \rpar^2\\
 &= \fracc{1}{1 - \rho^2}\lpar 
 \fracc{(y - \mu_2)^2}{\sigma_2^2} - 2\rho\fracc{(y - \mu_2)(x - \mu_1)}{\sigma_2\sigma_1} + \rho^2\fracc{(x - \mu_1)^2}{\sigma_1^2} \rpar \\
 &= \fracc{1}{1 - \rho^2}\lpar 
\fracc{y^2}{\sigma_2^2} - \fracc{2y\mu_2}{\sigma_2^2} - \fracc{2y\rho(x - \mu_1)}{\sigma_1\sigma_2} + \fracc{\mu_2^2}{\sigma_2^2} + \fracc{2\mu_2\rho(x - \mu_1)}{\sigma_1\sigma_2} + \fracc{\rho^2(x - \mu_1)^2}{\sigma_1^2}
 \rpar\\
 &= \fracc{y^2 - 2y\mu_2 -2y\rho\fracc{\sigma_2}{\sigma_1}(x - \mu_1) + \mu_2^2 + 2\mu_2\rho\fracc{\sigma_2}{\sigma_1}(x - \mu_1) +  \rho^2\fracc{\sigma_2^2}{\sigma_1^2}(x - \mu_1)^2}{(1 - \rho^2)\sigma_2^2}\\
 &= \fracc{y^2 - 2y\mu_2 -2y\rho\fracc{\sigma_2}{\sigma_1}(x - \mu_1) + (\mu_2 + \rho\fracc{\sigma_2}{\sigma_1}(x - \mu_1))^2}{(1 - \rho^2)\sigma_2^2}\\
 &=\fracc{\lpar y - (\mu_2 + \rho\fracc{\sigma_2}{\sigma_1}(x - \mu_1)) \rpar^2 }{(1 - \rho^2)\sigma_2^2}\\
  &=\fracc{\lpar y - g(x) \rpar^2 }{(1 - \rho^2)\sigma_2^2},
 \eee
 where $g(x) = (\mu_2 + \rho\fracc{\sigma_2}{\sigma_1}(x - \mu_1))$. So we have:
\bee
\int_{-\infty}^\infty P(x,y)dy &= Ae^{-
\fracc{1}{2} \fracc{(x - \mu_1)^2}{\sigma_1^2}} \cdot \int_{-\infty}^\infty e^{-\fracc{1}{2}B}\\
&= Ae^{-
\fracc{1}{2} \fracc{(x - \mu_1)^2}{\sigma_1^2}} \cdot \int_{-\infty}^\infty e^{-\fracc{1}{2}\fracc{\lpar y - g(x) \rpar^2 }{(1 - \rho^2)\sigma_2^2}},
\eee
So we let $\sigma' = \sqrt{1 - \rho^2}\sigma_2$. Thus:
\bee
\int_{-\infty}^\infty P(x,y)dy &= Ae^{-
\fracc{1}{2} \fracc{(x - \mu_1)^2}{\sigma_1^2}} \cdot \int_{-\infty}^\infty e^{-\fracc{1}{2}B}\\
&= Ae^{-
\fracc{1}{2} \fracc{(x - \mu_1)^2}{\sigma_1^2}} \cdot \sqrt{2\pi}\sigma' \cdot \int_{-\infty}^\infty \fracc{1}{\sqrt{2\pi}\sigma'} e^{-\fracc{1}{2}\fracc{\lpar y - g(x) \rpar^2 }{\sigma'^2}}.
\eee
But this integrand on the right is exactly a univariate Gaussian distribution in $y$ with mean $g(x)$ and variance $\sigma'^2$. So it integrates to $1$, and we have:
\bee
\int_{-\infty}^\infty P(x,y)dy &= Ae^{-
\fracc{1}{2} \fracc{(x - \mu_1)^2}{\sigma_1^2}} \cdot \int_{-\infty}^\infty e^{-\fracc{1}{2}B}\\
&= Ae^{-
\fracc{1}{2} \fracc{(x - \mu_1)^2}{\sigma_1^2}} \cdot \sqrt{2\pi}\sigma'\\
&= Ae^{-
\fracc{1}{2} \fracc{(x - \mu_1)^2}{\sigma_1^2}} \cdot \sqrt{2\pi}\sqrt{1 - \rho^2}\sigma_2\\
&= \fracc{1}{2\pi\sigma_1\sigma_2\sqrt{1 - \rho^2}}e^{-
\fracc{1}{2} \fracc{(x - \mu_1)^2}{\sigma_1^2}} \cdot \sqrt{2\pi}\sqrt{1 - \rho^2}\sigma_2\\
&= \fracc{1}{\sqrt{2\pi}\sigma_1}e^{-
\fracc{1}{2} \fracc{(x - \mu_1)^2}{\sigma_1^2}}.
\eee
Note from the definition of univariate Gaussian distribution that 
$$
P(x) =\int_{-\infty}^\infty P(x,y)dy
$$
is itself Gaussian with mean $\mu_1$ and variance $\sigma_1^2$. 
\bb

\item The mean is $\mu_1$ and the variance is $\sigma_1^2$. 
\bb

\item \textit{Assume $\rho = 0$. Prove that $P(x,y) = P(x)P(y)$. }

\begin{proof}
Note by the same excruciating derivation from part (a) we know:
$$
P(y) = \fracc{1}{\sqrt{2\pi}\sigma_2}e^{-
\fracc{1}{2} \fracc{(y - \mu_2)^2}{\sigma_2^2}}.
$$
So recall:
\bee
P(x,y) &= \fracc{1}{2\pi\sigma_1\sigma_2\sqrt{1 - \rho^2}}e^{-\fracc{1}{2}Z}.
\eee
And also recall:
\bee
Z = \fracc{1}{1 - \rho^2}\lpar 
\fracc{y - \mu_2}{\sigma_2} - \rho\fracc{x - \mu_1}{\sigma_1}
 \rpar^2 + \lpar \fracc{x - \mu_1}{\sigma_1}\rpar^2\\
 \eee
 Now since $\rho = 0$ we have:
 \bee
Z = \lpar 
\fracc{y - \mu_2}{\sigma_2} \rpar^2 + \lpar \fracc{x - \mu_1}{\sigma_1}\rpar^2\\
 \eee
 And so:
 \bee
P(x,y) &= \fracc{1}{2\pi\sigma_1\sigma_2\sqrt{1 - \rho^2}}e^{-\fracc{1}{2}\lpar  \lpar 
\fracc{y - \mu_2}{\sigma_2} \rpar^2 + \lpar \fracc{x - \mu_1}{\sigma_1}\rpar^2 \rpar }\\
&= \fracc{1}{2\pi\sigma_1\sigma_2}e^{-\fracc{1}{2}\lpar  \lpar 
\fracc{y - \mu_2}{\sigma_2} \rpar^2 + \lpar \fracc{x - \mu_1}{\sigma_1}\rpar^2 \rpar },
\eee
again since $\rho = 0$. But:
\bee
P(x,y) &= \fracc{1}{2\pi\sigma_1\sigma_2}e^{-\fracc{1}{2}\lpar  \lpar 
\fracc{y - \mu_2}{\sigma_2} \rpar^2 + \lpar \fracc{x - \mu_1}{\sigma_1}\rpar^2 \rpar }\\
&= \fracc{1}{\sqrt{2\pi}\sigma_1}\fracc{1}{\sqrt{2\pi}\sigma_2}e^{-
\fracc{1}{2} \fracc{(x - \mu_1)^2}{\sigma_1^2}}e^{-
\fracc{1}{2} \fracc{(y - \mu_2)^2}{\sigma_2^2}}\\
&= P(x)\cdot P(y).
\eee
\end{proof}
\end{enumerate}

\item \begin{enumerate}
\item 
\textit{Prove that if $A,B,C$ are mutually independent, then $A,B$ are conditionally independent given $C$. }

\begin{rem}
These are really easy, know how to do these if you got them wrong. 
\end{rem}

\begin{proof}
Let $A,B,C$ be mutually independent. Then we know by definition:
\bee
P(A,B,C) &= P(A)P(B)P(C)\\
P(A,B) &= P(A)P(B)\\
P(B,C) &= P(B)P(C)\\
P(A,C) &= P(A)P(C)
\eee
Recall from the definition of conditional independence that $A$ and $B$ are conditionally independent given $C$ if and only if we have:
$$
P(A|B,C) = P(A|C).
$$
So using product rule, we write:
$$
P(A|B,C) = \fracc{P(A,B,C)}{P(B,C)} = \fracc{P(A)P(B)P(C)}{P(B)P(C)} = P(A).
$$
But note:
$$
P(A|C) = \fracc{P(A,C)}{P(C)} = \fracc{P(A)P(C)}{P(C)} = P(A). 
$$
So we have the desired equality: $A,B$ are conditionally independent given $C$ by definition. 
\end{proof}

\item We give a  simple example JPT to illustrate part (a). 

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[H]
  \centering
  \caption{Joint probability table.}
    \begin{tabular}{r|r|r|r|r}
    \toprule
   & A & & $\neg$ A\\
   \hline
   & B & $\neg$ B & B & $\neg$ B\\
   \midrule
    C     & 0.125   & 0.125   & 0.125  & 0.125 \\
    $\neg$C    & 0.125   & 0.125  & 0.125  & 0.125 \\
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%

Note we have $P(A) = P(B) = P(C) = 0.5$. Note also 
\bee
&P(A,B) = P(B,C) = P(A,C) = 0.25 \\
= &P(A)P(B) = P(A)P(C) = P(B)P(C).
\eee
And finally note that $P(A,B,C) = 0.125 = P(A)P(B)P(C) = 0.5^3$. So we have mutual independence. And we see that $P(A|B,C) = \fracc{P(A,B,C)}{P(B,C)} = \fracc{0.125}{0.25} = \fracc{1}{2}$. And $P(A|C) = \fracc{P(A,C)}{P(C)} = \fracc{0.25}{0.5} = 0.5$. So they are equal and we have $A,B$ conditionally independent given $C$. 

\item \textit{Prove that if $A$ and $B,C$ are conditionally independent given $D$, then $A,B$ are conditionally independent given $D$. }

\begin{proof}
Since $A$ and $B,C$ are conditionally independent given $D$, we know:
$$
P(A|B,C,D) = P(A|D).
$$
We want to show:
$$
P(A|B,D) = P(A|D).
$$
Note that $P(B) = P(B,C|P(C) = 1)$. So we have:
$$
P(A|B,D) = P(A|B,C,D|P(C) = 1) = P(A|D).
$$
So we have the desired result. 
\end{proof}
\end{enumerate}

\begin{rem}
Know this is as well, it's really easy, just $d$-separation. If there's any single directed path that isn't d separated, then they're dependent. 
\end{rem}

\item \begin{enumerate}
\item Given that $E$ is empty, fanbelt broken is independent of \textbf{alternator broken} since it is d-separated by no charging; \textbf{battery age, battery dead, and battery meter} since they are d-separated by battery flat; and \textbf{no oil, no gas, fuel line blocked, starter broken, and dipstick} since they are all d-separated by one of lights, oil light, gas gauge, or car won't start. 

\item Given $E$ is empty, battery meter is independent of \textbf{alternator broke, fanbelt broken, and no charging} since they are d-separated by battery flat. It is also independent of \textbf{no oil, no gas, fuel line blocked, starter broken, and dipstick} since they are all d-separated by one of lights, oil light, gas gauge, or car won't start. 

\item Given $E = $ battery flat, battery age is conditionally independent of \textbf{no oil, no gas, fuel line blocked, starter broken, lights, oil light, gas gauge, car won't start, and dipstick} since d-separated by battery flat, which is observed. 


\item Given E = battery dead, no charging, we know battery flat is conditionally independent of \textbf{battery age, alternator broke, and fanbelt broken} since they are d separated by one of the nodes in $E$. And it is also independent of \textbf{no oil, no gas, fuel line blocked, starter broken, and dipstick} since they are all d-separated by one of lights, oil light, gas gauge, or car won't start. 

\end{enumerate}
\item 
\begin{enumerate}
\item 
\bee
P(b|j,m) &= \alpha P(b)\sum_E\left( P(E) \sum_A \left(P(A|b,E)P(j|A)P(m|A)\right) \right)\\
&= \alpha P(b)\sum_E\left( P(E \left(P(a|b,E)P(j|a)P(m|a) +P(\neg a|b,E)P(j|\neg a)P(m|\neg a) \right) \right)\\
&= \alpha P(b) [P(e) \left(P(a|b,e)P(j|a)P(m|a) +P(\neg a|b,e)P(j|\neg a)P(m|\neg a) \right)\\
&+ P(\neg e) \left(P(a|b,\neg e)P(j|a)P(m|a) +P(\neg a|b,\neg e)P(j|\neg a)P(m|\neg a) \right)].
\eee


\item 
\bee
\alpha^{-1} &= \sum_B\sum_E\sum_A P(B,E,A,j,m)\\
&= \alpha^{-1} \sum_B \left( P(B)\sum_E\left( P(E) \sum_A \left( P(A|B,E)P(j|A)P(m|A)\right) \right)\right)
\eee
Assume $B$ is true. Then we have:
\bee
\alpha^{-1}_b &=0.001 \sum_E\left( P(E) \sum_A \left( P(A|b,E)P(j|A)P(m|A)\right) \right)\\
&=0.001 \cdot 0.002 \sum_A \left( P(A|b,e)P(j|A)P(m|A)\right) \\
&+ 0.001 \cdot 0.998 \sum_A \left( P(A|b,\neg e)P(j|A)P(m|A)\right) \\
&=  0.001 \cdot 0.002 \cdot 0.95\cdot 0.90 \cdot 0.7\\
&+ 0.001 \cdot 0.998 \cdot 0.94\cdot 0.90 \cdot 0.7\\
&+  0.001 \cdot 0.002  \cdot 0.05 \cdot 0.05 \cdot 0.01  \\
&+ 0.001 \cdot 0.998  \cdot 0.06 \cdot 0.05 \cdot 0.01 \\
&\approx 0.0005922.
\eee
Now assume $B$ is false. Then we have: 
\bee
\alpha^{-1}_{\neg b} &=0.001 \sum_E\left( P(E) \sum_A \left( P(A|b,E)P(j|A)P(m|A)\right) \right)\\
&=0.999 \cdot 0.002 \sum_A \left( P(A|b,e)P(j|A)P(m|A)\right) \\
&+ 0.999 \cdot 0.998 \sum_A \left( P(A|b,\neg e)P(j|A)P(m|A)\right) \\
&=  0.999 \cdot 0.002 \cdot 0.29\cdot 0.90 \cdot 0.7\\
&+ 0.999 \cdot 0.998 \cdot 0.001\cdot 0.90 \cdot 0.7\\
&+  0.999 \cdot 0.002  \cdot 0.71 \cdot 0.05 \cdot 0.01  \\
&+ 0.999 \cdot 0.998  \cdot 0.999 \cdot 0.05 \cdot 0.01 \\
&\approx 0.00149.
\eee
So $\alpha^{-1} = \alpha^{-1}_b + \alpha^{-1}_{\neg b} = 0.00208$. Thus $\alpha = 479.8234$. 


\item \bee
P(b|j,m) &= 479.8234 P(b)\sum_E\left( P(E) \sum_A \left(P(A|b,E)P(j|A)P(m|A)\right) \right)\\
&= 479.8234 P(b)\sum_E\left( P(E \left(P(a|b,E)P(j|a)P(m|a) +P(\neg a|b,E)P(j|\neg a)P(m|\neg a) \right) \right)\\
&= 479.8234 P(b) [P(e) \left(P(a|b,e)P(j|a)P(m|a) +P(\neg a|b,e)P(j|\neg a)P(m|\neg a) \right)\\
&+ P(\neg e) \left(P(a|b,\neg e)P(j|a)P(m|a) +P(\neg a|b,\neg e)P(j|\neg a)P(m|\neg a) \right)]\\
&= 0.28417.
\eee
\item 
\bee
P(j,m) = \alpha^{-1} = 0.00208.
\eee

\end{enumerate}

\item 

\begin{rem}
Ask for solutions. 
\end{rem}

\begin{enumerate}
\item 
\begin{proof}
Let $(V,E)$ be an undirected path from $X \to W$. Then since this path must contain $U$ for some node $U \in Blanket(X)$, which we know by definition of the Markov blanket: the path must pass through one of the parents, children, or children's parents of $X$. Thus it satisfies the first d-separation criterion, and hence is conditionally independent. 
\end{proof}

\item \begin{proof}
 Assume $W$ is not $X$ and outside markov blanket. Then assume $W$ is not a descendant of $X$, and assume there exists an undirected path to it from $X$. Given the parents of $X$, we know $W$ is d separated from the grandparents of $X$ by the first d separation criterion since we are assuming we observe its parents. And it is d-separated from its siblings by the second d separation criterion (common cause) since we observe its parents. And since any undirected path to $X$ from a node which is not its descendant must pass through its grandparents or siblings, we know $W$ is conditionally independent of $X$. 
\end{proof}

\end{enumerate}

\setcounter{enumi}{6}

\begin{rem}
Ask for solution. You should know how to write joint probabilities from Markov networks. for problem 7
\end{rem}

\item 
\bee
\fracc{P(x_1 = 1 \vee Y = \Set{y_1,...,y_n})}{P(x_1 = -1 \vee Y = \Set{y_1,...,y_n})} &= e^{-E(x_+,y) + E(x_-,y)}\\
&= e^{-(h + h\sum_{i > 1} x_i - 2 \beta - \beta \sum_{i,j \neq 1,2;1,3} x_i x_j - \eta \sum_j x_iy_i)}\\
&\times e^{( -h + h\sum_{i > 1} x_i - 2 \beta - \beta \sum_{i,j \neq 1,2;1,3} x_i x_j - \eta \sum_j x_iy_i)}. \\
\eee

\end{enumerate}
























\end{document}


