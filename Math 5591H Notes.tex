%% filename: Math 5591H Notes.tex
%% version: 1.0
%% date: 2018/03/24
%%
%% American Mathematical Society
%% Technical Support
%% Publications Technical Group
%% 201 Charles Street
%% Providence, RI 02904
%% USA
%% tel: (401) 455-4080
%%      (800) 321-4267 (USA and Canada only)
%% fax: (401) 331-3842
%% email: tech-support@ams.org
%% 
%% Copyright 2006, 2008-2010, 2014 American Mathematical Society.
%% 
%% This work may be distributed and/or modified under the
%% conditions of the LaTeX Project Public License, either version 1.3c
%% of this license or (at your option) any later version.
%% The latest version of this license is in
%%   http://www.latex-project.org/lppl.txt
%% and version 1.3c or later is part of all distributions of LaTeX
%% version 2005/12/01 or later.
%% 
%% This work has the LPPL maintenance status `maintained'.
%% 
%% The Current Maintainer of this work is the American Mathematical
%% Society.
%%
%% ====================================================================

%    AMS-LaTeX v.2 driver file template for use with amsbook
%
%    Remove any commented or uncommented macros you do not use.

\documentclass[9pt,reqno,twoside]{amsbook}

%    For use when working on individual chapters
%\includeonly{}

%    Include referenced packages here.
\usepackage{}
\usepackage[top=0.75in,bottom=0.75in,left=1in,right=0.75in,papersize={6in,9in}]{geometry}
%\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
%\usepackage{bbm}
%\usepackage{cancel}
\usepackage{verbatim}
\usepackage{amsrefs}
\usepackage{enumitem}
%\usepackage{tikz}
%\usepackage{environ}
\usepackage{tikz-cd}
%\usepackage[pdf]{pstricks}
\usepackage{braket}
\usetikzlibrary{cd}
%\usepackage[ruled,linesnumbered]{algorithm2e}
\usepackage{adjustbox}
%\usepackage{changepage}
%\usepackage{import}
%\usepackage{newclude}
\usepackage[all,cmtip]{xy}

%============================================
%To fix overlapping TOC, comment out titlesec
%then compile, then put it back in, then comp
%-ile again. 
%============================================

\usepackage[]{titlesec}


\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{graphicx}
\usepackage{MnSymbol}
\usepackage[makeroom]{cancel}
\usepackage{imakeidx}
\makeindex




%All this crap needs to be removed and then
%put back in as well to fix TOC issue. 

\titlespacing{\section}{0mm}{7mm}{5mm}[0mm]
\titleformat{\section}[frame]
{\normalfont\scshape}
{\filright
\footnotesize
\enspace SECTION \thesection\enspace}
{8pt}
{\Large\scshape\filcenter}



\titleformat{\chapter}[display]
{\scshape\LARGE}
{\filleft\MakeUppercase{\chaptertitlename} \Huge\thechapter}
{4ex}
{\titlerule
\vspace{2ex}%
\filright}
[\vspace{2ex}%
\titlerule]




\usepackage{hyperref}

\hypersetup{
     colorlinks   = true,
     citecolor    = red
}












\let\oldemptyset\emptyset
\let\emptyset\varnothing
\theoremstyle{plain}
\newtheorem{Thm}{Theorem}
\newtheorem{Prob}[Thm]{Problem}
%\theoremstyle{definition}
\newtheorem{Remark}[Thm]{Remark}
\newtheorem{Tech}[Thm]{Technical Remark}
\newtheorem*{Claim}{Claim}
%----------------------------------------
%CHAPTER STUFF
\newtheorem{theorem}{Theorem}[chapter]
\numberwithin{section}{chapter}
\numberwithin{equation}{chapter}
%CHAPTER STUFF
%----------------------------------------
\newtheorem{lem}[theorem]{Lemma}
%\newtheorem{Q}[theorem]{Question}
\newtheorem{Prop}[theorem]{Proposition}
\newtheorem{Cor}[theorem]{Corollary}



\theoremstyle{definition}
\newtheorem{e}{Exercise}
\newtheorem{Def}[theorem]{Definition}
\newtheorem{Ex}[theorem]{Example}
\newtheorem{xca}[theorem]{Exercise}

\theoremstyle{remark}
\newtheorem{rem}[theorem]{Remark}
%NAMED THEOREMS
\theoremstyle{plain}
\newtheorem*{namedthm}{\namedthmname}
\newcounter{namedthm}
\makeatletter
	\newenvironment{named}[2]
	{\def\namedthmname{#1}
	\refstepcounter{namedthm}
	\namedthm[#2]\def\@currentlabel{#1}}
	{\endnamedthm}
\makeatother






\newcommand{\Mod}[1]{\ (\mathrm{mod}\ #1)}
\newcommand{\norm}{\trianglelefteq}
\newcommand{\propnorm}{\triangleleft}
\newcommand{\semi}{\rtimes}
\newcommand{\sub}{\subseteq}
\newcommand{\fa}{\forall}
\newcommand{\R}{\mathbb{R}}
\newcommand{\z}{\mathbb{Z}}
\newcommand{\n}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\renewcommand{\c}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\bb}{\vspace{3mm}}
\newcommand{\heart}{\ensuremath\heartsuit}
\newcommand{\mc}{\mathcal}
\newcommand{\bee}{\begin{equation}\begin{aligned}}
\newcommand{\eee}{\end{aligned}\end{equation}}
\renewcommand{\nequiv}{\not\equiv}
\newcommand{\lc}[2]{#1_1 + \cdots + #1_{#2}}
\newcommand{\lcc}[3]{#1_1 #2_1 + \cdots + #1_{#3} #2_{#3}}
\newcommand{\ten}{\otimes} %tensor product
\newcommand{\fracc}{\frac}
\newcommand{\tens}{\otimes}
\newcommand{\lpar}{\left(}
\newcommand{\rpar}{\right)}
\newcommand{\floor}{\lfloor}
\newcommand{\Tau}{\mc{T}}
\newcommand{\rank}{\text{rank}}
\DeclareMathOperator{\coker}{coker}
\newcommand*\pp{{\rlap{\('\)}}}
\newcommand{\counter}{\setcounter}
\newcommand{\gal}{\mathrm{Gal}}
\newcommand{\aut}{\mathrm{Aut}}
\newcommand{\fix}{\mathrm{Fix}}
\newcommand{\qwe}{\sqrt}
\newcommand{\wer}{\sqrt}






\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}
\renewcommand{\tt}{\text}
\renewcommand{\rm}{\normalshape}%text inside math
\renewcommand{\Re}{\operatorname{Re}}%real part
\renewcommand{\Im}{\operatorname{Im}}%imaginary part
\renewcommand{\bar}{\overline}%bar (wide version often looks better)
\renewcommand{\phi}{\varphi}


\makeatletter
\newenvironment{restoretext}%
    {\@parboxrestore%
     \begin{adjustwidth}{}{\leftmargin}%
    }{\end{adjustwidth}
     }
\makeatother


%---------END-OF-PREAMBLE---------
%---------------------------------

%    For a single index; for multiple indexes, see the manual
%    "Instructions for preparation of papers and monographs:
%    AMS-LaTeX" (instr-l.pdf in the AMS-LaTeX distribution).
\makeindex

\begin{document}

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\LARGE BRENDAN WHITAKER }\\[0.3cm] % Name of your university/college
\textsc{\LARGE THE OHIO STATE UNIVERSITY  }\\[0.3cm]
%\textsc{\Large JALANDHAR-144011, PUNJAB(INDIA) }\\[0.3cm]
\textsc{\Large Mathematics}\\[0.5cm] % Major heading such as course name
 % Minor heading such as course title

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.4cm]
{ \Huge \bfseries ABSTRACT ALGEBRA II\\
\vspace{1mm}  (MATH 5591H)}\\[0.03cm] % Title of your document
\HRule \\[1.5cm]

 
%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

%\begin{minipage}{0.4\textwidth}
%\begin{flushleft} \large
%emph{Author:}\\
%Brendan Whitaker \\Undergraduate\\3rd Year % Your name
%\end{flushleft}
%\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{center} \large
\emph{Instructor:} \\
Alexander Leibman\\Professor\\Dept. of Mathematics % Supervisor's Name
\end{center}
\end{minipage}\\[1cm]

% If you don't want a supervisor, uncomment the two lines below and remove the section above
%\Large \emph{Author:}\\
%John \textsc{Smith}\\[3cm] % Your name

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

{\large January-April, 2018 \\Lecture Notes}\\[1cm] % Date, change the \today to a set date if you want to be precise

%----------------------------------------------------------------------------------------
%	LOGO SECTION
%----------------------------------------------------------------------------------------

\vspace{15mm}
\includegraphics[scale=0.15]{osuLogo.png}\\[1cm] % Include a department/university logo - this will require the graphicx package
 
%----------------------------------------------------------------------------------------

\vfill % Fill the rest of the page with whitespace

\end{titlepage}

%\frontmatter

\title{ABSTRACT ALGEBRA II NOTES}

%    Remove any unused author tags.

%    author one information
\author{BRENDAN WHITAKER}
\address{}
\curraddr{}
\email{}
\thanks{}

%    author two information
\author{}
\address{}
\curraddr{}
\email{}
\thanks{}

\subjclass[2010]{12-XX}

\keywords{}

\date{SP18}

\begin{abstract}
A comprehensive set of notes for Professor Alexander Leibman's Abstract Algebra II course, taken SP18 at The Ohio State University. The material starts with Part III of Dummit and Foote's \textit{Abstract Algebra}, which covers Modules and Vector Spaces. 
\end{abstract}

%\maketitle

%    Dedication.  If the dedication is longer than a line or two,
%    remove the centering instructions and the line break.
%\cleardoublepage
%\thispagestyle{empty}
%\vspace*{13.5pc}
%\begin{center}
%  Dedication text (use \\[2pt] for line break if necessary)
%\end{center}
%\cleardoublepage

%    Change page number to 6 if a dedication is present.
\setcounter{page}{4}

\tableofcontents

\section{Review of groups}

\begin{theorem}

\begin{enumerate}
\item
\end{enumerate}
\end{theorem}



\section{Review of rings}

We note here that $\mathbb{Z}[x]/(x^2 - 2) \cong \mathbb{Z}[\sqrt{2}]$, but we also have $\mathbb{Z}[x]/(x^2 - 4) \ncong \mathbb{Z}$. And it is not correct to say $x = \sqrt{2}$ in the former case because these are two distinct elements in the quotient ring. 

\begin{Def}
We denote the \index{group of units}\textbf{group of units} of a ring $R$ as $R^\times$. 
\end{Def}


\begin{Def}
An \textbf{integral domain} is a commutative, unital ring with no zero-divisors. 
\end{Def}

\begin{Def}
We define the product $IJ$ of ideals $I,J$ as:
$$
IJ = \Set{i_1j_1 + \cdots i_nj_n|i_k \in I,j_k \in J}.
$$
\end{Def}

\begin{Def}
We define the sum $I+J$ of ideals $I,J$ as:
$$
I+J = \Set{i + j|i\in I,j \in J}.
$$
\end{Def}

\begin{Def}
A \textbf{Noetherian ring} $R$ is a ring in which any collection if ideals in $R$ has a maximal element. 
\end{Def}

\begin{Def}
A \textbf{Noetherian ring} $R$ is a ring in which any ideal is finitely generated. 
\end{Def}

\begin{theorem}[\textbf{Eisenstein's Criterion}]
Consider $q(x) = a_nx^n + a_{n - 1}x^{n - 1} + \cdots + a_1x + a_0$. If there exists a prime integer $p$ such that the following three conditions all apply:
\begin{itemize}
\item $p$ divides each $a_i$ for $i \neq n$,
\item $p$ does not divide $a_n$,
\item $p^2$ does not divide $a_0$,
\end{itemize}
then $q$ is irreducible over the rational numbers. 
\end{theorem}






\mainmatter
%    Include main chapters here.
%\include{}
\setcounter{part}{2}
\part{MODULES AND VECTOR SPACES}
\setcounter{chapter}{9}

%========compile.fast==================================


\chapter{Introduction to module theory}
\section{Basic definitions and examples}



\textbf{Monday, January 8th}
\begin{Def}
An \textbf{R-module} (left) is a set $M$ with: 
\begin{enumerate}
\item Abelian addition operation
\item Action of $R$ on $M$ s.t. 
\begin{enumerate}
\item $(r + s)m = rm + sm$ 
\item $(rs)m = r(sm)$
\item $r(m + n) = rm + rn$ --- this says that $r$ acts as a self homomorphism of the ring. 
\item $1m = m$
\end{enumerate}
So it's an abelian group with an $R$-action. 
\end{enumerate}
\end{Def}

We should think of elements of $R$ as ``scalars", and elements of $M$ as ``vectors".

\begin{lem}
The set Hom($M,M) = \{homomorphisms \text{ } \phi:M \to M\}$ is a ring. 
\end{lem} 

\begin{proof}
If you have two homs, you can add them, and the product is a composition. They are associative, under addition, they form a group. And we have the distributive law: 
$$
\phi(\xi + \psi) = \phi\xi + \phi\psi.
$$
You should think of it as similar to the ring of matrices. 
\end{proof}


\begin{lem}
If $M$ is an $R$-module, then we have a ring homomorphism $\Phi: R \to \text{Hom}(M,M)$. And then mapping is given by $\Phi: a \to \phi_a$ s.t. $\phi_a(u) = au$. 
\end{lem} 

Note that there may be elements of $R$ that act trivially, i.e. which send every element to zero. 

\begin{Def}
Module means left-module. 
\end{Def}

\begin{Def}
Modules with $1m = m$ are \textbf{unital} modules. All modules dealt with are unital. 
\end{Def}

\begin{Def}
An \textbf{$R$-submodule} is $N \leq M$ closed under the ring action ($rn \in N$). 
\end{Def}

\begin{Prop}[\textbf{The submodule criterion}]
Let $R$ be a ring and let $M$ be an $R$-module. A subset $N$ of $M$ is a submodule of $M$ if and only if:
\begin{enumerate}
\item $N \neq \emptyset $,
\item $x + ry \in N$ for all $r \in R$ and for all $x,y\in N$. 
\end{enumerate}
\end{Prop}

\begin{lem}
Every R-module $M$ has two submodules, $0$ and $M$. 
\end{lem}

\begin{lem}
$R$ is a module over itself, and in this case, the submodules of $R$ are exactly the left ideals of $R$. 
\end{lem}

\begin{Def}
The \textbf{free module} of rank $n$ over $R$ is 
$$
R^n = \{(a_1,...,a_n): a_i \in R\}.
$$
They are analogous to free groups. The set 
$$
\{(0,...,0,a_i,0,...,0)\}
$$
is the i-th component module and is a submodule of the free module. 
\end{Def}

\begin{lem}
If $M$ is an $R$-module and $S$ is a subring of $R$ with $1_S = 1_R$, then $M$ is automatically an $S$-module as well. 
\end{lem}

\begin{Def}
If $M$ is an $R$-module and $I$ a two sided ideal of $R$, we say $M$ is \textbf{annihilated} by $I$ when for all $a \in I$, and for all $m \in M$ we have:
$$
am = 0. 
$$
\end{Def}

\begin{Def}\label{annihilator1}
We define the \textbf{annihilator} of $M$ as Ann$(M) = \{a \in R: au = 0 \forall u \in M\} = \{a:aM = 0\}$. This is an ideal in $R$, and it is exactly the kernel of the homomorphism $\Phi:R \to Hom(M,M)$. 
\end{Def}

\begin{lem}
When $M$ is annihilated by $I$, we can make $M$ into an $(R/I)$-module by redefining our ring action as:
$$
(r + I)m = rm,
$$
which divys the distinct ring actions into cosets of the quotient ring. 
\end{lem}

\begin{lem}
When $I$ is maximal in $R$ and $IM = 0$, $M$ is a vector space over the field $R/I$. 
\end{lem}

\begin{Ex}
We give some examples of modules:
\begin{enumerate}
\item The $0$ module is $\{0\}$, where $a0 = 0$ $\forall a$. 
\item $R$ is an $R$-module. 

\item Free module of rank $n$, $R^n$ as defined above. 
\item Any abelian group is a $\mathbb{Z}$-module:
$$
nu = u + \cdots + u\text{ (n times)}. 
$$
\item Any ideal in $R$ is an $R$-module. $\forall u \in I$, $a \in R$ we have $au \in I$. 

\item 

\begin{enumerate}
\item Let $F$ be a field, and $V$ an $F$-vector space. Let $T$ be a linear transfomation of $V$. Then $V$ is an $F[x]$-module:
$$
(a_nx^n + \cdots a_1x + a_0)u = a_nT^n(u) + \cdots + a_1T(u) + a_0u. 
$$
We have this because we let $xu = Tu$. And we can apply the same construction to any ring, and any modulo over this ring, it doesn't have to be a field. 
\item $R$-module, $T:M \to M$ is a hom-sm (as $R$-module, $T(au) = aT(u)$) then $M$ is an $R[x]$-module, $xu = Tu$. 
\end{enumerate}
\item Let $R$ be a ring, $X$ is a set, and $M = \{\text{functions } X\to R\}$. \\
Then $M$ is an $R$-module, $(af)(x) = a(f(x))$. In this case we call $M$ an algebra. 
\end{enumerate}
\end{Ex}

\begin{Def}
If $A$ is an $R$-module, and $A$ is a ring itself with $a(uv) = (au)v = u(av)$ $\forall a \in R$ and $\forall u,v \in A$, then $A$ is called an $R$\textbf{-algebra}. 
\end{Def}

\begin{lem}
If $R$ is a commutative ring, then $\{\text{functions } X\to R\}$ and $R^n$ are $R$-algebras. 
\end{lem}

Also $M_n = \{n \times n \text{ matrices over } R\}$ is a (noncommutative) $R$-algebra. (Why?)\\
If $R$ is a subring of a ring $A$ and $R \subset Z(A)$, then $A$ is an $R$-algebra. Or: if $R,A$ are rings and $\phi:R \to A$ is a hom-sm with $\phi(R) \subset Z(A)$, then again $A$ is an $R$-algebra, $au = \phi(a)u$. 

\begin{lem}
For any ideal $I$ of a ring $R$, $R$ is an $I$-algebra. 
\end{lem}

\textbf{Constructions:}
\begin{enumerate}
\item Submodule: a subgroup $N \subset M$ s.t. $RN \subset N$. 
\item $S$ is a subring of $R$ and $M$ is an $R$-module, then $M$is an $S$-module \textbf{(reduction of scalars)}. 
\item $M$ is an $R$-module, then $M$ is an $R/\text{Ann}(M)$-module. 
$$
\bar{a}u = au, \bar{a} = a + \text{Ann}(M).
$$
\end{enumerate}

\textbf{Tuesday, January 9th}

Recall we noted that if we don't have the condition that $1u = u$. Assume that we don't have this condition. Let $M$ be an $R$-module without it. Then define:
$$
M_0 = \{u \in M: 1u = 0\},
$$
$$
M_1 = \{u \in M: 1u = u\}.
$$
We can check that both of these are submodules of $M$. $\forall c \in R$, if $u \in M_0$, then $1\cdot(cu) = c(1u) = 0$, so $cu \in M_0$.  And if $u \in M_1$, then $1(cu) = c(1u) = cu$, so $cu \in M_1$. So we have checked that the definition of submodule is satisfied. Also note that $M_0 \cap M_1 = 0$. \\
And $\forall u \in M$, $u = 1u + (u - 1\cdot u)$. The stuff on the right side of plus sign is in $M_0$ and left side is in $M_1$. So we have $M = M_0\oplus M_1$. \\
So $\forall c \in R$, $\forall u \in M_0$, $cu = c\cdot1u = 0$. So the above statement just says that each element in $M$ can be written as a sum of elements from $M_0,M_1$. 

Keep in mind that what we defined yesterday was a left module. A right module is an abelian group $M$ with mapping $M \times R \to M$ where $(u,a) \to ua$ s.t. we have:
\begin{enumerate}
\item $(u + v)a = ua + va$
\item $(u(a + b) = ua + ub$
\item $u(ab) = (ua)b$
\item $u1 = u$
\end{enumerate} 
So note that the first two conditions are unchanged in nature from left modules, since it doesn't matter on which side you multiply the scalar ($a$). But the third condition is different, because we are now using a right group action. In the left module we first multiplied $b$ by $u$ and then $a$. Here we do $a$ first, because right group action. 

\begin{lem}
If $R$ is commutative, then left modules are right modules because we apply commutativity to the difference described above in the third condition. 
\end{lem}

\begin{lem}
Left ideals in $R$ are left $R$-modules, and same for right. 
\end{lem}

\begin{Def}
A \textbf{two-sided $R$-module} is an abelian group with both left and right module structures. 
\end{Def}
Note that a two-sided ideal is an example of a two-sided module. We will only deal with commutative rings forever, and we assume that our modules are left modules, except maybe when we discuss tensor products. 

\begin{rem}
There are 2 definitions of \textbf{annihilators} in module theory. The first is the one used to define Ann$_R(N)$ where $N$ is a submodule of an $R$-module $M$, where we allow $N = M$. This is Definition \ref{annihilator1}. The second is the definition of the annihilator of an ideal in a module, given in Exercise 10 below, which is:
$$
Ann_M(I) = \{m \in M: am = 0, \forall a \in I\}.
$$
\end{rem}

\begin{rem}
If $N\sub M$, for some $R$-module $M$, then $N$ is always a left-ideal, but not necessarily two sided, this requires $N$ to be a submodule. But if $R$ is commutative, this is unimportant since left ideals are right ideals. 
\end{rem}

We define the annihilator of a subset $S \sub R$. 

\begin{Def}
$$
Ann_M(S) = \{u \in M: su = 0, \forall s \in S\}.
$$
\end{Def}

The annihilator above is an additive subgroup, but not a submodule, we need $S$ to be a \textbf{right} ideal in order to get a submodule, since we need $S(cu) = 0$, so we need $Sc \sub S$.  Also, it is a submodule if $R$ is commutative. 

\begin{rem}
If $R$ is commutative, annihilators of subsets of $R$ in $M$ are submodules, and annihilators of subsets of $M$ in $R$ are two-sided ideals. 
\end{rem}

Now Professor Leibman does Exercise 10.1.11 and several others from this section. 

Some particulars on the definitions of an $R$-algebra:

\begin{Def}[Leibman's Definition]
$A$ is an $R$-algebra if $A$ is a ring and an $R$-module so that:
$$
a(uv) = (au)v = u(av).
$$
\end{Def}

\begin{Def}[Dummit and Foote's Definition]
A ring $A$ is an $R$-algebra if we are given a ring homomorphism $\phi:R \to A$ s.t. $\phi(R) \sub Z(A)$, where we define the $R$-action on $A$ by $au = \phi(a)u$. 
\end{Def}

Note that if $1 \in A$, then define $\phi:R \to A$ by $\phi(a) = a \times 1 \in A$. Hence the two above definitions are equivalent when we have $1 \in A$. So our takeaway is that the top definition (Leibman's) is more general and just better in every way. 



\section*{10.1 Exercises}


\begin{enumerate}[label=\arabic*.]
\item
\textit{Prove that $0m = 0$ and $(-1)m = -m$ $\forall m \in M$. }

\begin{proof}
Suppose there exists $m$ s.t. $0m = c \neq 0$. Then because of the group structure of our module, we have: 
$$
c - c = 0 = 0m - 0m = (0 - 0)m = 0m
$$
which is a contradiction, since we assumed $0m \neq 0$. \\
We add:
$$
1m + (-1)m = (1 - 1)m = 0m = 0,
$$
so since $1m = m$, we know $1m + (-1)m = 0 \Rightarrow m + (-1)m = 0 \Rightarrow (-1)m = -m$. 
\end{proof}



\item \textit{Prove that $R^\times$ and $M$ satisfy the two axioms in Section 1.7 for a group action of the multiplicative group $R^\times$ on the set $M$. }

\begin{proof}
Recall that $R^\times$ denotes the group of units of $R$. The group action properties of a group $G$ acting on a set $X$ are: 
\begin{enumerate}
\item $g_1(g_2x) = (g_1g_2)x$,
\item $1x = x$ $\forall x \in X$.
\end{enumerate}
Note that the definition of an $R$ module stipulates that we have $(rs)m = r(sm)$ $\forall r,s \in R^\times$ and $\forall m \in M$. And another part of the definition of an $R$-module gives us that $1m = m$ $\forall m \in M$, and since $R^\times$ is a group, we have satisfied the definition of a group action. 
\end{proof}
\vspace{20mm}
\item \textit{Assume that $rm = 0$ for some $r \in R$ and some $m \in M$ with $m \neq 0$. Prove that $r$ does not have a left inverse (i.e. there is no such $s \in R$ s.t. $sr = 1$). }
\begin{proof}
Suppose there were such an $s$. Then we would have:
$$
srm = 1m = m = s(0) = 0,
$$
which is a contradiction, since we said $m \neq 0$. 
\end{proof}
\vspace{3mm}
\setcounter{enumi}{4}
\item \textit{For any left ideal $I$ of $R$, define: 
$$
IM = \{\sum_{\text{finite}}a_im_i:a_i\in I, m_i \in M\}
$$
to be the collection of all finite sums of elements of the form $am$ where $a \in I$ and $m \in M$. Prove that $IM$ is a submodule of $M$. }

\begin{proof}
We know $IM$ is nonempty since $I$ contains $0$, so $0m \in IM$, and by exercise 1, we know $0 \in IM$. So let $x,y \in IM$ such that:
$$
x = a_1m_1 + \cdots + a_km_k
$$
$$
y = b_1n_1 + \cdots + b_ln_l
$$
 with $a_i,b_i \in I$, $m_i,n_i \in M$, and let $r \in R$. Then we have the following by the distributive property of scalars in the definition of an $R$-module: 
\begin{equation}
\begin{aligned}
x + ry &= a_1m_1 + \cdots + a_km_k + r(b_1n_1 + \cdots + b_ln_l)\\
&= a_1m_1 + \cdots + a_km_k + rb_1n_1 + \cdots rb_ln_l.
\end{aligned}
\end{equation}
Now since $I$ is a left ideal, we know $rb_i \in I$ since $b_i \in I$, so $x + ry$ is a finite sum of elements of the form $a_im_i$ and so it is in $IM$. Then by the submodule criterion, $IM$ is a submodule of $M$. 
\end{proof}

\vspace{3mm}
\item \textit{Show that the intersection of any nonempty collection of submodules of an $R$-module $M$ is a submodule. }

\begin{proof}
Let $N = \cap N_i$ be an arbitrary collection of submodules of $M$. Recall from group theory that an arbitrary intersection of subgroups is a subgroups, so we know $N \leq M$. So we need only show that it is closed under the group action of $R$. So let $r \in R$, and let $n \in N$. Then $n \in N_i$ $\forall i$. So $rn \in N_i$ $\forall i$ since $N_i$ is a submodule of $M$. But then $rn \in N$ by definition, so $N$ is a submodule. 
\end{proof}

\vspace{3mm}
\item \textit{Let $N_1 \subset N_2 \subset \cdots$ be an ascending chain of submodules of $M$. Prove that $N = \cup_{i = 1}^\infty N_i$ is a submodule of $M$. }

\begin{proof}
We first prove that $N$ is a subgroup under addition of $M$. It is a subset of $M$ since it is a union of subsets of $M$. Since $0 \in N_1$, and $N_1 \subset N_i$ $\forall i$, we know $0 \in N_i$ $\forall i$, so $0 \in N$. Let $n \in N$, then $n \in N_i$ for some $i$, so we have $-n \in N_i \subset N$, so we have additive inverses. And let $n_1,n_2 \in N$, then $n_1 \in N_i,n_2 \in N_j$ for some $i,j$, and without loss of generality, we may assume $i \leq j$. Then $N_i \subset N_j$, so $n_1 \in N_j$, and by closure of the subgroup $N_j$, we know $n_1 + n_2 \in N_j \subset N$, so we have additive closure of $N$, hence it is a subgroup of $M$. Now we show that $N$ is closed under the ring action of $R$. So let $r \in R$, and let $n \in N$, then $n \in N_i$ for some $i$, so $rn \in N_i$ since $N_i$ is a submodule, and since $N_i \subset N$, we know $rn \in N$, so $N$ is a submodule. 
\end{proof}

\item \textit{An element $m$ of the $R$-module $M$ is called a torsion element if $rm = 0$ for some nonzero element $r \in R$. The set of torsion elements is denoted: } \label{ex10.1.8}
$$
Tor(M) = \{m \in M:rm = 0 \text{ for some nonzero } r \in R\}. 
$$

\begin{enumerate}
\item \textit{Prove that if $R$ is an integral domain, then $Tor(M)$ is a submodule of $M$ (called the torsion submodule of $M$). }
\begin{proof}
We know Tor$(M)$ is a subset of $M$ by its definition. We first prove it is an additive subgroup. Let $m \in $ Tor$(M)$. Then $\exists r \in R$, $r \neq 0$ s.t. $rm = 0$. Then consider $-m \in M$. From exercise 1 we know 
$
-m = (-1)m$, so we have:
$$
r(-m) = r(-1)m = (-1)rm = (-1)0 = 0,
$$
 since $R$ is commutative. So we have that $-m \in $ Tor$(M)$ as well, hence we have additive inverses. We check that it has additive closure. Let $m,n \in $ Tor$(M)$. Then we have $r,s \in R$, neither being zero, s.t. $rm = 0, sn = 0$. Now consider $m + n$. We have:
$$
rs(m + n) = rsm + rsn = srm + rsn = s0 + r0 = 0.
$$
Since we have no zero divisors, since $R$ is an integral domain, we know $rs \neq 0$, so $m  +n \in $ Tor($M$), we have additive closure, and Tor$(M)$ is a subgroup of $M$. Now we need only check that it is closed under the left action of $R$. So let $r \in R$ and $m \in $ Tor$(M)$. Then consider $rm$. We assume $r \neq 0$, since otherwise $rm = 0$ which is in our subgroup. And we know $\exists s \in R$, $s \neq 0$ s.t. $sm = 0$. Now we have $srm = rsm = r0 = 0$, so $rm$ is in Tor$(M)$. So it's a submodule. 
\end{proof}

\vspace{3mm}
\item \textit{Give an example of a ring $R$ and an $R$-module $M$ such that $Tor(M)$ is not a submodule (consider the torsion elements in the $R$-module $R$). }

So from the previous exercise, we know we must choose some $R$ which is not an integral domain. We consider the torsion elements in the $R$-module $R$, which are:
$$
\text{Tor}(R) = \{r \in R: sr = 0 \text{ for some nonzero }s \in R\},
$$
but these are exactly the right zero divisors of $R$. We consider the ring $R = \mathbb{Z}_6 \cong \z/6\z$, and the module of $R$ over itself. Note that in $R$, $2 \cdot 3 = 6 = 0$, $4\cdot 3 = 12 = 0$, and $1,5$ are not zero divisors, so we have:
$$
\text{Tor}(R) = \{0,2,3,4\}.
$$
So note that $2,3\in \text{Tor}(R)$ and $1 \in R$, but $2 + 1\cdot 3 = 5 \notin \text{Tor}(R)$, so by the submodule criterion, it is not a submodule. 

\vspace{3mm}
\item \textit{If $R$ has zero divisors, show that every nonzero $R$-module has nonzero torsion elements. }

\begin{proof}
Suppose $R$ has zero divisors. So $\exists r,s \in R$ nonzero such that $rs = 0$. Now let $M$ be an $R$-module. We wish to show that $\exists m \in M$ s.t. $m \neq 0$, $tm = 0$ for some nonzero $t \in R$. Let $n \in M$ s.t. $n \neq 0$. Now consider $sn \in M$ and $r \in R$. Now note that $rsn = 0$ and that $r$ and $sn$ are both nonzero, so $sn$ is a nonzero torsion element. 
\end{proof}
\end{enumerate}
\vspace{3mm}
\item \textit{If $N$ is a submodule of $M$, the annihilator of $N$ in $R$ is defined to be: 
$$
\text{Ann}_R(N) = \{r \in R:rn = 0 \text{ for all }n \in N\}.
$$
Prove that the annihilator of $N$ in $R$ is a two-sided ideal of $R$. 
}

\begin{proof}
Let $A = \text{Ann}_R(N)$. We first show that $A$ is an additive subgroup of $R$. We know it is nonempty since $0 \in A$, and it is a subset of $R$ by construction. Now let $x,y \in A$. Consider $x(-y) = -xy$. Note $-xyn = -x(yn) = -x0 = 0$ $\forall n \in N$, so by the subgroup criterion, $A$ is a subgroup. Let $r \in R$, $n \in N$, and $a \in A$. Observe:
$$
ran = r(an) = r0 = 0,
$$
$$
arn = a(rn) = 0,
$$
since $a$ annihilates $n$, and $N$ is closed under the action of $R$, so $rn \in N$, and hence $a$ also annihilates $(rn)$. Since our $n$ was arbitrary, this holds for all $n \in N$. Thus $ra \in A$ and $ar \in A$, and thus $RA \sub A$ and $AR \sub A$, so since it's also an additive subgroup, $A$ is a two-sided ideal. 
\end{proof}

\vspace{3mm}

\item \textit{If $I$ is a right ideal of $R$, the annihilator of $I$ in $M$ is defined to be: 
$$
\text{Ann}_M(I) = \{m \in M:am = 0 \text{ for all }a \in I\}.
$$
Prove that the annihilator of $I$ in $M$ is a submodule of $M$. 
}
\begin{proof}
Since $I$ is a right ideal, we know $Ir \sub I$ $\forall r \in R$. Let $A = \text{Ann}_M(I)$ which we know is nonempty since $0 \in M$ since it is an abelian group, and $a0 = 0$ $\forall a \in I$. Let $m,n \in A$, let $a \in I$, and let $r \in R$. Observe: 
$$
a(m + rn) = am + arn = 0 + arn = (ar)n = 0,
$$
since $a \in I \Rightarrow ar \in I$ ($I$ is right ideal), hence $n$ annihilates $(ar)$. Thus $(m + rn) \in A$. Then by the submodule criterion, since this holds for arbitrary $m,n  \in A$, $r \in R$, and $A$ is nonempty, we know $A$ is a submodule of $M$. 
\end{proof}

\item \textit{Let $M$ be the abelian group (i.e. $\z$-module) $\z/24\z \times \z/15\z \times \z/50\z$. }
\begin{enumerate}
\item \textit{Find the annihilator of $M$ in $\z$ (i.e. a generator for this principal ideal). }\\
Recall that Ann$_{\z}(M) = \{z \in \z:zm = 0 ,\forall m \in M\}$. Observe that the least common multiple of $24,15,50$ is $600$. We claim that this is a generator for the principal ideal given by Ann$_\z(M)$. We must only check that it is nonzero, which is obvious, and that $600m = 0$, $\forall m \in M$. So let $m \in M$, then $m = (a,b,c)$ s.t. $a \in \z/24\z$, $b \in \z/15\z$, and $c \in \z/60\z$. Now observe:
$$
600m = 600(a,b,c) = (600a,600b,600c) \equiv (0,0,0) = 0 \in M,
$$
since $600 \equiv 0 \mod 24,15,50$. So Ann$_\z(M) = \langle 600 \rangle$. 
\item \textit{Let $I = 2\z$. Describe the annihilator of $I$ in $M$ as a direct product of cyclic groups. }


Recall that Ann$_M(I) = \{m \in M: mr = 0,\forall r \in I\}$. Thus we know: 
$$
Ann_M(I) = \{(a,b,c) \in M: (a,b,c)\},
$$
$$
Ann_M(2\z) = \{0,12\} \times \{0\} \times \{0,25\}
$$

\end{enumerate}

\item

\begin{enumerate}



\item \textit{$N$ is a submodule of $M$, $I = Ann(N)$, and $K = Ann(I)$. Then $N \sub K$. Give an example where $N \neq K$. We have notation for annihilator, $AnnN = N^{\perp}$. }

Note that $N \sub (N^\perp)^\perp$. Also note that for all $a \in I$, $\forall u \in N$, $au = 0$, so $u \in I^\perp$. So take $M = \z_6 \times \z_6$, $N = \{0,3\} \times \{0\}$, where we consider these two objects as $\z$-modules (abelian groups). Note $I = N^\perp = 2\z$. And $I^\perp = \{0,3\} \times \{0,3\}$. 

\item \textit{$I$ is an ideal in $R$. Then $I \sub (I^\perp)^\perp$. Give an example where they are not equal. }

Take $M = \z_2, I = (4)$, then $I^\perp = M$, but $M^\perp = (2)$. 



\end{enumerate}

\item \textit{$I$ - ideal in $R$. $M' = \{u \in M: I^ku = 0 \text{ for some }k \in \n\}$. Then prove that $M'$ is a submodule. }

\begin{proof}
For any $k$, let $M_k = Ann(I_k)$. Then note that $M_1 \sub M_2 \sub \cdots$. If $I \sub J$, then $Ann(J) \sub Ann(I)$. Then $M' = \bigcup_{k = 1}^\infty M_k$ is a submodule (can be easily checked). 
\end{proof}

\setcounter{enumi}{17}

\item \textit{Let $V = \R^2$, and let $R = \R[x]$. Let $x$ be the 2 by 2 matrix with 0,-1,1,0. And let $x$ act on $V$ by counterclockwise rotations by 90 degrees. Then $V$ is an $R$-module. Prove that $V$ is simple. Note that $R$ is isomorphic to $\mathbb{C}$. }

\begin{proof}
A general rule is that submodules of $V$ are $x$-invariant subspaces of $V$ as an $R$-module. 
\end{proof}

\item \textit{Let $F = \R$, and $V = \R^2$, and let $T$ be the linear transformation that projects to the $y$-axis. Show that $V,0,$ the $x$-axis, and the $y$-axis are the only $F[x]$-submodule for $T$. }

\begin{proof}
Note it's obvious that $0,V$ work as subspaces. Note to be a subspace we need $x + ry$ to be in the space for all $r$. Note our scalars in this case are polynomials in $T$, and they're going to send this expression to $x$ plus some polynomial function of the y coordinate of $y$, so it will move up and down in a vertical line in $\R^2$ from where $x$ was. Now in order for this to be in a 1-dimensional subspace, we need to have all points above and below any point in the space also in our space, unless the y coordinates are all zero, so the $x$-axis and $y$-axis work, and non others do. Note any potential subspace must go through the origin as well. 
\end{proof}

\item \textit{Let $F = \R$, and $V = \R^2$, and let $T$ be the linear transformation that rotates by $\pi$. Show that every subspace of $V$ is an $F[x]$-submodule for $T$. }

\begin{proof}
For the trivial space and the whole space it is obvious. For any subspace of $V$ which is 1-dimensional, it's a line through the origin, and a rotation by $\pi$ brings us back to the same line. 
\end{proof}










\end{enumerate}



\section{Quotient modules and module homomorphisms}

\begin{Def}
Let $M,N$ be $R$-modules, $\phi:M\to N$ is an \textbf{$R$-homomorphism} if: 
\begin{enumerate}
\item $\phi(u + v) = \phi(u) + \phi(v)$
\item $\phi(au) = a\phi(u)$.
\end{enumerate}
\end{Def}
So it is a homomorphism of groups, and also preserves scalar mult. 
\begin{Def}
If $R$ is a field and $M$ is then a vector space, $\phi$ is called a \textbf{linear mapping}, or a \textbf{linear transformation}. 
\end{Def}

The set of all $R$-hom-sms from $M\to N$ is denoted by $\text{Hom}_R(M,N)$. 
\begin{Def}In the case $M = N$, hom-sms $M \to M$ are called \textbf{endomorphisms}, and:
$$
\text{Hom}_R(M,M) = \text{End}_R(M). 
$$
\end{Def}
\begin{Def}
Injective hom-sms are called \textbf{monomorphisms}. 
\end{Def}

\begin{Def}
Surjective hom-sms are called \textbf{epimorphisms}. 
\end{Def}

\begin{Def}
Bijective hom-sms are called \textbf{isomorphisms}. 
\end{Def}

\begin{Def}
Bijective endomorphisms ($M \to M$) are called \textbf{automorphisms}. 
\end{Def}

\begin{lem}
If $R$ is a commutative ring, $\text{Hom}_R(M,N)$ is an $R$-module, by 
\begin{itemize}
\item $\phi + \psi)(u) = \phi(u) + \psi(u)$,
\item $(a\phi)(u) = a\phi(u)$.
\end{itemize}
\end{lem}
So why does it have to be commutative?
\begin{proof}
So is $a\phi$ a hom-sm? So consider:
$$
(a\phi)(bu) = a(\phi(bu)) = ab\phi(u) \neq b(a\phi)(u) = ba\phi(u),
$$
if $ab \neq ba$, so if $R$ is noncommutative, $a\phi$ may not be a hom-sm. 
\end{proof}
If $R$ is commutative, $\text{End}_R(M)$ is an $R$-algebra, because $(\phi\psi)(u) = \phi(\psi(u))$, and you also have to prove that it is a ring. Under an addition it is a group, associativity is clear, and the distributive law:
$$
\phi(\psi + \xi)(u) = \phi(\psi(u) + \xi(u)) = \phi(\psi(u)) + \phi(\xi(u)) = (\phi\psi)(u) + (\phi\xi)(u),
$$
the first equality is by definition, the second is by def of hom-sm. And we also must check that scalar multiplication is preserved to prove that it is an algebra. We have:
$$
((a\phi)\psi)(u) = (\phi(a\psi))(u) = a(\phi\psi)(u)
$$
$$
((a\phi)\psi)(u) = (a\phi)(\psi(u)) - a(\phi(\psi(u)))
$$
$$
(\phi(a\psi))(u) = \phi((a\psi)(u)) = \phi(a\psi(u)) = a\phi(\psi(u))
$$
\textbf{check over these conditions, confusing}And Aut$_R(M)$ is a group under multiplication (compositions), which is exactly the group of units in End$_R(M)$. 
\vspace{3mm}
We outline some elementary properties of modules:
\begin{enumerate}
\item $0u = 0$
\begin{proof}
$$
0u = (0 + 0)u = 0u + 0u,
$$
so done. 
\end{proof}
\item $a0 = 0$
\item $(-a)u= a(-u) = -au$
\end{enumerate}

\begin{Ex}
We give some examples of $R$-hom-sms:
\begin{enumerate}
\item $\mathbb{Z}$-modules = abelian groups (written additively). So what are $\mathbb{Z}$-hom-sms of $\mathbb{Z}$-modules? They are of course, the hom-sms of the abelian groups. If $\phi:G\to H$ is a group hom-sm, then $\phi(nu) = n\phi(u)$. For vector spaces, this is not true. Note in this case:
$$
\phi:V \to W, \phi(u + v) = \phi(u) + \phi(v) \nRightarrow \phi(cu) = c\phi(u),
$$
it only works for $\mathbb{Z}$-modules. 
\item If $R$ is commutative, and $c \in R$, then $\phi(u) = cu$ is an $R$-endomorphism of $M$. Note:
$$
\phi(u + v) = c(u + v)= cu+ cv = \phi(u) + \phi(v),
$$
$\forall a \in R$, $\phi(au) = cau = acu = a\phi(u)$. 
\vspace{3mm}

Consider $\phi:\mathbb{Z} \to \mathbb{Z}$ given by $\phi(n) = 2n$. It isn't a ring hom-sm since it doesn't respect mult ($\phi(mn) \neq \phi(m)\phi(n)$. \textbf{But this is} a hom-sm of $\mathbb{Z}$-modules. Why? Because $\phi(mn) = m\phi(n)$. Now consider the ring of polyns $R = F[x,y], \phi:x \leftrightarrow y$. Then note $\phi$ is automorphism of $R$, but is not a hom-sm of $R$-modules. Why? because it doesn't respect multiplication by scalars, take 
$$
yx = \phi(xy) \neq x\phi(y) = xx. 
$$

The \textbf{kernel and image} of a hom-sm are submodules. There are no "normal" submodules, we can factorize by any of them. 

\end{enumerate}
\end{Ex}

\textbf{Wednesday, January 10th}

\begin{lem}
Let $\phi:M \to N$ be a hom-sm of $R$-modules. Then ker$\phi$ and $\phi(M)$ are submodules of $M$ and $N$ respectively. 
\end{lem}

\begin{proof}
Recall $K$ is a submodule of $M$ if $K$ is a subgroup of $M$ and $RK \sub K$. So we will show that the two objects in the above remark are submodules. The kernel and the image are groups, if $u \in ker\phi$, then for any $a \in R$, $\phi(au) = a\phi(u) = 0$l, so $au$ is in the kernel. If $v \in \phi(M)$, $v = \phi(u)$, then for any $a \in R$,
$$
av = \phi(au),
$$
so $av \in \phi(M)$. 
\end{proof}

\begin{Def}
A module $M$ is \textbf{simple}, or \textbf{irreducible}, if it has no submodules (except $0$ and itself). 
\end{Def}
There are many simple modules. We will discuss Schur's Lemma. 

\begin{lem}[\textbf{Schur's Lemma}]
If $M,N$ are simple $R$-modules, then any $R$-hom-sm $\phi: M \to N$ is either 0 or an isomorphism. 
\end{lem}

\begin{proof}
The kernel of $\phi$ is a submodule of $M$, so $ker\phi = 0$ or ker$\phi = M$. $\phi(M)$ is submodule, so it is either $0$ or $M$. If ker$\phi = M$, or $\phi(M) = 0$, then $\phi = 0$ (obvious). \\
Otherwise, ker$\phi = 0$ and $\phi(M) = N$, so $\phi$ is an isomorphism. 
\end{proof}
\begin{Cor}
If $R$ is commutative, and $M$ is a simple $R$-module, then End$_R(M) = $ Hom$_R(M,M)$ is a division ring. 
\end{Cor}

The only example of a \textbf{noncommutative division ring} we have is the quaternions:
 $$
\mathbb{H} = \{a + bi + cj + dk:a,b,c,d \in \mathbb{R}\}.
$$

Now we will discuss \textbf{factorization of modules}. 
\begin{Def}
Let $M$ be a module, and $N$ a submodule of $M$, then 
$$
M/N = \{a + N: a \in M\}
$$
has a structure of an $R$-module. 
\end{Def}

Recall that you needed a two-sided ideal to get a quotient ring, but we only need a left ideal to get a quotient module. 

\begin{Ex}
If $R$ is a ring, and $I$ is a left ideal in $R$, then $R/I$ is not a ring, but it is an $R$-module. 
\end{Ex}

\begin{Ex}
Consider space $R$ of square matrices with entries in a set $S$, and the ideal $I$ with zeroes in the first column and arbitrary elements in all other spots. Then $R/I$ is the set of all first columns and is isomorphic to $S^n$. The ideal is left and the resulting quotient is a module, but not a ring. 
\end{Ex}

Observe that $M/N$ is an abelian group. $\overline{u} = u + N \in M/N$. Let $a \in R$, then: 
$$
a\overline{u} = au + aN \subset au + N = \overline{au}.
$$
\textbf{Or:} if $v = u \mod N$, $v - u \in N$, then $av = au \mod N$, $av - au = a(v - u) \in N$. So multiplication by scalars is well defined on $M/N$. 


\begin{theorem}
\textbf{Isomorphism Theorems:}
\begin{enumerate}
\item If $\phi:M \to N$ is an $R$-hom-sm of $R$-modules, then $\phi(M) \cong M/$ker$\phi$ (isomorphic as modules). 
\item Let $N,K$ be submodules of an $R$-module $M$, then 
$$
N + K = \{u + v\}
$$
is a submodule and $(N + K)/K \cong N/(N \cap K)$. 
\item If $N$ is a submodule of $N$ and $K$ is a submodule of $N$, then: 
$$
M/N \cong \frac{(M/K)}{(N/K)}.
$$
\item Submodules of $M/N$ are in bijection with submodules of $M$ containing $N$. The correspondence is:
$$
K \leftrightarrow K/N
$$
where $K \sub M$ and $K/N \sub M/N$. 
\end{enumerate}
\end{theorem}

\begin{rem}
$N + K$ is the smallest module containing both $N$ and $K$. 
\end{rem}

\section*{10.2 Exercises}

\begin{enumerate}[label=\arabic*.]

\setcounter{enumi}{3}
\item \textit{$A$ is a $\z$-module. $H' =Hom(\z_n,A) = $?}

Recall $Hom(\z,A) \cong A$, since from another exercise we have 
$$
Hom(R,M) \cong M
$$
 as $R$-modules, since we map $\phi \in H$ to $\phi(1)$. So we do the same thing. We map $\phi \in H'$ to $\phi(1)$. So we must have $\phi(n) = n\phi(1) = 0$. So $\phi(1)$ must satisfy $n\phi(1) = 0$. So we have $\phi(1) \in $ Ann$(n)$. And this map is injective, $H' \to A$. On the other hand, if $b \in A$, and $nb = 0$, then define $\phi(\overline{k}) = bk$, $k \in \z$, and $\phi \in H'$, where $\overline{k} = k \mod n$. So, Hom$(\z_n,A) \cong \Set{a \in A: na = 0} = \text{Ann}(n)$. 

\textbf{Generalization: }What are $H_1 =$ Hom$(R,M) \cong M$? And what are $H_2 =$ Hom$(R/I,M) \cong$ ? So we must have that $H_2 \sub H_1$. We have: 
$$
H_2 = \Set{u \in M: Iu = 0} = Ann(I). 
$$
Then we map $\phi \mapsto \phi(1)$, and $I$ is sent to zero. 

Now Hom$(R^n,M) \cong M^n$, since we map $\phi \mapsto (\phi(e_1),...,\phi(e_n))$. Or we can use the exercise from the last homework:
$$
Hom(A \oplus B,M) \cong Hom(A,M) \oplus Hom(B,M),
$$
since:
$$
Hom(R^n,M) \cong Hom(R,M)^n \cong M^n.
$$
\textbf{Another one:} Hom$(R^n/I,M) \cong Ann(I) \sub M^n$, where $I$ is an ideal in $R^n$. \\
\textbf{Another one:} Hom$(A,B^n) \cong Hom(A,B)^n$, since we proved that 
$$Hom(A,B \oplus C) \cong Hom(A,B) \oplus Hom(A,C).
$$
\textbf{Another one:} Hom$(R^n,R^m) \cong R^{nm}$. These are $m \times n$ matrices over $R$. We have a basis $e_1,...,e_n \in R^n$ and a basis $\Set{b_i} \sub R^m$. So for any $i$, we have:
$$
\phi(e_i) = c_{1,i}b_1 + \cdots + c_{m,i}b_m.
$$ 
And these coefficients $\Set{c_i}$ are just elements of the matrix. We have a standard basis in this module, which are matrices which are zero everywhere except for one entry, and the value of this entry is $1$ (typical vector space basis over $\R$). We do get a different isomorphism if we change our basis, so is it canonical? It is canonical because $R^n$ has a standard basis, and so does $R^m$ and given these bases, we have a canonical basis for $R^{nm}$. If we deal with an abstract free module, we may not have a standard basis. \\
\textbf{Another one: }Hom$(R,R) \cong R$ as rings. This is easy. Map $\phi \mapsto \phi(1)$. The checking is easy. This is actually the ring End$_R(R)$. Let's check it: 
$$
(\phi\psi)(1) = \phi(\psi(1)) = \phi(\psi(1)\cdot 1) = \psi(1) \phi(1).
$$
We have this last equality because $\phi(1)$ is a scalar element of $R$ and so we can take it out of $\psi$. And We also know End$_R(R^n) \cong M_{n \times n}(R)$. Multiplication is defined so that this is a ring isomorphism. 

\setcounter{enumi}{5}
\item \textit{Prove that Hom$_\z(\z/n\z,\z/m\z) \cong \z/(n,m)\z$. }

\begin{proof}
Let $H = $ Hom$_\z(\z_n,\z_m)$, and let $K = \z/(n,m)$. Also, let $l = \gcd(n,m)$. Then $K = \z_l$. So let $\phi \in H$. Then $\phi:\z_n \to \z_m$. We note here that $\phi$ is completely determined by where it sends $1 \in \z_n$, since we must have $\phi(n\cdot 1) = \phi(0) = 0$ by the definition of a group homomorphism, thus we must have that $n\phi(1) = 0 \in \z_m$. In order to have $n\phi(1) = 0$, we need $\phi(1)$ to be a multiple of $m$. So we need $\phi(1)$ to be a multiple of $m/l$, since every prime factor in $l$ is also in the factorization of $n$, so we need only the prime factors of $m$ which are not in $l$, hence $\phi(1)$ must be a multiple of $m/l$. Now note there are exactly $l$ multiples of $m/l$ in $\z_m$. We denote these $a_0,...,a_{l - 1}$. So we have exactly $l$ distinct homomorphisms in $H$, so we denote these $\phi_0,...,\phi_{l - 1}$, where $\phi_i(1) = a_i = im/l \in \z_m$. Then let $\Phi: H \to K$ be given by:
$$
\Phi(\phi_i) = i \in \z_l.
$$
We prove this map is an isomorphism. 
\textbf{Homomorphism: } Observe: 
$$
\Phi(\phi_i + \phi_j) = \Phi(\phi_{i + j \mod l}) = i + j = \Phi(\phi_i) + \Phi(\phi_j) \in \z_l.
$$
The first equality is by the additive operation on the $\z$-module $H$, and the other equalities follow from the definition of $\Phi$ and the additive operation on $\z_l$. Since $\phi_i$ is a homomorphism of $R$-modules, it preserves multiplication by scalars, so we have $z\phi_i(1) = \phi_i(z) = za_i$, and since $\{a_i\} \cong \z_l$ as a group, we know $za_i = a_{zi \mod l}$. So we have:
$$
\Phi(z\phi_i) = zi = z\Phi(\phi_i) \in \z_l.
$$
So $\Phi$ preserves scalar mult, and hence it is a homomorphism. \\
\textbf{Surjectivity: } Let $i \in \z_l$. Then consider $\psi \in H$ s.t. $\psi(1) = im/l$, but this is exactly how we defined $\phi_i$, so we know $\phi_i = \psi$, and then $\Phi(\psi) = \Phi(\phi_i) = i$. So $\Phi$ is surjective. \\
\textbf{Injectivity: }Let: 
$$
\Phi(\psi) = \Phi(\xi),
$$
then since we enumerated all the elements of $H$, we know we must have $\psi = \phi_i$ and $\xi = \phi_j$ for some $0 \leq i,j \leq l - 1$. Then we have: 
$$
\Phi(\phi_i) = i = j = \Phi(\phi_j) \in \z_l,
$$
so $i \equiv j \mod l$, but since both these numbers are between $0$ and $l - 1$, we know $i  =j$, so $\psi = \xi$, and $\Phi$ is injective. Hence it is an isomorphism. 
\end{proof}

\setcounter{enumi}{8}
\item \textit{Let $R$ be a commutative ring. Prove that Hom$_R(R,M)$ and $M$ are isomorphic as left $R$-modules. [Show that each element of Hom$_R(R,M)$ is determined by its value on the identity of $R$.]}

\begin{proof}
Recall:
$$
H = \text{Hom}_R(R,M) = \{\phi:R \to M\},
$$
where $R$ and $M$ are $R$-modules. Let $\phi \in H$. Recall that from the definition of $H$, we know:
$$
\phi(rs + t) = r\phi(s) + \phi(t),
$$
for all $r,s,t \in R$. So note that $\forall r \in R$, we have:
$$
\phi(r) = r\phi(1_R),
$$
hence $\phi$ is complete determined by its value on $1_R$. Also observe that $\phi(1_R) \in M$, so define a map $\Phi:M \to H$ by $\Phi(m) = \phi_m$, where we define $\phi_m(1_R) = m$. We prove this map is an R-module isomorphism. We first prove it is an $R$-module homomorphism. So let $m,n \in M$, then we have:
$$
\Phi(m) + \Phi(n) = \phi_m + \phi_n
$$
Now we prove surjectivity. So let $\psi \in H$, then $\psi(1_R) = m$ for some $m \in M$, so we know $\psi = \phi_m$. Then note that $\Phi(m) = \phi_m$, so $\Phi$ is surjective. 
\end{proof}

\setcounter{enumi}{10}
\item \textit{Let $A_1,A_2,...,A_n$ be $R$-modules and let $B_i$ be a submodule of $A_i$ for each $i = 1,2,...,n$. Prove that: 
$$
(A_1 \times \cdots \times A_n)/(B_1 \times \cdots \times B_n) \cong (A_1/B_1) \times \cdots \times (A_n/B_n).
$$}

\begin{proof}
So let $A = (A_1 \times \cdots \times A_n)$, $B = (B_1 \times \cdots \times B_n)$, and $C = (A_1/B_1) \times \cdots \times (A_n/B_n)$. Note that: 
$$
A/B = \Set{(a_1,...,a_n) + B}.
$$
We know $B$ is a submodule of $A$ since it is clearly a subset since each component $b_i$ of $(b_1,...,b_n)$ is also in $A_i$. Also: 
$$
(b_1,...,b_n) + r(d_1,...,d_n) = (b_1,...,b_n) + (rd_1,...,rd_n) = (b_1 + rd_1,...,b_n + rd_n),
$$
because of how we defined add. and mult. by $R$ in the $R$ -module $B$, and because each $B_i$ is a submodule of $A_i$. 
Then we know $A/B$ is an $R$-module since we may factorize by any submodule of $A$. , so we let $\phi: A/B \rightarrow C$ be given by $$\phi((a_1,a_2,...,a_n) + B) = (a_1+B_1,a_2+B_2,...,a_n+B_n).$$ 
We prove that $\phi$ is an isomorphism.\\
\textbf{Homomorphism: } Let $(x_1,x_2,...,x_n) + B,(y_1,y_2,...,y_n) + B \in A/B$, then 
\begin{equation}
\begin{aligned}
	&\phi(((x_1,x_2,...,x_n) + B)
	+((y_1,y_2,...,y_n) + B)) \\
	&= \phi(((x_1,x_2,...,x_n)
	+(y_1,y_2,...,y_n)) + B)\\ 
	&= \phi((x_1+y_1,x_2+y_2,...,x_n+y_n) + B) \\
	&= (x_1+y_1+B_1,x_2+y_2+B_2,...,x_n+y_n+B_n)\\
 	&= (x_1+B_1,x_2+B_2,...,x_n+B_n)\\
 	&+(y_1+B_1,y_2+B_2,...,y_n+B_n)\\
  	&= \phi((x_1,x_2,...,x_n) + B)+\phi((y_1,y_2,...,y_n) + B), 
\end{aligned}
\end{equation}
by the direct product operation on $A/B$ and $C$. And for multiplication, we have: 

\bee
	\phi(r((x_1,...,x_n) + B)) 
	&= \phi(r(x_1,...,x_n) + B)\\
	&= \phi(rx_1,...,rx_n) + B)\\
	&= (rx_1 + B,...,rx_n + B)\\
	&= r(x_1 + B,...,x_n + B)\\
	&= r\phi((x_1,...,x_n) + B),
\eee
 so $\phi$ is a homomorphism. \\ 
\textbf{Injection: } Let $(x_1,x_2,...,x_n) + B,(y_1,y_2,...,y_n) + B \in A/B$, and let 
\begin{equation}
\begin{aligned}
\phi((x_1,x_2,...,x_n) + B) &= \phi((y_1,y_2,...,y_n) + B)\\
\Rightarrow  (x_1+B_1,x_2+B_2,...,x_n+B_n)&=(y_1+B_1,y_2+B_2,...,y_n+B_n). 
\end{aligned}
\end{equation}
So then we have that $x_i+B_i = y_i+B_i$ for all $i$, thus
 \begin{equation}
\begin{aligned}(y_1,y_2,...,y_n) + B &= (y_1,y_2,...,y_n)+(B_1 \times B_2 \times \cdots \times B_n)\\
&=
(y_1+B_1 \times y_2+B_2 \times \cdots \times y_n+B_n) \\
&= (x_1+B_1 \times x_2+B_2 \times \cdots \times x_n+B_n) = (x_1,x_2,...,x_n) + B
\end{aligned}
\end{equation}
 by the direct product operation, so $\phi$ is in injective. \\
\textbf{Surjection: } Let $(a_1+B_1,a_2+B_2,...,a_n+B_n) \in C$. Then we must have that $a_i \in A_i$ for all $i$ by definition of $C$ and the quotient modules $A_i/B_i$, so $(a_1,a_2,...,a_n) \in A \Rightarrow (a_1,a_2,...,a_n) + B \in A/B$, and $\phi((a_1,a_2,...,a_n) + B) = (a_1+B_1,a_2+B_2,...,a_n+B_n)$, so $\phi$ is surjective by definition. Hence $\phi$ is an isomorphism, and $A/B \cong C$. 
\end{proof}

\item \textit{Let $I$ be a left ideal of $R$ and let $n \in \n$. Prove: 
$$
R^n/IR^n \cong R/IR \times \cdots \times R/IR
$$. }
\begin{proof}
So we use the first isomorphism theorem. We map $R^n \to (R/I)^n$ by $(a_1,...,a_n) \mapsto (a_1 \mod I,...,a_n \mod I) = (\overline{a_1},...,\overline{a_n}) \in (R/I)^n$. This is clearly surjective. And the kernel is just $I^ = \Set{(a_1,...,a_n): a_i \in I}$. And $I^n = IR^n$, why? \begin{proof}Take:
 $$
 IR^n = \Set{\sum b_i(a_{i,1},...,a_{i,n}):b_i \in I}.
 $$
 And also take note:
 $
 (b_1,...,b_n) \in I$ can be written as:
 $$
 (b_1,...,b_n) = b_1(1,...,0) + \cdots + b_n(0,...,0,1) \in IR^n.
 $$
 So these are the same object. 
 \end{proof}
\end{proof}




\end{enumerate}

\section{Generation of modules, direct sums, and free modules}


Assume $R$ is a unital ring, i.e. that $1 \in R$. 
\begin{Def}
Let $M$ be an $R$-module and $S$ be a subset of $M$. We say that $M$ is \textbf{generated by $S$} if for any $u \in M$, there exists $v_1,...,v_k \in S, a_1,...,a_k \in R$ such that:
$$
u = a_1v_1 + \cdots + a_kv_k,
$$
which is called a \textbf{linear combination} of $v_1,...,v_k$. 
\end{Def}

We could define it another way. 
\begin{Def}
Let $S \sub M$. Then 
$$
RS = \{a_1v_1 + \cdots + a_kv_k: a_i\in R, v_i \in S\}
$$
is the smallest submodule of $M$ containing $S$. $RS$ is called the \textbf{submodule generated by $S$. }
\end{Def}

\begin{rem}
$M$ is generated by $S$ iff $M = RS$. 
\end{rem}

\begin{Def}
The \textbf{free module generated by $S$} is the set of functions $f:S \to R$ s.t. $f(s) = 0$ for all but finitely many $s \in S$. 
\end{Def}
So consider the case where $S$ is finite to simplify the discussion: If $S = \{s_1,...,s_n\}$, the free module is $\{a_1s_1+ \cdots + a_ns_n: a_i \in R,s_i \in S\}$. 


It is the direct sum of $|S|$ copies of $R$. Equivalently, the free module is:
$$
\{a_1s_1 + \cdots + a_ns_n:a_i \in R, s_i \in S\},
$$
the set of formal linear combinations of elements in $S$. Each element in this set corresponds to a function $f:s_i \to a_i$ and maps $s$ to zero if $s \neq s_1,...,s_n$. You should think of this like a free group. 

The difference between the above definitions is the free generated module is the case where $S$ is not a subset of $M$, it is just some random set. 

Let $M$ be an $R$-module, let $S$ be a subset of $M$. let $F$ be the free module generated by $S$, then we have a unique hom-sm $\phi:F \to M$ s.t. $\phi(s) = s$ $\forall s \in S$. 
$$
\phi(a_1s_1 + \cdots + a_ns_n) = a_1s_1 + \cdots + a_ns_n \in M.
$$
On the left hand side inside $\phi$ we see a formal linear combination, the $s$'s are just letters, we forget that they come from a subset of $M$. They are just symbols. On the right hand side, we are in $M$, so we remember that $S \sub M$. 

\begin{Ex}
Let $S = \{2,3\} \sub \z$. And let:
$$
F = \{n\cdot 2 + m \cdot 3\} \cong \z^2,
$$
where in the above we see $2,3$ as just symbols, easily replaceable by $x,y$. Now consider a map $F \to \z$, where $n\cdot 2 + m \cdot 3 \to 2n + 3m$ and on the left hand side of this map, we then remember that $2,3$ are numbers. 
\end{Ex}




If $M$ is generated by $S$, then $\phi$ is surjective, and $M \cong F/ker\phi$. 

\begin{Def}
If $M$ has a finite generated set $S$, then $M$ is \textbf{finitely generated}. 
\end{Def}

\begin{rem}
If $|S| = n$, then $M$ is a factor module of $R^n$. 
\end{rem}

\begin{Def}
$M$ is called \textbf{cyclic} if it is generated by just one element, $M = Ru$ for some $u \in M$. 
\end{Def}
In this case, $M \cong R/I$, I is a left ideal in $R$. We have $\phi:R \to M$ - surjective, which maps $a$ to $au$, and $I = ker\phi$ is a left ideal. Observe: 
$$
I = \{a: au = 0\} = \text{Ann}(u). 
$$
And $au = 0 \Rightarrow \forall b \in R, (ba)u = 0$, so $ba \in I$. But $ab(u) = ?$

\textbf{Thursday, January 11th}

\begin{rem}
When $M$ is cyclic, we know:
$$
M \cong R/I
$$
where $I = \text{Ann}(u) = \{a:au = 0\}$, which is a left ideal. Recall that $u$ is the generator of $M$. 
\end{rem}

\begin{lem}
An abelian group $G$ is cyclic as a $\z$-module if and only if it is cyclic as a group. 
\end{lem}
\begin{proof}
$\exists u \in G$ s.t. $G = \z u = \{nu:n \in \z\}$. 
\end{proof}

\begin{rem}
Let $M$ be an $F$-vector space, and let $T$ be a linear transformation of $M$. Then $M$ is an $F[x]$-module by $xu = Tu$, $u \in M$. 
$$
(a_nx^n + \cdots + a_1x + a_0)u = a_nT^nu + \cdots + a_1Tu + a_0u.
$$
\end{rem}

Also, $M$ is cyclic as an $F[x]$-module if $\exists u$ s.t. $\forall v \in M$, $\exists n,a_i$ s.t. $v = a_nT^nu + \cdots + a_1Tu + a_0u$. 

\begin{Def}
That is, if $u,Tu,T^2u,...$ span $M$, then $u$ is called a \textbf{cyclic vector} for $T$. 
\end{Def}

\begin{lem}
For any simple module $M$ is cyclic
\end{lem}
\begin{proof}
 take any nonzero $u \in M$, then $Ru$ is a nonzero submodule, so $Ru = M$. 
\end{proof}

Converse is not true: $\z_6$ as a $\z$-module is cyclic (generated by 1), but not simple (has a submodule $2\z_6 = \{0,2,4\}$. 

Every group is a factor group of a free group. 

\vspace{3mm}
\textbf{Friday, January 12th}
Now we'll do some exercises. Professor Leibman does Exercises 10.1.5,6 which are completed above. He defines the annihilator of a subset of $M$ again, and proves it is a left ideal. This is Exercise 10.1.9. 

\textbf{Tuesday, January 16th}

Take $R = F[x_1,x_2,...]$. Consider $R$ as a module over itself. It is generated by $1$, since $R = R \cdot 1$. 

Note $I = (x_1,x_2,..)$-submodule of $R$, all polynomials with zero constant term. 

\begin{lem}
$I$ as defined above is not finitely generated. 
\end{lem}


\begin{proof}
Assume that it is finitely generated. So $I = R(f_1,...,f_k)$, for $f_i \in I$, where we assume $f_i$ has zero constant term.  Let $x_1,...,x_n$ be all variables appearing in $f_1,...,f_k$, then any nonzero element of $R(f_1,...,f_k)$ (these are linear combinations of the $f$'s) contains at least one of $x_1,...,x_n$, since $f_i$ has zero constant term. But $I$ is not such, $x_{n + 1} \in I$, so $I \neq R(f_1,...,f_k)$. 
\end{proof}

\begin{Ex}
Let $R = F[x,y]$, and $I = (x,y)$. Then $R$ is an $R$ module, is generated by $1$, and $I$ needs at least two generators. So we can think of this as $R$ being a one "dimensional" module, but $I$ is not one "dimensional". Let
$$
f = ax + by + g(x,y),
$$
$\forall g \in R$, $gf = c(ax + by) + (\cdots)$. We are assuming $a,b \neq 0$. The linear part of $I$ is two "dimensional". So
$$
\{g \in R, gf = c(ax + by) + (\cdots)\} \neq I. 
$$
If $b \neq 0$, $x \notin Rf$, and if $a \neq 0$, $y \notin Rf$. 
\end{Ex}

Consider $M_1 \oplus M_2 = M_1 \times M_2$ is called direct sum = direct product, for two or finitely many modules. And this direct sum has the universal repelling property (appendix A). 

For $M_1,M_2,...$. 

\begin{Def}
Direct product $M_1 \times M_2 \times \cdots = \Pi_{i = 1}^\infty M_i$ is 
$$
M = \{(u_1,u_2,...): u_i \in M_i\},
$$
with $(u_1,u_2,...) + (v_1,v_2,...) = (u_1 + v_1,u_2 + v_2,...)$. And scalar mult. is defined as one would expect. 


\end{Def}

More generally, if $M_\alpha$ $\alpha \in \Lambda$ are modules, then $\Pi_{\alpha \in \Lambda} = \{f: \Lambda \to \cup_{\alpha + \Lambda}M_\alpha: f(\alpha) \in M_\alpha\}$. For any $\alpha$, choose $u_\alpha \in M_\alpha$. 

Elements: $(u_\alpha)_{\alpha \in \Lambda}$. 

Direct Sums: 
$$M_1 \oplus M_2 \oplus \cdots = \{(u_1,u_2,...): u_i \in M_i,u_i = 0, \text{ for all but finitely many } i\}.
$$
 This is a subset of $M_1 \times M_2 \times \cdots$. 

Another way: 

$M_1 \oplus M_2 \oplus \cdots = \{u_{i_1} + u_{i_2} + \cdots + u_{i_k}:u_{i_j} \in M_{i_j}\}$. This is a submodule of $M_1 \times M_2 \times$. 

For $M_\alpha$, $\alpha \in \Lambda$, 

$$
\bigoplus_{\alpha \in \Lambda} M_\alpha = \{f: \Lambda \to \cup M_\alpha: \forall \alpha \in \Lambda, f(\alpha) \in M_\alpha, f(\alpha) = 0 \text{ for all but finitely many } \alpha\}.
$$ 
This is a superset of $\Pi_{\alpha \in \Lambda}M_\alpha$. 

Universal Properties: 

$R$-modules $M_\alpha$, $\alpha \in \Lambda$. 

\begin{enumerate}
\item Direct Product: 
Category: objects are (Module $N$ with hom-sms $\psi_\alpha: N \to M_\alpha$, $\forall \alpha \in \Lambda$. 

Morphisms: given two objects 
$(N_1,\psi_\alpha:N \to M_\alpha)$

$(N_2,\psi_\alpha:n \to M_\alpha)$
a morphism is a hom-sm $\phi: N_1 \to N_2$ 
s.t. $\psi_\alpha = \phi_\alpha \phi$ $\forall \alpha$. 

Direct product is universal attracting object. 
\end{enumerate}

\textbf{Wednesday, January 17th}

\begin{theorem}[\textbf{Chinese Remainder Theorem for Modules}]
Let $R$ be a commutative unital ring, $I_1,...,I_n$ be pairwise comaximal ideals in $R$:
$$
(I_i + I_j = (1),\forall i\neq j),
$$
and $M$ be a an $R$-module. Then the homomorphism $M \to M/I_1M \oplus \cdots \oplus M/I_nM$ given by $u \to (u \mod I_1M,...,u \mod I_nM)$ induces an isomorphism $M/(I_1\cdots I_n)M \to M/I_1M \oplus \cdots M/I_nM$ and $I_1M \cap \cdots \cap I_nM = (I_1 \cdots I_n)M$. 
\end{theorem}
\begin{proof}
For $n = 2$. $I_1 + I_2 = (1)$. 

Define $\phi: M \to M/I_1M \oplus M/I_2M$, where ker$\phi = I_1M \cap I_2M$. And there exists $a_1 \in I_1,a_2 \in I_2$ s.t. $a_1 + a_2 = 1$, since comaximal. And $\forall u \in I_1M \cap I_2M$, we have:
$$
u = 1u = a_1u + a_2u.
$$
So, $I_1M \cap I_2M \sub I_1I_2M$, since the first term in the righthand side above is in $I_1(I_2)M$ and so is the second term, by commutativity. Also, $I_1I_2M \sub I_2M \cap I_2M$. So $I_1I_2M = I_1M \cap I_2M = \text{ker}\phi$. 

\textbf{Surjectivity: } $\forall u_1,u_2 \in M$, put $u = a_2u_1 + a_1u_2$. Then:
$$
u = (1 - a_1)u_1  + a_1u_2 = u_1 + a_1(u_2 - u_1) = u_1 \mod I_1M,
$$
and $u = u_2 \mod I_2M$. So $\phi(u) = (\overline{u}_1,\overline{u}_2)$. 

Now for $n \geq 3$, we use induction. 
\begin{lem}
$I_1$ and $I_2\cdots I_n$ are comaximal. 
\end{lem}
\begin{proof}
$\forall i = 2,...,n$, let $a_i \in I_1,b_i \in I_i$ be s.t. $a_i + b_i = 1$. Then:
$$
1 = \prod(a_i + b_i) = (something ) + b_2\cdots b_n,
$$
where something is in $I_1$ and $b_2\cdots b_n \in I_2\cdots I_n$. 
\end{proof}
Then:
$$
M/(I_1I_2 \cdots I_n)M \cong M/I_1 \oplus M/(I_2\cdots I_n)M \cong \cdots \cong M/I_1M \oplus \cdots \oplus M/I_nM
$$
by induction, where the last $\cong$ is under the mapping $$
u \to (u \mod I_1M ,...,u \mod I_nM).
$$
\end{proof}



\begin{Def}
$M_1,M_2$ are submodules of $M$. $M$ is said to be an internal direct sum $M = M_1 \oplus M_2$ if there exists a $\phi: M \to M_1 \oplus M_2$ where we also have maps in a diamond up to $M_1$ and down to $M_2$ s.t. $\phi|_{M_1} = Id_{M_1}$ and the same for $M_2$. 
\end{Def}

\begin{rem}
We say that we have an \textbf{internal direct sum} if the above map exists. The "internal" means that we are working entirely inside a parent module $M$. 
\end{rem}

\begin{theorem} \label{thm10.67}
Let $M_1,M_2$ be submodules of $M$. Then $M = M_1 \oplus M_2$ (\textbf{internal direct sum}) if and only if $\forall u \in M$ is uniquely representable in the form $u = u_1 + u_2$ s.t. $u_1 \in M_1$, $u_2 \in M_2$ if and only if $M = M_1 + M_2$ and $M_1 \cap M_2 = 0$. These are all equivalent definitions. 
\end{theorem}
Let $M_\alpha$, $\alpha \in \Lambda$ be submodules of$M$. $M$ is an (internal) direct sum of $M_\alpha$:
$$
M = \bigoplus_{\alpha \in \Lambda}M_\alpha,
$$
if there exists an isomorphism $\phi: M \to \bigoplus_{\alpha \in \Lambda}M_\alpha$ where the target space is the external, formal direct sum, s.t. $\phi|_{M_\alpha} = Id_{M_\alpha}, \forall \alpha$. This is so if and only if $\forall u \in M$ is uniquely representable as $u = \sum_{i = 1}^k u_{\alpha_i}$ for some distinct $\alpha_1,...,\alpha_k$ where $u_{\alpha_i} \in M_{\alpha_i},\forall i$ and if and only if $M = \sum M_\alpha$ and $\forall \alpha$, $M_\alpha \cap \sum_{\beta \neq \alpha}M_\beta = 0$. 

\textbf{Free Modules. }
\begin{Def}
A \textbf{free module} is a direct sum of finitely or infinitely many copies of $R$, 
$$
F_\Lambda = \bigoplus_{\alpha \in \Lambda}R = \{a_{\alpha_1} + \cdots + a_{\alpha_k}: k \in \n,\alpha_i \in \Lambda,a_{\alpha_i} \in R\},
$$
where the sum of $u$'s above is a formal sum. We can also define it as: 
$$
F_\Lambda = \{(a_\alpha)_{\alpha \in \Lambda}: a_\alpha \in R,\forall \alpha,a_\alpha = 0 \text{ for all but finitely many }\alpha\}.
$$
Note $R$ is unital here. 
\end{Def}
If $M$ is an $R$-module, $v_\alpha \in M,\alpha \in \Lambda$. Then there exists a unique hom-sm $\phi: F \to M$ s.t. $\phi(e_\alpha) = v_\alpha,\forall \alpha$ where $e_\alpha = (a_\beta)_{\beta \in \Lambda},a_\alpha = 1,a_\beta = 0$ for $\beta \neq \alpha$. As an example, if $\Lambda = \{1,...,k\}$, we have: 
$$
e_1 = (1,0,...,0),
$$
$$
e_k = (0,...,0,1). 
$$
Also, we say that a module $M$ is free if $M \cong$ a free module $F$. This is so if any only if $M$ has a \textbf{basis}: elements $u_\alpha,\alpha \in \Lambda$.  
 
\vspace{5mm}
\textbf{Thursday, January 18th}

\vspace{5mm}
Let $R$ be a unital ring. Recall that a \textbf{free module} is $\bigoplus_{\alpha \in \Lambda} R$. If $\Lambda$ is finite, $\Lambda = k$, then this is just $R^k$. 
\begin{Def}
Our \textbf{standard basis} in $R^k$ is 
$$
e_1 = (1,0,...,0),
$$
$$
e_k = (0,...,0,1). 
$$
\end{Def}
\begin{Def}
The \textbf{rank} of the free module is $k$. 
\end{Def}

In $F = \oplus_{\alpha \in \Lambda} R$, the standard basis is $e_\alpha = (a_\beta)_{\beta \in \Lambda},a_\alpha = 1,a_\beta = 0$ for $\beta \neq \alpha$. 
\begin{Def}
For any $u \in F$, $u= \sum_{\alpha \in \Lambda}a_\alpha e_\alpha$, $a_\alpha \in R$ uniquely, where $a_\alpha =0$ for all but finitely many $\alpha$. (Namely, $u = (a_\alpha)_{\alpha \in \Lambda}$). 
\end{Def}

\begin{rem}
For a basis the representation must be unique, for generators, it does not. 
\end{rem}
\begin{Def}
Also, if $M \cong F$ for some $\Lambda$, $M$ is called \textbf{free of rank} $|\Lambda|$. 
\end{Def}
\begin{rem}
$M$ is free if and only if it has a basis: a set$\{u_\alpha, \alpha \in \Lambda\} \sub M$ s.t. every $u$ in $M$ is uniquely representable in the form:
$$
u= \sum_{\alpha \in \Lambda}a_\alpha u_\alpha,
$$
where the $a$'s are zero for all but finitely many of them. 
\end{rem}

\begin{Def}
A set $\{u_\alpha, \alpha \in \Lambda\}$ in a module $M$ is \textbf{linearly independent} if $\sum_{\alpha \in \Lambda}a_\alpha u_\alpha = 0$ only if $a_{\alpha_i} = 0$ for all $i$. In other words, a linear combination is zero if and only if all the coefficients are zero. 
\end{Def}

\begin{rem} \label{rem1076}
A set $\{u_\alpha, \alpha \in \Lambda\}$ is a basis in $M$ if and only if it is linearly independent and generates $M$. (It is unique since if we have two representations $u = \sum_{\alpha \in \Lambda}a_\alpha u_\alpha = \sum_{\alpha \in \Lambda}b_\alpha u_\alpha$, then $\sum_{\alpha \in \Lambda}(a_\alpha - b_\alpha) u_\alpha = 0$ so by linear independence, all the differences of coefficients are zero.)
\end{rem}

\begin{theorem}
Any vector space is a free module. More generally, if $R$ is a division ring, the any $R$ module is free. 
\end{theorem}
\begin{proof}
Let $M$ be a nonzero $R$-module.
Take the maximum linearly independent set $B$ in $M$. It exists by Zorn Lemma. Indeed, take a nonzero element $u \in M$, then $\{u\}$ is linearly independent (proof is elementary, requires that $R$ is a division ring). If you have a chain/subset tower of linearly independent sets, then their union is linearly independent, by Zorn Lemma. If $\mathcal{B}$ is a chain of linearly independent sets in $M$, the n $\bigcup \mathcal{B}$ is linearly independent. If 
$$
u_1,...,u_n \in \bigcup \mathcal{B},
$$
$$
\sum_{i  = 1}^na_i u_i = 0,
$$
 find $C \in \mathcal{B}$ s.t. these $u$'s are all in $C$, $C$ is linearly independent, so $a_i = 0$ for all $i$. So Zorn applies, and $B$ exists. Now we claim that $B$ generates $M$ so $B$ is a basis by Remark 10.76. 
 \begin{proof}
 Let $u \notin RB$. Then $B\cup \{u\}$ is linearly independent, contradiction, since then it would be bigger than $B$ but $B$ is maximal. 
 Indeed, if: 
 $$
 au + \sum_{i  = 1}^k a_i u_i = 0,
 $$
  for some $u_{\alpha_i} \in B$ for all $i$. If $a = 0$, then all $a$'s are zero, since $B$ is linearly independent. If $a$ is note zero, then $u = -a^{-1}\sum_{i  = 1}^k a_i u_i\in RB$, which is impossible, since we said $u \notin RB$ at beginning of claim. 
   \end{proof}
\end{proof}

\begin{rem}
It is impossible for all of $M$ to be linearly independent since we have $u$ and $2u$ which are clearly dependent. 
\end{rem}

\begin{Ex}
$\R$ is a vector space over $\mathbb{Q}$. We need Hamel basis. We start with $\{1,\alpha_1,\alpha_2,...\}$, you need to get more than countably many. This has something to do with Zorn Lemma. The process cannot be defined with an algorithm. 
\end{Ex}

\begin{Def}
The rank of a vector space is called the \textbf{dimension}. 
\end{Def}

\begin{theorem}
The dimension of a vector space is uniquely defined: If $R$ is a field, or a division ring, then $R^n \ncong R^m$ for $n \neq m$. This is for finite  dimension, but for infinite dimensions it is also true. 
\end{theorem}

\begin{proof}
\textbf{Finite Case: } Let $R$ be a division ring, let $M$ be an $R$-module, let $\{u_1,...,u_n\}$, and $\{v_1,...,v_m\}$ be two bases in $M$. We claim $m = n$. 
\begin{proof}
Assume that $n \geq m$. We have $v_1 = a_1u_1 + \cdots a_nu_n$ for some coefficients not all zero. Without loss of generality, assume that the first $a_1$ is nonzero. Then:
$$
u_1 = a_1^{-1}v_1 - a_1^{-1}a_2u_2 - \cdots - a_1^{-1}a_nu_n.
$$
Then $R\{v_1,u_2,...,u_m\} = M$: if 
$$
v = \sum_{i = 1}^n b_i u_i = b_1u_1 + \sum_{i = 2}^n b_iu_i = b_1(a_1^{-1}v_1 - a_1^{-1}a_2u_2 - \cdots) + \sum b_i u_i = cv_1 + \sum_{i = 2}^n c_i = u_i.
$$
And $\{v_1,u_2,...,u_n\}$ is linearly independent: if 
$$
cv_1 + c_2u_2 + \cdots + c_nu_n = 0 = c(a_1u_1 + \cdots a_nu_n) + c_2u_2 + \cdots + c_nu_n = ca_1 u_1 + \sum{i = 2}^n d_iu_i,
$$
then $ca_1 = 0$, so $c = 0$, so all $c$'s are zero. Hence $\{v_1,u_2,...,u_n\}$ is a basis. Next, replace one of $u_2,...,u_n$ by $v_2$, and without loss of generality, replace $u_2$ with $v_2$. And after $m$ steps, we see that $\{v_1,...,v_m,u_{m + 1},...,u_n\}$ is a basis. But $\{v_1,...,v_m\}$ is a basis, so by definition those extra $u$'s don't exist, and $n = m$. 
\end{proof}
\begin{rem}
If $R$ is commutative and unital, then the rank of a free $R$-module is well defined: 
$$
R^n \ncong R^m,
$$
where $n \neq m$. 
\end{rem}
\begin{proof}
Let $I$ be a maximal ideal in $R$, exists by Zorn Lemma. The $R/I$ is a field. Then $R^n/(IR^n) \cong (R/I)^n = F^n$, where $R^n$ is the free module of rank $n$. And $R^m/(IR^m) \cong F^m$. And $F^n \ncong F^m$ since dimension is well defined, so $R^n \ncong R^m$. 
\end{proof}
\end{proof}

\textbf{Friday, January 19th}
\vspace{5mm}
We do exercises from Section 10.2,3. 

\begin{Def}
An $R$-module is called a \textbf{torsion module} if for each $m \in M$, there exists a nonzero $r \in R$ s.t. $rm = 0$. 
\end{Def}


\textbf{Monday, January 22th}
\vspace{5mm}

\begin{rem}
Any module $M$ has a maximal linearly independent set $B$ of elements, by Zorn Lemma. 
\end{rem}


\begin{lem}
If $M$ is a torsion module, this set is empty. 
\end{lem}

\begin{proof}
Any element is not linearly independent if you take it alone. $\forall u$ $\exists a \neq 0$, s.t. $au = 0$. 
\end{proof}

However, $B$ doesn't have to generate $M$. So what can we say about the submodule generated by $B$. The submodule $RB$ has as basis $(B)$, a linearly independent system which generates this module. \textbf{So, it's free. } And in fact: 

\begin{rem}
It is the maximal free submodule: when you factorize by this submodule, you get a torsion ring. 
\end{rem}

\begin{lem}
$M/RB$ is a torsion module if and only if $B$ is a \textbf{maximal linearly independent set}. 
\end{lem}
\begin{proof}
We assume $R$ is unital, since otherwise, $B$ may not be in $RB$. Or we could define $RB$ as $RB \cup B$. 
Indeed, if $\exists u \in M$ s.t. $\overline{u} \equiv u \mod RB$ is not a torsion element, this means that $au \notin RB$ $\forall a \neq 0 \in R$. This is because "0" in the quotient module is the kernel, $RB$ so $au$ cannot be in $RB$. Then if: 
$$
au + c_1v_1 = \cdots c_kv_k = 0,
$$
 with $v_i \in B, c_i \in R, a \in R$, then $a = 0$, since if $a$ was nonzero, then $au = -c_1v_1 - \cdots - c_kv_k \in RB$. so $c_1v_1 + \cdots c_kv_k = 0$, so $c_i = 0$ for all $i$, so $\{u\}\cup B$ is linearly independent, contradiction, since $B$ was the largest linearly independent set in $M$. 
 
 For any $u$, there exists a nonzero $a$ s.t. $a\bar{u} \in RB$, so $au + c_1v_1 + \cdots c_kv_k = 0$ for some $c_i \in R,v_i \in B$. 
\end{proof}

\vspace{5mm}
\begin{Ex}
Let $R = F[x,y], M = (x,y)$. The lines represent the ideals $(x),(y)$, and the empty box in the bottom left corner are just the constants. 





$B = \Set{x}$. $RB = (x)$. And $M/RB = M/(x)$. 
\end{Ex}



\section*{10.3 Exercises}
\begin{enumerate}[label=\arabic*.]
\setcounter{enumi}{6}
\item \textit{Let $N$ be a submodule of $M$. Prove that if both $M/N$ and $N$ are finitely generated, then so is $M$. }
\begin{proof}
Suppose $M$ is not finitely generated. Then we have:
$$
M/N = RA,
$$
where $A = \{x_1 + N,...,x_n + N\}$. And since $N$ is also finitely generated, we know $N = RA_N$, and $M -N$ is not finitely generated. 
Now we know $x_i \in M - N$ since otherwise we would have $x_i + N = N$. So then since $M$ is not finitely generated, we know $\exists y \in M - N$ s.t. $y \notin R\{x_i\}$, hence $y + N \notin RA = \{(rx_1) + N,...,(rx_n)+ N\}$, but since $y \in M - N$ we know $y + N \neq N$, hence $y + N \in M/N$. But we said $M/N = RA$, so this is a contradiction, so we must have that $M$ is finitely generated. 
\end{proof}

\setcounter{enumi}{11}
\item \textit{Let $R$ be a commutative ring and let $A,B$, and $M$ be $R$-modules. Prove the following isomorphisms of $R$-modules: }
\begin{enumerate}
\item \textit{$Hom_R(A\times B,M) \cong Hom_R(A,M) \times Hom_R(B,M)$.}
\begin{proof}
Let $H = \text{Hom}_R(A\times B,M)$, $H_A = \text{Hom}_R(A,M)$, and $H_B = \text{Hom}_R(B,M)$. Let $\Phi: H_A \times H_B\to H$ be given by $\Phi((\phi,\psi)) = \phi + \psi$, where $\phi \in H_A,\psi \in H_B$. We prove this is an isomorphism of $R$-modules. 

\textbf{Homomorphism: }Observe: 
\begin{equation}
\begin{aligned}
\Phi((\phi_1,\psi_1) + (\phi_2,\psi_2)) &= \Phi((\phi_1 + \phi_2,\psi_1 + \psi_2)) = \phi_1 + \psi_1 + \phi_2 + \psi_2\\ &= \Phi((\phi_1,\psi_1)) + \Phi((\phi_2,\psi_2)).
\end{aligned}
\end{equation}

In the above expression, the first equality comes from the definition of addition in $H_A \times H_B$. The second and third equalities comes from the definition of $\Phi$. And we also know: 
$$
\Phi(r(\phi,\psi)) = \Phi((r\phi,r\psi)) = r\phi + r\psi = r(\phi + \psi) = r\Phi((\phi,\psi)),
$$
hence $\Phi$ preserves mult. by $R$, by the definition of scalar multiplication on the $R$-module $H_A \times H_B$, and the definition of $\Phi$. 

\textbf{Surjectivity: } Let $\phi \in H$. Then $\phi:A\times B \to M$. So let $\phi \in H_A$ be given by $\phi(a) = \phi(a,0)$,
and let $\psi \in H_B$ be given by $\phi(b) = \phi(0,b)$. Then we have: $\Phi((\phi,\psi)) = \phi$.  Then $\Phi$ is surjective. 


\textbf{Injectivity: } Let $\Phi((\phi_1,\psi_1)) = \phi_1 + \psi_1 = \phi_2 + \psi_2 = \Phi((\phi_2,\psi_2)) \in H_A \times H_B$. Then note that 
$$
(\phi_1 + \psi_1)(a,0) = \phi_1(a) = \phi_2(a) = (\phi_2 + \psi_2)(a,0),
$$
and the same holds when we let $a = 0$, and use an arbitrary $b$ value, so we get that $\psi_1 = \psi_2$ as well. Hence $\Phi$ is injective. And thus it is an isomorphism. 
\end{proof}
\item \textit{$Hom_R (M,A \times B) \cong Hom_R(M,A) \times Hom_R(M,B)$.}
\begin{proof}
Let $H = \text{Hom}_R(M, A\times B)$, $H_A = \text{Hom}_R(M,A)$, and $H_B = \text{Hom}_R(M,B)$. Let $\Phi:H_A \times H_B \to H$ be given by $\Phi((\phi,\psi)) = (\phi,\psi) \in H$, where $\phi \in H_A$, and $\psi \in H_B$. We prove this map is an isomorphism. 

\textbf{Homormorphism: }Observe: 
\begin{equation}
\begin{aligned}
\Phi((\phi_1,\psi_1) + (\phi_2,\psi_2)) &= \Phi((\phi_1 + \phi_2,\psi_1 + \psi_2)) = (\phi_1 + \phi_2,\psi_1 + \psi_2)\\ &=(\phi_1,\psi_1) + (\phi_2,\psi_2) =  \Phi((\phi_1,\psi_1)) + \Phi((\phi_2,\psi_2)).
\end{aligned}
\end{equation}
The first equality follows from addition in the $R$-module $H_A \times H_B$, the second comes from the definition of $\Phi$, the third comes from addition in $H$, and the last again comes from the definition of $\Phi$.  And we also know: 
$$
\Phi(r(\phi,\psi)) = \Phi((r\phi,r\psi)) = (r\phi,r\psi) = r(\phi , \psi) = r\Phi((\phi,\psi)),
$$
by the definition of scalar mult. in $H$, hence since $\Phi$ preserves addition and scalar multiplication, we know it is a homomorphism. 

\textbf{Surjectivity: } Let $\phi \in H$, then we know $\phi: M \to A \times B$. Then the image of any element of $M$ under $\phi$ is a two dimensional vector whose first component lives in $A$, and whose second component lives in $B$. So let $\phi:M \to A$ be given by $\phi(m) = \phi(m)_1$, the first component of $\phi(m)$. and let $\psi(m) = \phi(m)_2$. Then $\Phi((\phi,\psi)) = (\phi,\psi) = \phi$. Hence $\Phi$ is surjective. 

\textbf{Injectivity: } Let $\Phi((\phi_1,\psi_1)) =(\phi_1,\psi_1) = (\phi_2,\psi_2) =  \Phi((\phi_2,\psi_2))$. Then we must have $\phi_1 = \phi_2$, and $\psi_1 = \psi_2$, since otherwise we do not have equality of these ordered pairs of hom-sms in $H$. But then we have shown that the arguments of $\Phi$ are equal in this case, so $\Phi$ must be injective. 
\end{proof}
\end{enumerate}

\setcounter{enumi}{14}
\item \textit{An element $e \in R$ is called a \textbf{central idempotent} if $e^2 = e$ and $er = re$ for all $r \in R$. If $e$ is a central idempotent in $R$, prove that $M = eM \oplus (1 - e)M$. }

\begin{proof}
So we wish to show that $M$ is the direct sum of the two specified submodules. Note that we know that these sets are both submodules by Exercise 14 of Section 1, which tells us that $zM$ is a submodule for any $z$ in the center of $R$. We know $e$ is in the center since it is a central idempotent. And $(1 - e)r = r - er = r - re = r(1 - e)$. So it is also in the center. Now we need only show that $M = eM + (1 - e)M$, and that $eM \cap (1 - e)M = 0$. 


Let $m \in M$. Then $m = em + (1 - e)m = em + m - em$, where $em \in eM$, and $(1 - e)m \in (1 - e)M$, so $m \in eM + (1 - e)M$. Now let $em + (1 - e)n \in eM + (1 - e)M$. Then we have $em + n - en = n + e(m - n)$. So we know $M = eM + (1 - e)M$. So let $m \in eM \cap (1 - e)M$. Then $m = en_1 = (1 - e)n_2$ for some $n_1,n_2 \in M$. Then we have: 
$$
m = en_1 = (1 - e)n_2 = e^2n_1 = e(1 - e)n_2 = (e - e^2)n_2 = (e - e)n_2 = 0,
$$
so we have shown that if $m \in eM \cap (1 - e)M$, $m = 0$, so $eM \cap (1 - e)M = 0$. And thus $M = eM \oplus (1 - e)M$ by definition. 
\end{proof}

\setcounter{enumi}{17}
\item \textit{Let $R$ be a PID, let $M$ be an $R$-module, and assume that $aM = 0$ for some $a \neq 0$ where $a \in R$. Let:
$$
a = p_1^{r_1}\cdots p_k^{r_k},
$$
distinct primes in $R$ $\forall i$, and let: $$
M_i = Ann(p_i^{r_i}) = \{u \in M: p_i^{r_i}u = 0\}.
$$
 Then $M = M_1 \oplus \cdots \oplus M_k$. }
 
  \begin{proof}
 $\forall i$, let $a_i = a/p_i^{r_i} (= \prod_{j \neq i}p_j^{r_j})$. Then $a_iM \sub M_i$, since 
 $$
 p_i^{r_i}(a_iM) = aM = 0
 $$
  (by assumptions of theorem). Then: 
 $$
 gcd(a_1,...,a_k) = 1,
 $$
 so there exists $c_1,...,c_k \in R$ s.t. $c_1a_1 + \cdots + c_ka_k = 1$. So $\forall u \in M$,
 $$
 u = c_1a_1u + \cdots + c_ka_ku \in M_1 + \cdots M_k.
 $$
 Now let $u \in M_i \cap (\sum_{j \neq i}M_j)$. Then $p_i^{r_i},a_i \in Ann(u)$. So, $(p_i^{r_i}) = (1) \sub Ann(u)$, so $u = 0$. So $\forall i,M_i \cap (\sum_{j \neq i}M_j) = 0$. 
 \end{proof}
 \setcounter{enumi}{21}
 \item \textit{Let $R$ be a Principal Ideal Domain, let $M$ be a torsion $R$-module, and let $p$ be a prime in $R$ (do not assume $M$ is finitely generated, hence it need not have a nonzero annihilator). The \textbf{p-primary component of $M$} is the set of all elements of $M$ that are annihilated by some positive power of $p$. }
 \begin{enumerate}
 \item \textit{Prove that the $p$-primary component is a submodule. }
 \begin{proof}
 Let $N$ denote the $p$-primary component of $M$. Note that: 
 $$
 N = \Set{m \in M: \exists k\in \n,p^km = 0}. 
 $$
We apply the submodule criterion. Note that $N \neq \varnothing$ since $0 \in N$. Let $x,y \in N$, and let $r \in R$. Then we know $\exists k,l \in \n$ s.t. $p^kx = p^ly = 0$. Observe: 
$$
p^kp^l(x + ry) = p^lp^kx + rp^kp^ly = p^l0 + rp^k0 = 0,
$$
so we know $x + ry \in N$, hence by the submodule criterion, $N$ is a submodule of $M$. 
 \end{proof}
 
 \item \textit{Prove that this definition of $p$-primary component agrees with the one given in Exercise 18 when $M$ has a nonzero annihilator. }
 \begin{proof}
 Assume $M$ has a nonzero annihilator $a$, and this is the minimal such element. Then let $p^\alpha$ be a prime power factor in the prime factorization of $a$. Let:
 $$
 N = \Set{m \in M: \exists k\in \n,p^km = 0}.
 $$
 In Exercise 18, the definition given for the annihilator of $p^\alpha$ is: 
 $$
 A = Ann_M(p^\alpha) = \{m \in M: p^\alpha m = 0\}.
 $$
 So clearly any element of $A$ is in $N$; just let $k =\alpha$. So let $m \in N$. Then $\exists k \in \n$ s.t. $p^km = 0$. Suppose $k > \alpha$. Then since $am = 0$, we must have some other product of primes $r = r_1\cdots r_l\mid a$ s.t. $r \nmid p^{\alpha}$. But since we proved that $N$ is a submodule in part (a), we know $Ann(N) = \Set{r \in R: rm = 0,\forall m \in N}$ is an ideal in $R$. Note then that $r,p^k \in Ann(N)$. But since $p^k \nmid r$ since otherwise we would have $p^k \mid a$, which is impossible since we said $r > \alpha$. So then $r \notin Rp^k$, hence $Ann(N)$ is not a principal ideal, but this is impossible, since we are in a PID, so we must have $k \leq \alpha$. Hence $m \in A$, and thus $N \sub A$, and the definitions are equivalent, because the sets are equal. 
 
\begin{comment}
  \textbf{This is sketchy, couldn't you just have that $m$ is also annihilated by some power of some other prime in $a$?}
 
 \textbf{The case of finitely many components (when $M$ is annihilated by a nonzero element of $R$) was considered in class; in fact, the general case can be reduced to it, since $M$ is a torsion module and thus every element of $M$ is contained in an annihilator submodule of some nonzero element of $R$.}

\textbf{The annihilator of a single element $m$ need not be a power of a prime element of $R$ -- a direct sum of submodules is not the union of these submodules, and $m$ does not have to belong to one of these submodules. You have to show that $m$ is a sum, $m=m_{1}+...+m_{k}$ where for each $i$, $m_{i}$ is contained in a primary component of $M$.}
\end{comment}
 \end{proof}
 
 \item \textit{Prove that $M$ is the (possibly infinite) direct sum of its $p$-primary components $\Set{M_i}$, as $p$ runs over all primes of $R$. }
 
 \begin{proof}
 Let $\Set{p_i}$ be all the primes in $R$. $\forall i$, let $a_i = \prod_{j \neq i}p_j^{r_j}$. Then $a_iM \sub M_i$, since $p_i^{r_i}(a_iM) = \prod_{j = 1}^\infty p_j^{r_j}M = 0$ (since $M$ is a torsion module, and hence $\forall m \in M$ there exists a nonzero $r \in R$ s.t. $rm = 0$, and the prime decomposition of $r$ is in $\prod_{j = 1}^\infty p_j^{r_j}$). Then: 
 $$
 gcd(a_1,a_2,...) = 1,
 $$
 so there exists $c_1,c_2,... \in R$ not necessarily all nonzero s.t. $c_1a_1 + \cdots = 1$. So $\forall u \in M$,
 $$
 u = \sum_{i = 1}^\infty c_ia_i \in M_1 + M_2 + \cdots.
 $$
 Now let $u \in M_i \cap (\sum_{j \neq i}M_j)$. Then $p_i^{r_i},a_i \in Ann(u)$. So, $(p_i^{r_i}) = (1) \sub Ann(u)$, so $u = 0$. So $\forall i,M_i \cap (\sum_{j \neq i}M_j) = 0$. So since we know $M = M_1 + M_2 + \cdots$, and the pairwise intersection of each of these is $0$, we know that $M = M_1 \oplus M_2 \oplus \cdots$. 
 \end{proof}
 
 \end{enumerate}
 
 \setcounter{enumi}{26}
 \item \textit{We show that \textbf{free modules over noncommutative rings need not have a unique rank.} Let:
$$
M = \z^{n} = \Set{(a_1,...,a_n): a_i \in \z}.
$$
Let $R = End_\z(M)$. Consider $R$ as a module over itself. It is a free module of rank 1. We claim $R \cong R^2$. And so we would have $R \cong R^n$ for any $n$. 
 }
 
 \begin{proof}
 Consider $\phi_1,\phi_2,\psi_1,\psi_2 \in R$. Define: 
 \bee
 \phi_1(a_1,a_2,...) &= (a_1,a_3,a_5,...)\\
 \phi_2(...........) &= (a_2,a_4,...)\\
 \psi_1(...........) &= (a_1,0,a_2,0,...)\\
 \psi_2(...........) &= (0,a_1,0,a_2,...)
 \eee
 We claim that $\Set{\phi_1,\phi_2}$ is a basis of $R$ as an $R$-module, so $R \cong R^2$ as $R$-modules. \textbf{Why this implication!! Ask after class.} The general situation: $M$ is $R$ module, $u_1,...,u_n$ is basis in $M$. Then every element of $M$ is a linear combination uniquely. Then $M \cong R^n$ under isomorphism $u \mapsto (a_1,...,a_n)$, the coefficients of the unique linear combination representing $u$. We have:
 $$
 \phi_1\psi_1 = \phi_2\psi_2 = 1,
 $$
 $$
 \phi_1\psi_2  =\phi_2\psi_1 = 0,
 $$
 $$
 \psi_1\phi_1 + \psi_2\phi_2 = 1.
 $$
 These can be checked easily. Any $\phi = \phi 1 = (\phi\psi_1)\phi_1 + (\phi\psi_2)\phi_2)$. So $\phi_1,\phi_2$ generate $\phi$. If $\beta_1\phi_1 + \beta_2\phi_2 = 0$, then $\beta_1\phi_1\psi_1 + \beta_2\phi_2\psi_1 = 0$. So we get $\beta_1 = 0$. And to get $\beta_2 = 0$, we multiply on the right by $\psi_2$ instead of $\psi_1$. And they are linearly independent. And thus a basis, so the claim is fulfilled.
 \end{proof}
 
 
 \setcounter{enumi}{-1}
 \item \textit{Let $M$ be an $R$-module and let $I,J$ be ideals in $R$. }
 
 \begin{enumerate}
 \item \textit{Prove that Ann$(I + J) = Ann(I) \cap Ann(J)$. }
 \begin{proof}
 Let $m \in Ann(I + J)$. Then $(i + j)m = 0$ for all $i \in I,j \in J$. Then letting $i = 0$, we know $m \in Ann(J)$, and letting $j = 0$, we know $m \in Ann(I)$. So $Ann(I + J) \sub Ann(I) \cap Ann(J)$. Now let $m \in Ann(I) \cap Ann(J)$. Then $im = 0,\forall i \in I$, and $jm = 0, \forall j \in J$. Then we have: 
 $$
 (i + j)m = im + jm = 0 + 0 = 0,
 $$
 by he definition of an $R$-module. So $Ann(I) \cap Ann(J) \sub Ann(I + J)$. Hence they are equal. 
 \end{proof}
 \item \textit{Prove that $Ann(I) + Ann(J) \sub Ann(I \cap J)$.  }
 
 \begin{proof}
 Let $m \in Ann(I) + Ann(J)$. Then $m = n + k$ for some $n \in Ann(I),k \in Ann(J)$. Let $i \in I \cap J$. Then we know: 
 $$
 im = i(n + k) = in + ik = 0 + 0 = 0,
 $$
  by the distributivity of the action of $R$ on $M$, and since $i \in I$, and $i \in J$, and since $n,k$ are in the respective annihilators. Thus $m \in Ann(I \cap J) \Rightarrow  Ann(I) + Ann(J) \sub Ann(I \cap J)$. 
 \end{proof}
 \item \textit{Give an example where the inclusion in part (b) is strict. }
 
 Let $R$ be the ring of continuous functions $f:[0,1] \to \R$. Note this is not an integral domain since we can construct zero divisors in the form of a pair piecewise functions, one of which is zero on half the interval, and the other being zero on the other half. We consider the $R$-module of $R$ over itself. Then let $I$ be the ideal of functions which are zero on $[0,1/2]$, and $J$ be the ideal of functions which are zero on $[1/2,1]$. Now note that $I + J \neq R$ since $f(x) = 1$ is in $R$, but not in $I + J$, since all functions in $I + J$ are zero at $1/2$. But $I \cap J = 0$, since these functions must be zero across both halves, and so $Ann(I \cap J) = R$, and so $Ann(J) + Ann(I) = I + J \subsetneq R = Ann(I \cap J)$. 
 
 We give another example. Consider $R = F[x,y]$, 
 $$
 M = R/(xy) = \Set{a_0 + b_1x + \cdots b_nx^n + c_1y + \cdots + c_ny^n}.
 $$ 
 $I = (x),J = (y)$, and $I \cap J = (xy)$. Then we have: 
\bee
Ann(I) &= \Set{c_1y + \cdots + c_ny^n} \sub M,\\
Ann(J) = \Set{b_1x + \cdots + b_nx^n}.
\eee
And $Ann(I \cap J) = Ann(xy) = M$. And $F \sub Ann(I \cap J) \subsetneq Ann(I) + Ann(J)$. 
 
 \item \textit{If $R$ is commutative and unital and $I,J$ are comaximal, prove that $Ann(I \cap J) = Ann(I) + Ann(J)$. }
 
 \begin{proof}
 Assume $R$ is commutative and unital, and $I,J$ are comaximal. 
 Let $m \in Ann(I + J) = Ann((1)) = Ann(R)$ since $I,J$ are comaximal, and $R$ is commutative and unital. So $rm = 0$ for all $r \in R$. So then $m \in Ann(I)$, and since $0 \in Ann(J)$, we may write $m = m + 0$, so $m \in Ann(I) + Ann(J)$. And thus $Ann(I + J) \sub Ann(I) + Ann(J)$. So they are equal by the result of part (b). \textbf{This is just very wrong, I think. }
 \end{proof}
\end{enumerate}  

\end{enumerate}

\section{Tensor products of modules}

\textbf{Monday, January, 22nd}

Let $R$ be a unital, commutative ring. Let $M,N$ be $R$-modules. We have:
$$
M \times N = M \oplus N.
$$
i.e. $(u,v) = u + v = (u,0) + (0,v)$. If we want to actually multiply $M,N$, multiply elements of $M$ and elements of $N$: $uv$. Then the first thing we need is for it to be distributive. Then we want:
 \bee
 (u_1 + u_2)v &= u_1v + u_2v,\\
 u(v_1 + v_2) &= uv_1 + uv_2.
 \eee
 \begin{Def}
 A mapping $\beta: M \times N \to K$ is said to be \textbf{bilinear} if for all $v \in N$, $\beta:(\cdot,v):M \to K$ is a hom-sm, and for any $u \in M$, $\beta(u,\cdot):N \to K$ is a hom-sm, that is, $\forall v \in N,\forall u_1,u_2 \in M$:
 $$
 \beta(u_1 + u_2,v) = \beta(u_1,v) + \beta(u_2,v).
 $$
 And $\forall u \in M,a \in R$:
 $$
 \beta(au,v) = a\beta(u,v).
 $$
 And $\forall u \in M,\forall v_1,v_2 \in N$:
 $$
 \beta(u,v_1  + v_2) = \beta(u,v_1) + \beta(u,v_2).
 $$
 Where above, we put $\beta_v(u) = \beta(u,v)$ for all $u \in M$. $\beta_v:M \to K$. 
 \end{Def}
 
 \begin{Def}
 The \textbf{tensor product} $M \otimes_R N$ is an $R$-module with a bilinear mapping $\beta:M \times N \to M \otimes N$ such that for any module $K$, and any bilinear mapping $\gamma:M \times N \to K$, there is a unique homomorphism $\phi: M \otimes N \to K$ such that $\gamma = \phi \circ \beta$, i.e.: 
 
 \begin{center}
\begin{tikzcd}
 & M \times N \arrow[rd, "\gamma"] \arrow[ld, "\beta"'] &  \\
M \otimes N \arrow[rr, "\phi"] &  & K
\end{tikzcd}
\end{center}

is commutative. 
 
 \end{Def}
 
 In the above diagram, the top and left nodes together are the universal object. And the morphism is $\phi$. 
 
  We need to prove this because the above definition is not constructive. We just said it's a module with certain properties. Now we construct it explicitly. If such a module exists, then it is unique up to isomorphism. 
 
 \begin{Prop}
 $M \otimes N$ exists. 
 \end{Prop}

 
 \begin{proof}
 Let $\mathcal{M}$ be the free $R$-module generated by $M \times N$ - as a set. That is, the set of formal linear combinations of pairs $(u,v) \in M \times N$. So:
 $$
 \mathcal{M} = \Set{a_1(u_1,v_1) + \cdots + a_n(u_n,v_n):a_i \in R, (u_i,v_i) \in M \times N}.
 $$
 Let $\mathcal{L}$ be the submodule of $\mathcal{M}$ generated by elements of the form: 
 \bee
 (u_1 + u_2,v) - (u_1,v) - (u_2,v),\\(u,v_1 + v_2) - (u,v_1) - (u,v_2),\\
 (au,v) - a(u,v),\\
 (u,av) - (a(u,v)).\\
 \eee
 We want the bilinearity relations to be satisfied, so we declare all these elements to be zero. 
 Claim: $\mathcal{M}/\mathcal{L} = M \otimes N$. So what are elements of this module? These are classes of linear combinations of pairs. Elements of $\mathcal{M}/\mathcal{L}$ are classes (module $\mathcal{L}$) of linear combinations of $(u,v)$. The class of $(u,v)$ is denoted by $u \otimes b$. So 
 $$
 \mathcal{M}/\mathcal{L} = \Set{\sum a_i(u_i \otimes v_i): a_i \in R, u_i \in M,v_i \in N}.
 $$
 
 \begin{Def}The elements of of the set above are called \textbf{tensors}, and $u \otimes v$ is called a \textbf{simple tensor}. 
 \end{Def}
 So:
  $$
 \mathcal{M}/\mathcal{L} = \Set{\sum a_i\overline{(u_i,v_i)}}.
 $$
 And:
 \bee
 \overline{(u,v)} &= u \otimes v,\\
 \overline{(u_1 + u_2,v)} &= \overline{(u_1,v)} + \overline{(u_2,v)},\\
 (u_1 + u_2) \otimes v &= u_1 \otimes v + u_2 \otimes v.
 \eee
 The mapping $M \times N \to \mathcal{M}/\mathcal{L}$ given by $(u,v) \mapsto u \otimes v$ is bilinear: in $\mathcal{M}/\mathcal{L}$, we have:
 \bee
 (u_1 + u_2) \otimes v &= u_1 \otimes v + u_2 \otimes v,\\
 \beta(u_1 + u_2,v) &= \beta(u_1,v) + \beta(u_2,v),
 \eee
 where the stuff in the bottom line is equal to the stuff it lines up with in the top line. Also: 
 \bee
 (au) \otimes v &= a(u \otimes v),\\
 u \otimes (v_1 + v_2) &= u\otimes v_1 
+ u\otimes v_2,\\
u\otimes (av) &= a(u \otimes v).
\eee
if $\gamma:M \times N \to K$ is bilinear, we have a unique homomorphism $\Phi: M \to K$ with $\Phi(u,v) = \gamma(u,v)$ for all $u,v$. 

Since $\gamma$ is bilinear, $\Phi(\mathcal{L}) = 0$,
\bee
(\Phi(u_1 + u_2,v) - (u_1,v) - (u_2,v)) 
&= \Phi(u_1 + u_2,v) - \Phi(u_1,v) - \Phi(u_2,v)\\
&= \gamma(u_1 + u_2,v) - \gamma(u_1,v) - \gamma(u_2,v).
\eee
and the same holds for all other relations. So $\Phi$ is factorized to a hom-sm $\phi: \mathcal{M}/\mathcal{K} \to K$. It is unique since $\mathcal{M}$ is generated by $M \times N$ s.t. $\phi(u \ten v) = \gamma(u,v)$. 
 \end{proof}
 
 \textbf{Tuesday, January 23rd}
 
\vspace{5mm}

Let $R$ be commutative, unital. And $M,N$ be $R$-modules. Then:
$
M \otimes N = M \otimes_R N$ is an $R$-module consisting of \textbf{tensors: } 

$$
a_1(u_1 \otimes v_1) + \cdots + a_n(u_n \otimes v_n),
$$

with $a_i \in R,u_i \in M,v_i \in N$. It is generated by \textbf{simple tensors} $u \otimes v$, with relations: 
 \bee
  (u_1 + u_2) \otimes v &= u_1 \otimes v + u_2 \tens v,\\
 (au) \otimes v &= a(u \otimes v),\\
 u \otimes (v_1 + v_2) &= u\otimes v_1 
+ u\otimes v_2,\\
u\otimes (av) &= a(u \otimes v).
\eee
And it has no other relations! It has a universal property: for any $R$-module $K$ and a bilinear mapping $\gamma: M \times N \to K$ there exists a unique hom-sm $\phi:M \otimes N \to K$ such that $\phi(u \otimes v) = \gamma(u,v)$ for each $u \in M,v \in N$. 

\begin{lem}
We list some properties. 
\begin{enumerate}
\item If $M = RB$, $N = RC$, then $M \otimes N = R(B \otimes C)$, where: 
$$B \otimes C = \Set{u\otimes v:u \in B,v \in C}.
$$ 
\begin{proof}
$\forall u \in M$, $u = \sum a_iu_i,u_i \in B$. And $\forall v \in N, v = \sum b_jv_j,v_j \in C$. Then:
$$
u \otimes v = \sum_{i,j}a_ib_j(u_i \otimes v_j).
$$
i.e. any tensor in $M \otimes N$ is a linear combinations of such simple tensors. 
\end{proof}
\item $\forall u \in M$, $u \otimes 0 = 0$, and $\forall v \in N$, $0 \otimes v = 0$. 

\begin{proof}
$u \otimes 0 = u \otimes (0 + 0) = u \otimes 0 + u \otimes 0$, so we must have that it is zero. 
\end{proof}

\item $\forall$ module $M$, $M \otimes 0 = 0 \otimes M = 0$. 

\item $\forall$ module $M$, $M \otimes R \cong R \otimes M \cong M$. 

$R$ plays the role of the identity in this algebra of modules. 

\begin{proof}
Take any tensor, it is of the form: 
\bee \label{eqn10.17}
a_1(u_1 \otimes b_1) + \cdots + a_n(u_n \otimes b_n) &= a_1b_1(u_1 \otimes 1) + \cdots + a_nb_n(u_n \otimes 1)\\
&= (a_1b_1u_1) \otimes 1 + \cdots + (a_nb_nu_n) \otimes 1\\
&= (a_1b_1u_1 + \cdots + a_nb_nu_n) \otimes 1 = v \otimes 1.
\eee
Note in the above, $b_i \in R$, so we can take them out: $u \otimes b = u \otimes (b \cdot 1) = b(u \otimes 1)$. 

So define a hom-sm $\phi:M \ten R \to M$ by: 
$$
\sum_{i = 1}^n b_i(u_i \ten a_i) \mapsto \sum_{i = 1}^n a_ib_iu_i.
$$
Why is $\phi$ defined, and why is it a homomorphism? First, define $\gamma: M \times R \to M$, $\gamma(u,a) = au$. This $\gamma$ is bilinear. If $u$ is fixed, it is linear with respect to $a$, if $a$ is fixed, it is linear with respect to $u$. So there exists a unique hom-sm $\phi: M \tens
R$ s.t. $\phi(u \tens a) = au$ $\forall u \in M,a \in R$. This is our $\phi$. Why is it an isomorphism. Construct the inverse mapping. Take $u \mapsto u \tens 1$. And why do we need to prove that it is defined? There are relations in the module. 

\begin{Ex} The same tensor can be written in several ways as a sum of simple tensors, in $\z \tens \z$:
$$
5 \tens 6 = 2 \tens 6 + 3 \tens 6.
$$
The left hand side is sent to $30$, but we need it to be bilinear or something. 

\end{Ex}

So the inverse is a homomorphism, $M \to M \tens R$. It is an inverse, since if we start with a tensor, send it to the $v \tens 1$ in Equation \ref{eqn10.17}, we get: 
$$
\sum a_i(u_i \tens b_i) \mapsto^\phi \sum a_ib_iu_i \mapsto^{\phi^{-1}} (\sum a_ib_iu_i)\tens 1.
$$
\end{proof}

\item $M \tens N \cong N \tens M$ by the map $u \tens v \mapsto v \tens u$. We send $(u,v) \mapsto v \tens u$ - linear, so $\phi$ exists. And $v \tens u \mapsto u \tens v$ is its inverse. 

\item $M \tens N) \tens K \cong M \tens (N \tens K)$ by the map: 
$$
(u \tens v) \tens w \mapsto u \tens (v \tens w).
$$

\item $M \tens (M \oplus K) \cong (M \tens K) \oplus (M \tens K)$. 

So in naive set theory, any collection of objects is a set, but this leads to immediate crap. So we use ZFC, axiomatic. Modules don't form a set because there are too many of them, lmao. 

\begin{proof}
Let $\phi:u \tens (v,w) \mapsto (u\tens v,u \tens w)$. We only define $\phi$ on simple tensors, then by linearity, it is extended to all tensors. Is it well defined? Yes, $\phi$ is a well-defined hom-sm if:
$$
(u,(v,w)) \mapsto (u \tens v,u \tens w),
$$
 is bilinear. The stuff on the left side, domain, is  in $M \times (N \oplus K)$. So we check it: 
\bee
(u_1 + u_2,(v,w)) &\mapsto ((u_1,u_2) \tens v,(u_1,u_2) \tens w)\\
 ((u_1,u_2) \tens v,(u_1,u_2) \tens w) &= (u_1 \tens v + u_2 \tens v, u_1 \tens w + u_2 \tens w)\\
&= (u_1 \tens v,u_1 \tens w) + (u_2 \tens v, u_2 \tens v).
\eee
To prove that this is an ismorphism, we need its inverse. Define $\psi: (M \tens N) \oplus (M \tens K) \to M \tens (N \oplus K)$. The direct sum is also a universal object. Define $\psi_1: M \tens N \to M \tens (N \oplus K),\psi_2: M \tens K \to M \tens (N \oplus K)$ by: 
\bee
\psi_1(u \tens v) &= u \tens (v,0),\\
\psi_2(u \tens w) &= u \tens (0,w).
\eee
There is a unique hom-sm $\psi$ s.t. $\psi|_{M \tens N} = \psi_1$, and $\psi|_{M \tens K} = \psi_2$. So we have: 
$$
(\psi(\alpha,\beta) = \psi_1(\alpha) + \psi_2(\beta)).
$$
So we check that they are inverses of each other. We have: 
\bee
\phi:u \tens (v,w) \mapsto (u\tens v,u \tens w) \mapsto_\psi u \tens (v,0) + u \tens (0,w) = u \tens (v,w).
\eee
\end{proof}
\end{enumerate}
\end{lem}

\textbf{Wednesday, January 24th}

Tensors come from physics, from algebra, from topology. Linear transformations, bilinear forms, ... There are many objects that can be interpreted as tensors. In algebra, they can be used to extend scalars. 

\begin{Ex}
It can be shown that:
$$
C(x \times y) = \bar{C(x) \tens C(y)}.
$$
\end{Ex}

For any $R$-module $M$, $M \tens R \cong M$, and $M \tens(N \oplus K) \cong (M \tens N) \oplus (M \tens K)$. 

\begin{rem}
$M \tens R^n \cong M^n = M \oplus \cdots \oplus M$. 
\end{rem}

\begin{rem}\label{1097}
$R^n \tens R^m \cong R^{nm}$-free of rank $nm$. 
\end{rem}

\begin{Ex}
\vspace{5mm}
\begin{enumerate}


\item \textit{Prove that $M \tens R/I \cong M/(IM)$. What number is this????}
\begin{proof}
The mapping $\gamma:(u,\bar{a}) = a \mod I \mapsto \bar{au} = au \mod (IM)$. This is well-defined, and is bilinear, so it satisfies the properties required for a tensor product. $a + c \mapsto \bar{au + cu} \in IM, c \in I$. So hom-sm $\phi:M \tens (R/I) \to M/(IM)$ is defined. And we have: $u \tens \bar{a} \mapsto \bar{au}$. And it has the inverse: $\psi:\bar{u} \mapsto (u \tens \bar{1}$, defined from $M/IM \to M \tens R/I$. You should understand that $\gamma$ is not an isomorphism itself. Now why is this new map well defined, first? If you replace $u$ by $\bar{u }+ \bar{\sum b_iv_i}$, where $b_i \in I$. Then under our new map we have:
$$
\psi:\bar{u} = \bar{\sum b_iv_i} \mapsto (u + b_iv_i) \tens \bar{1} = (u \tens \bar{1} + \sum(v_i \tens \bar{b_i},
$$
where $\sum(v_i \tens \bar{b_i} = 0 \mod (M \tens I)$. So $\psi$ is well-defined, and $\psi = \phi^{-1}$. 
\end{proof}

\item \textit{Consider $\z_3 \tens \z_2$. }

We may write 
\bee
1 \tens 1 &= (3 - 2) \tens 1 \\
&= 3 \tens 1 - 2 \tens 1\\
&= 3 \tens 1 - 2( 1 \tens 1)\\
&= 3 \tens 1 - 1 \tens 2\\
&= 0 \tens 1 - 1 \tens 0\\
&= 0 - 0 = 0.
\eee
And for any $n \tens k = nk(1 \tens 1) =0$. So $\z_3 \tens \z_2 = 0$. 
\end{enumerate}
\end{Ex}

\begin{lem}
If $(n,m) = 1$, then $\z_n \tens \z_m = 0$. 
\end{lem}

\begin{proof}
There exists $k,l$ s.t. $kn + lm = 1$. Then:
$$
1 \tens 1 = (kn + lm) \tens 1 = k(n \tens 1) + l (1 \tens m) = 0.
$$
And $\forall (a \tens b) = ab(1 \tens 1) = 0$. 
\end{proof}

\begin{lem}
Let $(n,m) = d$. Then $\z_n \tens \z_m \cong \z_d$. 
\end{lem}

\begin{proof}
Define $\phi: \z_n \tens \z_m \to \z_d$ by $\phi(\bar{k}\tens \bar{l}) = kl \mod d$. If you add a multiple of $n$ to $k$, the result will be the same because $d|n$, and same for $m$ ($(\bar{k},\bar{l}) \mapsto kl \mod d$ is bilinear). Why is it a homomorphism? Let's check that it is surjective. Note that $1 \tens 1 \mapsto 1$, and $1$ generates $\z_d$. So done: $\phi(1 \tens a) = a \mod d$, so $\phi$ is surjective. Or maybe better to just define an inverse, since injectivity looks hard to prove. \begin{comment}Any tensor $w \in \z_n \tens \z_m$ is such that: 
\bee
w &= \sum_{i = 1}^n c_i(\bar{k_i} \tens \bar{l_i})\\
&= \sum_{i  =1}^n c_ik_il_i (1 \tens 1)
\eee
And $\phi(w) = \sum{i = 1}^n c_ik_i l_i \mod d$. So $w \in ker\phi$ if and only if $\sum c_ik_il_i \equiv 0 \mod d$.\end{comment} Now take the element $1 \tens 1,2(1\tens 1),...,(d - 1)(1\tens 1) \neq 0$. But $d(1 \tens 1) = 0$. So the order of $1 \tens 1$ is $d$, and $(1 \tens 1)$ generates $\z_n \tens z_m$. Note that:
$$
\phi(k(1\tens 1)) = k \mod d,
$$ 
for any $k$, so it's injective or something because the kernel is 0 maybe. 
\end{proof}

\begin{rem}
If $G$ is a finite abelian group, then 
$$
G \cong \z_{p_1^{r_{1,1}}} \oplus \cdots \oplus \z_{p_1^{r_{1,k_1}}} \oplus ( ..p_2.. ) \oplus \cdots \oplus (..p_l..). 
$$
So we have $G \tens_\z \z_{p_1} \cong \z{p_1}^{k_1}$. We get this by multiplying by each component and using the previous result, that since the $\z$'s are relatively prime, they go to zero. So $\forall p\mid |G|$, $G \tens \z_p \cong \z_p^k$ where $k$ is the number of $p$-elementary divisors of $G$. 
\end{rem}

\begin{rem}
Let $R$ be an integral domain, let $F$ be the field of quotients of $R$. 
$$
F \tens_R M = ?
$$
\end{rem}

\begin{lem}
If $M$ is a torsion module, then $F \tens_R M = 0$. 
\end{lem}

\begin{proof}
Let $u \in M$. Find $a \neq 0$ s.t. $au = 0$. Then:
$$
1 \tens u = (aa^{-1})\tens u = a^{-1} \tens (au) = 0.
$$
But this is not enough. Moreover, for any $b \in F$, we know: 
$$
b \tens u = (aa^{-1}b) \tens u = (a^{-1}b) \tens (au) = 0.
$$
\end{proof}

\begin{Ex}
Consider $F^2 \tens F^2 \cong F^4$, where $\Set{e_1,e_2}$ is a basis. So basis of $F^2 \tens F^2$ is:
$$
\Set{e_1 \tens e_1,e_1 \tens e_2,e_2 \tens e_1,e_2 \tens e_2}.
$$
So $F^2 = e_1F \oplus e_2F$. Any tensor from $F^2 \tens F^2$ is of the form:
$$
a_{1,1}(e_1 \tens e_1) + a_{1,2}(e_1 \tens e_2) + a_{2,1}(e_2 \tens e_1) + a_{2,2}(e_2 \tens e_2),
$$
its coordinates form: 
$$
\left( 
\begin{matrix}
a_{11} & a_{12}\\
a_{21} & a_{22}
\end{matrix}
\right).
$$
So tensors are in bijection with $2 \times 2$ matrices. A simple tensor: 
$$
(a_1e_1 + a_2e_2) \tens (b_1e_1 + b_2e_2) \leftrightarrow 
\left(
\begin{matrix}
a_1b_1 & a_1b_2\\
a_2b_1 & a_2 b_2
\end{matrix}
\right),
$$
which is degenerate of rank 1. So simple tensors correspond to matrices of rank 1, determinant 0. So note that $(e_1 \tens e_2) + (e_2 \tens e_1)$ is not simple. 
\end{Ex}

\begin{rem}
For an $R$-algebra $S$, we have $S \tens_R$ is an $S$-module. 
\end{rem}


\textbf{Thursday, January 25th}
\vspace{5mm}
\begin{Ex}
We give an example of when $M,N$ are $R$-modules but the tensor product of submodules of these are not a submodule of the tensor product of $M,N$. Note $\z \sub \Q$ as $\z$-modules. But $\z \tens_\z \z_2 \cong \z_2$, and $\Q \tens_\z \z_2 = 0$. 
\end{Ex}

\begin{lem}
Let $S$ be an $R$-algebra, $M$ be an $R$-module then $S \tens_R M$ has a structure of an $S$-module by $\alpha(\beta \tens u) = \alpha \beta \tens u, \alpha,\beta \in S,u \in M$. 
\end{lem}

\begin{proof}
Note we have: 
\bee
\alpha\left( \sum a_i(\beta_i \tens u_i)\right) &= \sum a_i ( \alpha \beta_i \tens u_i).
\eee
The line in the above expression, we are checking if the same thing works for arbitrary tensors, since we defined it in the statement of the problem for just simple tensors. We have to check that elements of the kernel... when we convert the stuff in the left to the right, we have to factorize by a submodule. Is the operation well defined? The operation is: $S \tens M \to S \tens M$ where $\beta \tens u \mapsto \alpha \beta \tens u$. It is well-defined because it is a bilinear operation: $(\beta,u) \mapsto \alpha\beta \tens u$. This is our universal approach: first we defined some mapping on simple tensors: $\beta \tens u \mapsto \alpha \beta \tens u$, which should be a homomorphism of $R$-modules. Then to check that it is well-defined, we need to check that it is bilinear on the set $S times M$. Checking bilinearity: 
$$
(\alpha \beta, u) \mapsto \alpha a \beta \tens u = a \alpha \beta \tens u = a(\alpha \beta \tens u).
$$
Checking conditions, we have: 
\bee
(\alpha_1 + \alpha_2)(\beta \tens u) &= \alpha_1 \beta \tens u + \alpha_2 \beta \tens u = \alpha_1(\beta \tens u) + \alpha_2(\beta \tens u),\\
\alpha(w_1 + w_2) &= \alpha w_1 + \alpha w_2,w_i \in S \tens_R M,\\
\alpha_1(\alpha_2w) = (\alpha_1 \alpha_2)w.
\eee
The second line is because $w \mapsto \alpha w$ is a homomorphism. This is called \textbf{extension of scalars. }
\end{proof}

\begin{Ex}
We give some examples of \textbf{extension of scalars. }

\begin{enumerate}
\item Let $F$ be the field of quotients of an integral domain $R$. Then $F \tens_R M$ is an $F$-module, that is, a vector space. This operation kills all torsion, but preserves the free part. In the case $M = M_1 \oplus M_2$, where $M_1$ is free, $M_2$ is torsion. Not always the case, but it is in PID, or in finitely generated abelian groups. So $F \tens M = F \tens M \oplus 0$. It has the same rank as $M_1$. If $M_1 = R^n$, then $F \tens M_1 \cong F^n$, and it is a vector space. 

\item Let $V$ be an $R$-vector space. Consider $\c \tens_\R V$ - this is a $\c$-vector space, called \textbf{the complexification of $V$.} We have: 
\bee
\c = \R \oplus i\R &= R \Set{1,i},\\
\c \tens V &= (\R \tens V) \oplus (i\R \tens V) = (1 \tens V) \oplus (i \tens V).
\eee
So we have: $a \tens u = 1 \tens au$, and $ib \tens u = i \tens bu$. So $\forall w \in \c \tens V$, $w = 1 \tens u + i \tens v = u + iv$. Also, 
$$
(a + ib)(u + iv) = (au - bv) + i(av + bu),
$$
$u + iv \in V + iV$. So this is similar to how we get new elements when we extend from $\R$ to $\c$. If $V$ is $n$-dimensional, with basis $\Set{e_1,...,e_n}$, then $\c \tens V$ is $n$-dimensional $\c$-vector space with basis $\Set{e_1,...,e_n}$ and $2n$-dimensional $\R$-vector space with basis $\Set{e_1,...,e_n,ie_1,...,ie_n}$. Now let $T$ be a linear transformation of $V$. Then $V$ is an $\R[x]$-module by $xu = Tu$. We have: 
$$
\left( \sum_{k = 0}^n a_k x^k\right) u = \sum_{k = 0}^n a_kT^ku.
$$
So $\c[x] \tens_{\R[x]} V$ is a $\c[x]$-module, that is, we have a $\c$-vector space $\c \tens_\R V$ on which $T$ acts as a linear transformation. 

\item Let $A_1,A_2$ be $R$-algebras. Then $A_1 \tens_R A_2$ has a structure of an $R$-algebra by 
$$
\alpha_1 \tens \beta_1) \cdot (\alpha_2 \tens \beta_2) = (\alpha_1\alpha_2)\tens(\beta_1\tens \beta_2).
$$
Shit should be checked, but it works. The \textbf{subscript below the tensor product symbol} represents where the scalars are from. 

\item \textit{This is a problem from the book. If $S$ is an $R$-algebra, prove that $S \tens_R R[x] \cong S[x]$. }

\begin{proof}
You need to check something like this:
$$
(\alpha_1 \tens (a_1x^n + \cdots + a_1x + a_0)) \cdot (\alpha_2 \tens (...)).
$$
The map would be given by: 
$$
\alpha_1 \tens (a_1x^n + \cdots + a_1x + a_0)\mapsto a_n\alpha x^n + \cdots a_1 \alpha x + a_0\alpha.
$$
Here it is just defined for simple tensors. So any polynomial of $S$ is the image of some tensor, since we have:
$$
\alpha_n \tens x^n + \cdots + \alpha_1 \tens x + \alpha_0 \tens 1 \mapsto \alpha_nx^n + \cdots + \alpha_1 x + \alpha_0.
$$
\end{proof}

\item Prove $R[x] \tens_R R[y] \cong R[x,y]$. 

\begin{proof}
$$
(a_nx^n + \cdots a_1 x + a_0) \tens (b_my^m + \cdots + b_1 y + b_0) \mapsto a_nb_mx^ny^n + \cdots + a_0b_0 = p(x)q(x).
$$
We map $x^n \tens y^m \mapsto x^n y^m$. Again this is an exercise from the book. 
\end{proof}

\end{enumerate}
\end{Ex}

\begin{Def}
Let $\phi_1 \in Hom(M_1,N_1)$ and $\phi_2 \in Hom(M_2,N_2)$. Then $\phi_1 \tens \phi_2: M_1 \tens M_2 \to N_1 \tens N_2$. is defined by $\phi_1 \tens \phi_2(u_1 \tens u_2) = \phi_1(u_1) \tens \phi_i(u_2)$. This is the \textbf{tensor product of two homomorphisms. }
\end{Def}
We prove it is a homomorphism. 
\begin{proof}
It is well defined since the mapping $(u_1,u_2) \mapsto \phi_1(u_1) \tens \phi_2(u_2)$ is bilinear. 
So we get a mapping $Hom(M_1,N_1) \times Hom(M_2,N_2) \to Hom(M_1 \tens M_2,N_1 \tens N_2)$. This mapping is also bilinear. We have to check four identities. So it defines a homomorphism: 
$$
Hom(M_1,N_1) \tens Hom(M_2,N_2) \to Hom(M_1 \tens M_2,N_1 \tens N_2),
$$
as we have seen from the definition/construction of a tensor product. 
\end{proof}

\begin{Def}
Assume that $M$ is an $R$-module. We want to convert it to an algebra. If we take $M \tens M$, then we multiply two vectors, but we cannot multiply these tensors. To have an algebra you must be able to multiply any elements. So if we want an algebra, we take $R \oplus M \oplus (M \tens M) \oplus (M \tens M \tens M) \oplus (...) \oplus \cdots$. 
This is called the \textbf{tensor algebra of $M$. }
\end{Def} \label{gradedalgebra}

\begin{Def}
An \textbf{graded algebra}: 
$$
A = A_0 \oplus A_1 \oplus A_2 \oplus \cdots,
$$
such that $\forall i,j$, $A_i\cdot A_j \sub A_{i + j}$. 
\end{Def}




\begin{Ex}
We give a neat example that demonstrates why the location of the scalars is important: 
$$
\c \tens_\R \c \cong \R^4,
$$
$$
\c \tens_\c \c \cong \c \cong \R^2.
$$
\end{Ex}

\begin{rem}
In the case $M = M_1 \oplus M_2$, where $M_1$ is free, $M_2$ is torsion. Not always the case, but it is in PID, or in finitely generated abelian groups. So $F \tens M = F \tens M \oplus 0$. It has the same rank as $M_1$. If $M_1 = R^n$, then $F \tens M_1 \cong F^n$, and it is a vector space. So this operation \textbf{kills all torsion.}
\end{rem}

\begin{rem}
Recall that an \textbf{$R$-algebra} is basically an $R$-module which is also a ring. So it is an $R$-module with multiplication. 
\end{rem}

\textbf{Friday, January 26th}

We do some exercises from the book starting with $10.4.8$. 

\textbf{Monday, January 29th}

We go to Section 10.5.
\section*{10.4 Exercises}
\begin{enumerate}[label=\arabic*.]


\setcounter{enumi}{7}

\item \textit{Let $R$ be an integral domain, $Q$-field of quotients, $N$ an $R$-module, and $U = R^* = R - \Set{0}$. We define: $U^{-1}N = U \times N/\sim$. Where: 
$$
(u,n) \sim (u',n') \text{ if } v(u'n - un') = 0,
$$
for some $v$. Denote $(u,n) = \fracc{n}{u}$. We define addition: 
$$
\fracc{n}{u}+\fracc{m}{v} = \fracc{vn + um}{uv}.
$$
And we define multiplication by scalars: 
$$
r\fracc{n}{u} = \fracc{rn}{u}.
$$
And we claim that $U^{-1}N$ becomes an $R$-module. 
}

\begin{enumerate}
\item 

\begin{proof}
We have: 
\bee
(u,n) \sim (u',n') \sim (u'',n'') &\Rightarrow^{?} (u,n) \sim (u'',n''),\\
u'n = un',u''n' = u'n'' &\Rightarrow^{?} un'' = u''n.
\eee
But we have $u''u'n = u''un' = uu'n''$, so $u'(u''n - un'') = 0$. So the relation given by the book, $u'n - un'$ must be the wrong one, so we now switch to using the relation stated above. So we have: 
\bee
(u,n) \sim (u',n') \sim (u'',n'')& \Rightarrow^{?} (u,n) \sim (u'',n''),\\
nu'n = nun',n'u''n' = v'u'n'' &\Rightarrow^{?}.
\eee
So we have $vv'u''u'n = vv'u''un' = vv'uu'n''$, so $vv'u'(u''n - un'') = 0$. So $(u,n) \sim (u'',n'')$. If $(n,u) \sim (n',u'),(m,v) \sim (m',v')$ is $(uv,vn + um) \sim (u'v',v'n' + u'm')$. Is this true? Leibman thinks so. Let's believe that this is true. Or not. Let's check. So we have $w(mv' - m'v) = 0$ for some $w \neq 0$. And $w'(nu' - n'u) = 0$ for some $w' \neq 0$. Now we take:
\bee
&ww'(uv(v'n' + u'm') - u'v'(un + um))\\ =&ww'[vv'(un' - u'n) + uu'(vm' - v'm)]\\
 =& 0.
\eee
And then we have to check something else, which we will skip. So $U^{-1}N$ is an $R$-module, $U^{-1}N = \Set{\fracc{n}{u},n \in N,u \in R - \Set{0}}$. Up until this point, we only needed that $R$ is commutative, and that $U$ is multiplicatively closed, we did not need that $R$ was integral domain yet. 

\end{proof}

\item \textit{Prove that $U'N \cong Q \tens_R N$. }

\begin{proof}
Define $\phi: Q \tens_R N \to U^{-1}N$ by $\fracc{a}{b}\tens n \mapsto \fracc{an}{b}$. Of course it is a well-defined homomorphism, it can be easily checked. Can we construct an inverse? Define $\psi: U^{-1}N \to Q \tens_R N$ by $\fracc{n}{u} \mapsto \fracc{1}{u}\tens n$. And check that $\psi = \phi^{-1}$. Well, is it well-defined? We don't have to check that it is a homomorphism, since if it is the inverse of one, then it is one itself. If $(n',u') \sim (n,u)$, is $\fracc{1}{u}\tens n' = \fracc{1}{u}\tens n$? Let $v(un' - u'n) = 0$. Then:
\bee
\fracc{1}{u'}\tens n' &= uv \cdot \left(\fracc{1}{uvu'}\tens n' \right)\\
&= \fracc{1}{uvu'}\tens uvn'\\
&= \fracc{1}{uvu'}\tens u'vn\\
&= \fracc{1}{u}\tens n.
\eee
So it's well defined. Now why is it the inverse of $\phi$. Observe: 
\bee
\fracc{n}{u}\overset{\psi}{\mapsto} \fracc{1}{u}\tens n &\overset{\phi}{\mapsto} \fracc{n}{u},\\
\fracc{v}{u}\tens n = v(\fracc{1}{u}\tens n) &\overset{\phi}{\mapsto}
v \cdot \fracc{n}{u}\overset{\psi}{\mapsto} \fracc{1}{u}\tens vn = \fracc{v}{u}\tens n.
\eee
\end{proof}

\item \textit{Prove that $\fracc{1}{d}\tens n = 0$ if and only if $rn = 0$ for some $r \in R^*$. }

\begin{proof}
Under the isomorphism from part (b) we have:
$$
\fracc{1}{d}\tens n \overset{\phi}{\mapsto} \fracc{n}{d}\in U^{-1}N.
$$
And $\fracc{n}{d} = 0 = \fracc{0}{1}$ if and only if $r(n - 0) = 0$ for some $r \neq 0$. And we make use of this result in Exercise 10.4.9. 
\end{proof}

\item \textit{Let $A$ be an abelian group. Then prove $\Q \tens_\z A = 0$ if and only if $A$ is a torsion group, $|a| < \infty$ $\forall a \in A$. }

\begin{proof}
We claim that $\Q \tens N = 0$ if and only if $N = Tor(N)$. If $N = Tor(N)$, then $Q \tens N = 0$. Recall that $A$ being abelian is equivalent to saying it is a $\z$-module. So $R = \z$ in this case, so $\Q = Q$, the field of fractions of the ring. If $N = Tor(N)$, then $Q \tens N =0$. If $Q \tens N = 0$, then $\forall n \in N$, $1 \tens n = 0$< so $\exists r \neq 0$, s.t. $rn = 0$ by part (c). So $n \in Tor(N)$ for all $n$. 
\end{proof}

\end{enumerate}








\item \textit{Suppose $R$ is an integral domain with quotient field $Q$ and let $N$ be any $R$-module. Let $Q \tens_R N$ be the module obtained from $N$ by extension of scalars from $R$ to $Q$. Prove that the kernel of the $R$-module homomorphism $\iota: N \to Q \tens_R N$ is the torsion submodule of $N$. [Exercise 10.1.\ref{ex10.1.8},Exercise 10.4.8]}

\begin{proof}
Recall that the torsion submodule is defined as: 
$$
Tor(N) = \{n \in N:rn = 0 \text{ for some nonzero } r \in R\}. 
$$
And recall that $\iota(n) = 1 \tens n$. Let $n \in Tor(N)$. Then $\iota(n) = 1 \tens n$. Since $n \in Tor(N)$, there exists $r \neq 0$ such that $rn = 0$, and we also have $1/r \in Q$. So we have: 
$$
1 \tens n = 1(1 \tens n) = \fracc{1}{r}r(1 \tens n) = \frac{1}{r}(1 \tens rn) = \fracc{1}{r}(1 \tens 0) = 0.
$$
Thus $n \in ker\iota$, and $Tor(N) \sub ker \iota$. Now let $n \in ker\iota$. Then
$$
\iota(n) = 1 \tens n = 0 = 1 \tens 0. 
$$ 
So we must have that there exists $r \neq 0$ s.t. $rn = 0$. And by the result of Exercise 10.4.8(c), we know that $(1/d) \tens n = 0$ if and only if there exists $r \in R$ s.t. $rn = 0$. 
\begin{comment} Then we would have:
$$
1 \tens n = \fracc{1}{r}r \tens n = \fracc{1}{r}\tens rn = \fracc{1}{r}\tens 0 = 0 = 1 \tens 0.
$$
\end{comment}
Hence we know $n \in Tor(N)$. 
\end{proof}

\item \textit{Suppose $R$ is commutative and $N \cong R^n$ is a free $R$-module of rank $n$ with $R$-module basis $e_1,...,e_n$. }

Recall the definition of a free module of rank $n$: 
\begin{Def}
A \textbf{free module} is a direct sum of finitely or infinitely many copies of $R$, 
$$
F_\Lambda = \bigoplus_{\alpha \in \Lambda}R = \{a_{\alpha_1} + \cdots + a_{\alpha_k}: k \in \n,\alpha_i \in \Lambda,a_{\alpha_i} \in R\},
$$
where the sum of $u$'s above is a formal sum. We can also define it as: 
$$
F_\Lambda = \{(a_\alpha)_{\alpha \in \Lambda}: a_\alpha \in R,\forall \alpha,a_\alpha = 0 \text{ for all but finitely many }\alpha\}.
$$
Note $R$ is unital here. 
\end{Def}

\begin{enumerate}
\item \textit{For any nonzero $R$-module $M$ show that every element of $M \tens N$ can be written uniquely in the form $\sum_{i = 1}^n m_i \tens e_i$ where $m_i \in M$. Deduce that if $\sum_{i  =1}^n m_i \tens e_i = 0$ in $M \tens N$, then $m_i = 0$ for $i  = 1,...,n$. }

\begin{proof}
Let $t = a_1(u_1 \otimes v_1) + \cdots + a_l(u_l \otimes v_l) \in M \tens N$. And for each $v_i \in N$ we have: 
$$
v_i = r_1e_1 + \cdots + r_ne_n,
$$
with $r_j \in R$ uniquely by the definition of our standard basis. Then we may write: 
\bee
t &= a_1(u_1 \tens (r_{1,1}e_1 + \cdots + r_{1,n}e_n)) + \cdots + a_l(u_l \tens (r_{l,1}e_1 + \cdots + r_{l,n}e_n))\\
&= (a_1u_1 \tens (r_{1,1}e_1 + \cdots + r_{1,n}e_n)) + \cdots + (a_lu_l \tens (r_{l,1}e_1 + \cdots + r_{l,n}e_n))\\
&= ((a_1u_1 \tens r_{1,1}e_1) + \cdots + (a_1u_1 \tens r_{1,n}e_n)) + \cdots + ((a_lu_l \tens r_{l,1}e_1) + \cdots + (a_lu_l \tens r_{l,n}e_n))\\
&= ((a_1r_{1,1}u_1 \tens e_1) + \cdots + (a_1r_{1,n}u_1 \tens e_n)) + \cdots + ((a_lr_{l,1}u_l \tens e_1) + \cdots + (a_lr_{l,n}u_l \tens e_n))\\
&= ((a_1r_{1,1}u_1 \tens e_1) + \cdots + (a_lr_{l,1}u_l \tens e_1) ) + \cdots + ((a_1r_{1,n}u_1 \tens e_n) + \cdots + (a_lr_{l,n}u_l \tens e_n))\\
&= ((a_1r_{1,1}u_1 \cdots + a_lr_{l,1}u_l) \tens e_1)  + \cdots + ((a_1r_{1,n}u_1  + \cdots + a_lr_{l,n}u_l) \tens e_n).
\eee
So letting $m_i = (a_1r_{1,i}u_1 \cdots + a_lr_{l,i}u_l)$, we have:
$$
t = \sum_{i = 1}^n m_i \tens e_i,
$$
where $m_i \in M$. Assume that $\sum_{i = 1}^n m_i \tens e_i = 0$. If each term in the sum is identically zero, then the result is proved, all $m_i = 0$. So without loss of generality, assume $m_1,...,m_k \neq 0$ for some $k \leq n$. If the $m_i$'s are linearly independent, then since the $e_i$'s are also linearly independent:
$$
\sum_{i = 1}^n m_i \tens e_i = 0 \Rightarrow m_i = 0, \forall i,
$$ which is a contradiction. So then we must have that the $m_i$'s are linearly dependent. So we can write: 
$$
\sum_{i  =1}^k m_i \tens e_i = \sum_{i  =1}^k r_i  m \tens e_i = \sum_{i  =1}^k   m \tens r_ie_i = 0.
$$
If $m = 0$ we are done, contradiction, since then $m_i = 0$ for all $i$. If $\sum r_ie_i = 0$ we have a contradiction, since then the basis wouldn't be linearly independent. So we must have that all $m_i = 0$. 
\end{proof}

\item \textit{Show that if $\sum m_i \tens n_i = 0$ in $M \tens N$ where the $n_i$ are merely assumed to be $R$-linearly independent, then it is not necessarily true that all the $m_i$ are 0. [Consider $R = \z,n  =1,M = \z/2\z$, and the element $1 \tens 2$.] }
\begin{proof}
Note that now we relax the assumption that our elements from $R^n$ generate $R^n$. So now they are only linearly independent. We have:
$$
1 \tens 2 = 2 \tens 1 = 0 \tens 1 = 0,
$$
but $1 \neq 0 \in \z/2\z$, and $2$ is just a single element of some $R$ module over $R$, so it is linearly independent. So we have found a counterexample. 
\end{proof}
\end{enumerate}

\setcounter{enumi}{14}

\item \textit{Prove that $M \tens (N \oplus K) \cong (M \tens N) \oplus (M \tens K)$. The same is true for: 
$$
M \tens \left( \bigoplus_{\alpha \in \Lambda} N_\alpha \right) \cong \bigoplus_{\alpha \in \Lambda}(M \tens N_\alpha),
$$ 
which uses the same proof. But:
$$
M \tens \left( \prod_{\alpha \in \Lambda} N_\alpha \right)\ncong \prod_{\alpha \in \Lambda}(M \tens N_\alpha).
$$
}

Example: $R = \z,M = \Q,N_i = \z_{2^i},i = 1,2,...$ Consider:
$$
\Q \tens \left(\prod_{i = 1}^\infty \z_{2^i} \right) \neq 0,
$$
by 8.  But note:
$$
\prod_{i  =1}^\infty\left(\Q \tens \z_{2^i} \right) = 0. 
$$

\item \textit{Suppose $R$ is commutative and let $I$ and $J$ be ideals of $R$, so $R/I,R/J$ are naturally $R$-modules . }

\begin{enumerate}
\item \textit{Prove that every element of $R/I \tens_R R/J$ can be written as a simple tensor of the form $(1 \mod I) \tens (r \mod J)$. }

\begin{proof}
Let:
 $$
 t = a_1(b_1 \mod I \otimes c_1 \mod J) + \cdots + a_l(b_l\mod I \otimes c_l\mod J)  \in R/I \tens_R R/J,
 $$
 with $a_i,b_i,c_i \in R$. Then we have: 
 \bee
 t &= a_1b_1(1 \mod I \otimes c_1 \mod J) + \cdots + a_lb_l(1 \mod I \otimes c_l\mod J)\\
 &= (1 \mod I \otimes a_1b_1c_1 \mod J) + \cdots + (1 \mod I \otimes a_lb_lc_l\mod J)\\
 &= 1 \mod I \tens (a_1b_1c_1 + \cdots + a_lb_lc_l) \mod J,
 \eee
 so since $(a_1b_1c_1 + \cdots + a_lb_lc_l) \in R$, we have written $t$ as a simple tensor. 
\end{proof}

\item \textit{Prove that there is an $R$ module isomorphism $R/I \tens_R R/J \cong R/(I + J)$ mapping $(r \mod I) \tens (r' \mod J)$ to $rr' \mod (I + J)$. }

\begin{proof}
Let $\phi: R/I \tens_R R/J \to R/(I + J)$ be given by $\phi((r \mod I) \tens (r' \mod J)) = rr' \mod (I + J)$. We prove this is an isomorphism. Since we proved that every element of $ R/I \tens_R R/J$ can be written as a simple tensor of the form $(1 \mod I) \tens (r \mod J)$, we need only to check elements of this form.

\textbf{Homomorphism: } We have: 
\bee
&\phi((1 \mod I) \tens (r \mod J) + (1 \mod I) \tens (s \mod J))\\
=&\phi((1 \mod I) \tens (r + s \mod J))\\
=&r + s \mod (I + J)\\
=&r \mod (I + J) + s \mod (I+ J)\\
=&\phi((1 \mod I) \tens (r \mod J)) + \phi((1 \mod I) \tens (s \mod J)).
\eee
So addition is preserved, and for $a \in R$, we also have: 
\bee
\phi(a((1 \mod I) \tens (r \mod J))) &= \phi(((a \mod I) \tens (r \mod J)))\\
&= ar \mod (I + J)\\
&= a(r \mod (I + J))\\
&= a \phi((1 \mod I) \tens (r \mod J)).
\eee
So $\phi$ is an $R$-module homomorphism. 

\textbf{Injectivity: }Observe: 
\bee
\phi((1 \mod I) \tens (r \mod J)) &= \phi((1 \mod I) \tens (s \mod J)),
\eee
which gives us: 
\bee
r \mod (I + J) = s \mod (I + J),
\eee
thus we know $r - s \in (I + J)$. So $r - s = j \mod I$ for some $j \in J$. So we have: 
\bee
&(1 \mod I) \tens (r \mod J) - (1 \mod I) \tens (s \mod J)\\
=&(1 \mod I) \tens (r - s \mod J)\\
=&(r - s \mod I) \tens (1 \mod J)\\
=&(j \mod I) \tens (1 \mod J)\\
=&(1 \mod I) \tens (j \mod J)\\
=& 0.
\eee
So $\phi$ must be injective. 

\textbf{Surjectivity: }Let $r \mod (I + J) \in R/(I+ J)$. Then $\phi((1 \mod I) \tens (r \mod J)) = r \mod (I + J)$, so $\phi$ is surjective. Hence $\phi$ is an isomorphism. 
\end{proof} 
\end{enumerate}

\setcounter{enumi}{19}

\item \textit{Let $I = (2,x)$ be the ideal generated by $2$ and $x$ in the ring $R = \z[x]$. Show that the element $2 \tens 2 + x \tens x$ in $I \tens_R I$ is not a simple tensor, i.e., cannot be written as $a \tens b$ for some $a,b \in I$. }

\begin{proof}
Define $t = 2 \tens 2 + x \tens x$. We first express $t$ as a simple tensor in $R$. We define $\beta:\z[x] \times \z[x] \to \z[x] \tens \z[x]$ given by $\beta((p(x),q(x)) = p(x) \tens q(x)$. We also define $\gamma:\z[x] \times \z[x] \to \z[x]$ given by $\gamma((p(x),q(x)) = p(x)q(x)$. This map is bilinear, so we have an induced homomorphism $\phi:\z[x] \tens \z[x] \to \z[x]$, so altogether, we have:
\begin{center}
\begin{tikzcd}
 & \mathbb{Z}[x] \times \mathbb{Z}[x] \arrow[rd, "\gamma"] \arrow[ld, "\beta"'] &  \\
\mathbb{Z}[x] \otimes \mathbb{Z}[x] \arrow[rr, "\phi"] &  & \mathbb{Z}[x]
\end{tikzcd}.
\end{center}
 Then we would have:
$$
p \tens q = 2 \tens 2 + x \tens x,
$$
for some $p,q \in \z[x]$. But we also know: 
$$
2 \tens 2 + x \tens x = 4(1 \tens 1) + x \tens x = 4(1 \tens 1) + x^2(1 \tens 1) = (4 + x^2)(1 \tens 1) \in \z[x]
$$
But $(4 + x^2)$ is a prime in $\z[x]$. To write $t$ as a simple tensor in $\z[x]$, we must have $4 + x^2 = ab$ for some $a,b \in \z[x]$, so that we may write: 
$$
ab(1 \tens 1) = a \tens b \in \z[x].
$$
So let $4 + x^2 = ab$, and since it is a prime and we are in $\z[x]$, without loss of generality, we must have $b = 1$, but note that $1 \notin I$, so it is impossible to write $t$ as a simple tensor in $I \tens_R I$, since under the same bilinear map $\gamma$, we have:
\begin{center}
\begin{tikzcd}
 & I \times I \arrow[rd, "\gamma"] \arrow[ld, "\beta"'] &  \\
I \otimes I \arrow[rr, "\phi"] &  & I^2
\end{tikzcd},
\end{center}
from which we see that the image $u \tens v \mapsto uv$ of any simple tensor is reducible. 
\end{proof}
\begin{comment}
Leibman's solution: Note that when you map $I \tens I \to I^2$ and map $u \tens v \mapsto uv$, we map $2 \tens 2 + x \tens x$ to $x^2 + 4$ which is irreducible, but when we map a simple tensor, it must be irreducible. 
\begin{center}
\begin{tikzcd}
 & I \times I \arrow[rd, "\gamma"] \arrow[ld, "\beta"'] &  \\
I \otimes I \arrow[rr, "\phi"] &  & I^2
\end{tikzcd}.
\end{center}
\end{comment}

\item \textit{Suppose $R$ is commutative, and let $I$ and $J$ be ideals of $R$. }

\begin{enumerate}
\item \textit{Show that there is a surjective $R$-module homomorphism from $I \tens_R J$ to the product ideal $IJ$ mapping $i \tens j$ to the element $ij$. }

\begin{proof}
Let $\phi:I \tens_R J \to IJ$ be given by:
 $$
 \phi(r_1(i_1 \tens j_1) + \cdots + r_n(i_n \tens j_n)) = r_1i_1j_1 + \cdots + r_ni_nj_n.
 $$
  We show that $\phi$ is a surjective homomorphism of $R$-modules. Observe:
\bee
&\phi((r_1(i_1 \tens j_1) + \cdots r_n(i_n \tens j_n)) + (s_1(i_1' \tens j_1') + \cdots s_m(i_m' \tens j_m')))\\
= &\phi(r_1(i_1 \tens j_1) + \cdots r_n(i_n \tens j_n)+ s_1(i_1' \tens j_1') + \cdots s_m(i_m' \tens j_m'))\\
 = &r_1i_1j_1 + \cdots + r_ni_nj_n + s_1i_1'j_1' + \cdots + s_mi_m'j_m'\\
= &\phi((r_1(i_1 \tens j_1) + \cdots r_n(i_n \tens j_n)) + \phi((s_1(i_1' \tens j_1') + \cdots s_m(i_m' \tens j_m'))).
\eee
So $\phi$ preserves addition. Additionally:
\bee
\phi(r(i \tens j)) 
&= \phi((ri \tens j))\\
&= rij\\
&= r\phi((i \tens j)).
\eee
So $\phi$ also preserves scalar multiplication for simple tensors and thus for general tensors as well. Now we show that $\phi$ is surjective. Let $r \in IJ$. Then 
$$
r = \sum_{k = 1}^n i_kj_k,
$$ 
for $i_k \in I,j_k \in J$. Then $\phi(i_1 \tens j_1 + \cdots + i_n \tens j_n) = r$, because we already proved $\phi$ is a homomorphism and hence preserves addition, so $\phi$ is surjective. 
\end{proof}

\item \textit{Give an example to show that the map in (a) need not be injective [Exercise 10.4.17]. }


Consider $I = (2,x)$ and $R = \z[x]$. We define a map: $\phi:I \tens_R I \to II = I$ given by $\phi(i \tens j) = ij$. By part (a), we know it is a surjective homomorphism. Note:
$$
\phi(2 \tens x) = \phi(x \tens 2) = 2x.
$$
But from Exercise 10.4.17(c), we know that $2 \tens x \neq x \tens 2$ in $I \tens_R I$. 

\end{enumerate}
\end{enumerate}

\section{Exact sequences and tensor algebras}

\textbf{Monday, January 29th}

\begin{Def}
Let $M$ be an $R$-module. $\forall k \in \n$ let $\tau_k(M) = M \tens \cdots \tens M$ ($k$-times). These are called the set of $k$-tensors. Note that $\tau_0(M) = R$ and $\tau_1(M) = M$. We have:
$$
\tau(M) = \bigoplus_{K = 0}^\infty \tau_k(M),
$$
called the \textbf{tensor algebra of $M$}. 
\end{Def}

Elements look like sums:
$$
a + u_1 + b_2(u_2 \tens u_3) + b_3(u_5 \tens u_6 \tens u_7) + \cdots + d_8(u_9 \tens u_{10}).
$$

\begin{Def}
\textbf{Universal Property: }if $A$ is an $R$=algebra and $\phi: M \to A$ is a hom-sm of $R$-modules, then $\exists$ a unique hom-sm $\Phi:\tau(M) \to A$ of $R$-algebras such that:
\bee
\Phi|_{M = \tau_1(M)} &= \phi,
\Phi(u_1 \tens \cdots \tens u_k) &= \phi(u_1) \cdots \phi(u_k) \in A.
\eee
\end{Def}

\begin{Ex}
\begin{enumerate}
\item Let $M = R$. Then we have $\tau(R) = R \oplus R \oplus R \cdots$. The elements are finite multiplications. Let's artificially introduce basis vector. Let $M = Rx$, ($x = 1$). Where $x$ is the basis vector. Then 
\bee 
\tau_1(R) &= Rx,\\
\tau_2(R) &= R \tens R = R(x \tens x),\\
\tau_3(R) &= R(x \tens x \tens x),\\
\tau(R) &= R \oplus R \tens R \oplus R \tens R \tens R \oplus \cdots \cong R[x].
\eee

\item $M = R^2 = R\Set{x,y}$. Then:
$$
\tau(R) \cong \Set{\text{polynomials in non-commutative variables x and y. }} = RG,
$$
since $x \tens y \neq y \tens x$. where $G$ is the free semigroup generated by $x,y$:
$$
G = \Set{1,x,y,x^2,xy,yx,y^2,xyx,...}.
$$
\end{enumerate}
\end{Ex}

\begin{Def}
\textbf{Symmetric algebra of $M$}. Let $\mathcal{C}(M)$ be the ideal in $\tau(M)$ generated by $u \tens v - v\ tens u, u,v \in M$. The algebra $\mathcal{S}(M) = \tau(M)/\mathcal{C}(M)$ is called the \textbf{symmetric algebra of $M$.}
\end{Def}
So in the above definition, we just declare that $x \tens y = y \tens x$, and you can switch them along any chain of tensor products:
$$
u_1 \tens u_2 \tens u_3 \tens u_4 = u_4 \tens u_2 \tens u_3 \tens u_1 \in \mathcal{S}(M).
$$

The ability to do it on more than two tensors follows from the structure of transpositions in symmetric groups. What even is a higher degree tensor? We prove something:
\begin{proof} 
$\forall k$ $S_k$ acts of $\tau_k(M)$ by $\sigma(u_1 \tens \cdots u_k) = u_{\sigma(1)} \tens \cdots \tens u_{\sigma(k)}$. The action of any transposition is trivial module $\mathcal{C}(M)$ or something like that. 
\end{proof}

\begin{Def}
\textbf{Universal Property: }If $A$ is a commutative $R$-algebra, and $\phi:M \to A$ is a hom-sm of $R$-modules, then there is a unique hom-sm $\Phi:\mathcal{S}(M) \to A$ such that $\Phi|_{M} = \phi$. Also $\mathcal{S}(M)$ is still a \textbf{graded algebra} (Definition \ref{gradedalgebra}):
$$
\mathcal{S}(M) = R \oplus \mathcal{S}_1(M) \oplus \mathcal{S}_2(M) \oplus \cdots
$$
where the second term is $\cong M$. Why is it unique? Because we have no choice as to send $\Phi(u_1 \tens \cdots u_k) = \phi(u_1) \cdots \phi(u_k)$. 
\end{Def}

\begin{Ex}
Take $M$ to be the free module generated by two elements $M = R\Set{x,y}$. And then $\mathcal{S}(M) \cong R[x,y]$ (now we have commutativity!).
\end{Ex}

\begin{Def}
\textbf{Exterior Algebra: }let $A(M)$ be the ideal in $\tau(M)$ generated by tensors $u \tens u$, $u \in M$. The algebra:
$$
\Lambda(M) = \tau(M)/A(M),
$$
is called the exterior algebra of $M$. 
\end{Def}

In $\Lambda(M)$, instead of $\tens$, we write "$\wedge$" - wedge. So we have:
$$
\Lambda(M) = \Set{a + u_1 + u_2 \wedge u_3 + u_4 \wedge u_5 \wedge u_6 + \cdots ()}.
$$
In $\Lambda(M)$, $u \wedge u = 0$ for all $u$, and:
$$
u \wedge v = -v \wedge u.
$$
\begin{proof} 
$$
(u + v) \wedge (u + v) = u \wedge u + u \wedge v + v \wedge u + v \wedge v.
$$
\end{proof}

$ \int fdx \neq \int f(\phi(t))dt$. But we do have:
$$
\int fdx = \int f(\phi(t))\phi'(t) dt.
$$
$fdx$ is called the \textbf{differential form}. And we have $\int f(x,y)dx \wedge dy$, where $f(x,y)dx \wedge dy$ is called a differential form of second order. Recall: 
$$
\int f(x,y)dx \wedge dy = \int f(x(u,v),y(u,v))\cdot |J|du\wedge dv.
$$
Where $|J|$ is the Jacobian of $(u,v) \mapsto (x,y)$ as vertical vectors. 

\textbf{Bonus Problem: } If $x= au + bv$ and $y = cu + dv$ prove that $x \wedge y = \det \left(\begin{matrix}
a & b\\
c & d
\end{matrix}\right) u \wedge v$. 

Now what the hell is going on when our ring is \textbf{noncommutative}. Let $M,N$ be a left $R$-module. Can you have: 
$$
(au) \tens v = u \tens av?
$$
But then we have the following:
$$
(abu) \tens v = (bu) \tens (av) = u \tens (bav) = (bau) \tens v.
$$
So we have some weird new hidden relations. So now:
$$
ab(u \tens v) = ba (u \tens v),
$$
$R$ acts on $M \tens N$ as a commutative ring. 

\begin{rem}
If $M$ is a right $R$-module and $N$ is a left $R$-module, product $M \tens_R N$ where:
$$
(ua) \tens v = u \tens (av),
$$ and this will be okay, no problem like this. But you cannot take scalars out. So it is not equal to $a(u \tens v)$. It is an abelian group only, not an $R$-module. Mappings with this property are called \textbf{balanced maps} since they are missing one of the four bilinearity properties. 
\end{rem}

\textbf{Tuesday, January 30th}

\begin{Def}
A diagram of sets and mappings is a \textbf{commutative diagram} if for all paths with common starting and ending points, the composition of mappings along these paths is the the same. 
\end{Def}

\begin{Def}
A sequence $A_{i -1} \rightarrow^{\phi_{i - 1}} A_i\rightarrow^{\phi_{i}} A_{i + 1} \rightarrow^{\phi_{i + 1}} A_{i + 2} \rightarrow$ of hom-sms of groups, rings, or modules is \textbf{exact} if $\forall i$ $Image(\phi_i) = ker(\phi_{i  +1})$. 
\end{Def}

\begin{rem}\label{exact1}
$0 \to A \to_{\phi} B$ is exact if and only if $\phi$ is injective. ($ker(\phi) = Image(0)$)
\end{rem}

\begin{rem}\label{exact2}
$B \to_\psi C \to 0$ is exact if and only if $\phi$ is surjective. ($\psi(B) = ker(C \to 0) = C$)
\end{rem}

\begin{rem}
The sequence $0 \to A \to_{\phi} B \to_{\psi} C \to 0$ is exact (a \textbf{short exact sequence}) if and only if $\phi$ is injective (monomorphism),$\psi$ is surjective (epimorphism), and $\phi(A) = ker(\psi)$. So we have $C \cong B/\phi(A)$, $\phi(A) \cong A$. 
\end{rem}

For groups: $1 \to A \to B \to C \to 1$, $C \cong B/A$. 

\begin{Def}
The short exact sequence \textbf{splits} if there exists a hom-sm $\sigma:C \to B$ called a \textbf{section}, such that $\psi \circ \sigma = Id_C$. So we have:
\begin{center}
\begin{tikzcd}
0 \arrow[r] & A \arrow[r, "\phi"'] & B \arrow[r, "\psi"'] & C \arrow[r] \arrow[l, "\sigma"', bend right] & 0
\end{tikzcd}
\end{center}
In this case, then $B \cong A \oplus C$,
$$
B = \phi(A) \oplus \sigma(C),
$$
with an internal direct product. 
\end{Def}

\begin{proof} 
$\forall b \in B$:
\bee
\psi(b - \sigma(\psi(b))) &= \psi(b) - \psi \circ \sigma(\psi(b))\\
&= \psi(b) - \psi(b) = 0.
\eee
So $b - \sigma(\psi(b)) \in \phi(A)$, so $b = \sigma(c) + \phi(a)$ for $c = \psi(b)$ and some $a \in A$. If $\phi(a) = \sigma(c)$, then:
$$
0 = \psi(\phi(a)) = \psi(\sigma(c)) = c,
$$
so $\sigma(c) = 0$, and $\phi(a) = 0$, so $\sigma(C) \cap \phi(A) = 0$. 
\end{proof}

\begin{Def}
A homomorphism of two short exact sequences is a commutative diagram of the sort:
\begin{center}
\begin{tikzcd}
0 \arrow[r] & A \arrow[r] \arrow[d, "\alpha"] & B \arrow[r] \arrow[d, "\beta"] & C \arrow[r] \arrow[d, "\gamma"] & 0 \\
0 \arrow[r] & A' \arrow[r] & B' \arrow[r] & C' \arrow[r] & 0
\end{tikzcd},
\end{center}
with exact rows. 
\end{Def}

\begin{lem}[\textbf{SHORT FIVE LEMMA}]
Let:
\begin{center}
\begin{tikzcd}
0 \arrow[r] & A \arrow[r, "\phi"] \arrow[d, "\alpha"] & B \arrow[r, "\psi"] \arrow[d, "\beta"] & C \arrow[r] \arrow[d, "\gamma"] & 0 \\
0 \arrow[r] & A' \arrow[r, "\phi'"] & B' \arrow[r,"\psi'"] & C' \arrow[r] & 0
\end{tikzcd},
\end{center}
be a homomorphism of short exact sequences. 
\begin{enumerate}[]
\item[]
\begin{enumerate}
\item If $\alpha$ and $\gamma$ are surjective, then $\beta$ is surjective. 
\item If $\alpha$ and $\gamma$ are injective, then $\beta$ is injective. 
\item If $\alpha$ and $\gamma$ are isomorphisms, then $\beta$ is an isomorphism. 
\end{enumerate}
\end{enumerate}
\end{lem}

\begin{proof} 
\textbf{(b)} We make use of the diagram! Let $\alpha$,$\gamma$ be injective. Let $b \in B$, and assume that $\beta(b) = 0$. Consider:
\bee 
\gamma(\psi(b)) = \psi'(\beta(b)) = \psi'(0) = 0.
 \eee 
 But $\gamma$ is injective, so $\psi(b) = 0$. But the first row is exact, so $b = \phi(a)$ for some $a \in A$. Then:
 \bee 
 \phi'(\alpha(a)) = \beta(\phi(a)) = \beta(b) = 0,
  \eee 
  But $\phi',\alpha$ are injective, so $a = 0$, so $b = \alpha(a) = 0$. 
\end{proof}

\begin{proof} \textbf{(a)}
Let $\alpha,\gamma$ be surjective. Let $b' \in B'$ (we need tos how that $b' = \beta(b)$ for some $b \in B$). So take $\psi'(b')$ but then since this is in $C'$ and $\gamma$ is surjective, so there exists $c \in C$ such that $\gamma(c) = \psi'(b')$. Next, $\psi$ is surjective, so we have $\hat{b} \in B$ such that $\psi(\hat{b}) = c$. So we have: 
$$
\gamma(c) = \gamma(\psi(\hat{b})) = \psi'(b').
$$
Consider: 
$$
\psi'(b' - \beta(\hat{b})) = \psi'(b') - \psi'(\beta(\hat{b})) = \gamma(c) - \gamma(\psi(\hat{b})) = 0.
$$
The second row is exact, so $\exists a' \in A'$ such that:
$$
\phi'(a') = b' - \beta(\hat{b}),
$$
$\alpha$ is surjective, so $\exists a \in A$ such that $a' = \alpha(a)$. 
\textbf{If a row is exact, this means the image of first map is the kernel of the second map. } Take $b = \hat{b} + \phi(a)$. Then:
\bee 
\beta(b) = \beta(\hat{b}) + \beta(\phi(a)) = \beta(\hat{b}) +  \phi'(\alpha(a)) = \beta(\hat{b}) + b' - \beta(\hat{b}) = b'. 
\eee 
Part (c) is a corollary of the others. 
\end{proof}

\begin{lem}[\textbf{SNAKE LEMMA}]
$\coker(\alpha) = A'/\alpha(A)$. We have a commutative diagram with exact rows and columns.
%
\[
  \xymatrix@!{
              && {\ker(\alpha)} \ar[r]   & {\ker(\beta)} \ar[r]   & {\ker(\gamma)}
                    \ar`r[d]`[l]`^d[lll]|!{[];[d]}\hole|!{[l];[dl]}\hole|!{[ll];[dll]}\hole
                    `[dddll]|!{[ddllll];[ddll]}\hole [dddll]
                                                                       &   \\
              && A  \ar[r]^{\psi}      & B  \ar[r]^{\phi}      & C \ar[r]    & 0 \\
    0 \ar[rr] && A\pp \ar[r]^{\psi'}   & B\pp \ar[r]^{\phi'}   & C\pp        &   \\
              && {\coker(\alpha)} \ar[r] & {\coker(\beta)} \ar[r] & {\coker(\gamma)} &   \\
    % vertical arrows
    \ar"1,3";"2,3"   \ar"1,4";"2,4"   \ar"1,5";"2,5"
    \ar"2,3";"3,3"^a \ar"2,4";"3,4"^b \ar"2,5";"3,5"^c
    \ar"3,3";"4,3"   \ar"3,4";"4,4"   \ar"3,5";"4,5"
  }
\]
Then there exists a homomorphism $\delta:ker\gamma \to ker\alpha$ such that the snake is exact. 
\end{lem}

\textbf{Wednesday, January 31st}


\begin{rem}
Let $M$ be an $R$-module. Let $A$ be a submodule of module $B$. Then it may be that $A \tens M$ is not a submodule of $B \tens M$!

Consider:
\begin{center}
\begin{tikzcd}
0 \arrow[r] & A \arrow[r, "\phi"] & B
\end{tikzcd}
\end{center}
which is exact! Then we have:
\begin{center}
\begin{tikzcd}
0 \arrow[r] & A \otimes M \arrow[r, "\phi \otimes Id_M"] & B \otimes M
\end{tikzcd},
\end{center}
which may not be exact. 
\end{rem}

\begin{Ex}
In $\z \tens_\z \z_2$ we have:
$$
\forall n \in \z,2n \tens 1 = n \tens 2 = 0.
$$ 
So observe:
\begin{center}
\begin{tikzcd}
2\mathbb{Z} \otimes_{\mathbb{Z}} \mathbb{Z}_2 \arrow[r, "\phi"'] & \mathbb{Z} \otimes_\mathbb{Z} \mathbb{Z}_2
\end{tikzcd},
\end{center}
where we have $\phi \tens Id = 0$. Before we had an embedding: 
\begin{center}
\begin{tikzcd}
0 \arrow[r] & 2\mathbb{Z} \arrow[r, "\phi"', hook] & \mathbb{Z}
\end{tikzcd},
\end{center}
but it is no longer an embedding when we take the tensor product with $\z_2$. 
\end{Ex}

We have a similar example: 
\begin{Ex}
\begin{center}
\begin{tikzcd}
0 \arrow[r] & \mathbb{Z} \arrow[r, "\phi"', hook] & \mathbb{Q} \\
0 \arrow[r] & \mathbb{Z} \otimes \mathbb{Z}_2 \arrow[r] & \mathbb{Q} \otimes \mathbb{Z}_2
\end{tikzcd}
\end{center}
where taking the tensor product with $\z_2$, we have $\z \tens \z_2 \cong \z_2$, but $\Q \tens \z_2 = 0$. So it isn't injective. 
\end{Ex}

\begin{rem}
The tensor product preserves surjectivity, but not injectivity. 
\end{rem}
\begin{lem}
If:
\begin{center}
\begin{tikzcd}
B \arrow[r, "\psi"'] & C \arrow[r] & 0
\end{tikzcd}
\end{center}
is exact ($\psi$ is surjective), then:
\begin{center}
\begin{tikzcd}
B \otimes M \arrow[r, "\psi \otimes Id"'] & C \otimes M \arrow[r] & 0
\end{tikzcd}
\end{center}
is exact ($\psi \tens Id$ is surjective).
\end{lem}

\begin{proof}
$\forall c \in C,u \in M$, find $b \in B$ s.t. $\psi(b) = c$,
then 
$$
(\psi \tens Id)(b \tens u) = c \tens u.
$$
 And simple tensors generate $C \tens M$, so $\psi \tens Id(B \tens M) = C \tens M$. 
\end{proof}

\begin{theorem}
If:
\begin{center}
\begin{tikzcd}
0 \arrow[r] & A \arrow[r, "\phi"'] & B \arrow[r, "\psi"'] & C \arrow[r] & 0
\end{tikzcd}
\end{center}
is exact ($\Rightarrow \psi \circ \phi = 0$, then:
\begin{center}
\begin{tikzcd}
A \otimes M \arrow[r, "\phi \otimes Id"'] & B \otimes M \arrow[r, "\psi \otimes Id"'] & C \otimes M \arrow[r] & 0
\end{tikzcd}
\end{center}
is exact. (Note that it is still exact on the right, but not on the left, since the zero is dropped)
\end{theorem}

\begin{proof}
First, $(\psi \tens Id)\cdot(\phi \tens Id) = 0$.
$$
(\psi \tens Id)((\phi \tens Id)(a \tens u)) = (\psi \tens Id)(\phi(a) \tens u) = \psi(\phi(a)) \tens u = 0 \tens u = 0.
$$
So we have a hom-sm: $\gamma:B \tens M/((\psi \tens Id)(A \tens M)) \to C \tens M$. We claim this is an isomorphism, so $ker(\psi \tens Id) = (\phi \tens Id)(A \tens M)$. 
\begin{proof}
Define a hom-sm $C \tens M \to M B \tens M/((\psi \tens Id)(A \tens M))$ by:
$$
c \tens u \mapsto b \tens u \mod ((\psi \tens Id)(A \tens M)),
$$
where $b$ is s.t. $\psi(b) = c$. Why is it well defined? If $b'$ is another element in $B$ s.t. $\psi(b') = c$, then:
$$
b' = b \mod \phi(A),
$$
 so $b' \tens u = b \tens u \mod (\phi(A) \tens M)$. So it's well defined. Next we check that it's bilinear, it's obvious. Claim is that this is inverse of $\gamma$ and it is, we skip the details. 
\end{proof}
\end{proof}

\begin{rem}
Recall that if we have $\phi:M \to N$, $K \sub M$, and $\phi(K) = 0$ then we must have a hom-sm $M/K \to N$. 
\end{rem}

\begin{rem}
$\tens M$ is a \textbf{functor} from the category of $R$-modules to itself:
$A \Rightarrow A \tens M$, and $A \to B \Rightarrow A \tens M \to B \tens M$. 
\end{rem}

\begin{Def}
A \textbf{functor} from category $\mathcal{C}_1$ to category $\mathcal{C}_2$ is a "mapping" that maps objects to objects and morphisms to morphisms, and preserves compositions of morphisms:
\begin{center}
\begin{tikzcd}
 & B \arrow[rr, "F"] &  & F(B) \\
A \arrow[ru, "\phi"] \arrow[rr, "F"] &  & F(A) \arrow[ru, "F(\phi)"] & 
\end{tikzcd}.
\end{center}
\end{Def}

\begin{Def}
A functor is \textbf{exact} if it maps short exact sequences to short exact sequences. 
\end{Def}

\begin{rem}
$\tens M$ is a \textbf{right-exact} functor (loses exactness at the left term). 
\end{rem}

\begin{Def}
A functor is \textbf{right-exact} if for any short exact sequence:
\begin{center}
\begin{tikzcd}
0 \arrow[r] & A \arrow[r, "\phi"'] & B \arrow[r, "\psi"'] & C \arrow[r] & 0
\end{tikzcd},
\end{center}
the sequence:
\begin{center}
\begin{tikzcd}
F(A) \arrow[r, "F(\phi)"'] & F(B) \arrow[r, "F(\psi)"'] & F(C) \arrow[r] & 0
\end{tikzcd}
\end{center}
is exact. 
\end{Def}


\begin{rem}
If $R$ is non-commutative, $M$ is a left $R$-module, then $\tens m$ is a functor from category of right $R$-modules to the category of abelian groups. 
\end{rem}

There are good modules that preserve exact sequences. They are called flat modules. 

\begin{Def}
$M$ is \textbf{flat} if whenever:
\begin{center}
\begin{tikzcd}
0 \arrow[r] & A \arrow[r, "\phi"'] & B
\end{tikzcd}
\end{center}
is exact, the sequence:
\begin{center}
\begin{tikzcd}
0 \arrow[r] & A \otimes M \arrow[r, "\phi \otimes Id"'] & B \otimes M
\end{tikzcd}
\end{center}
is exact. In this case, $\tens M$ is an exact functor. 
\end{Def}

Now what modules are flat? The ring $R$ itself is flat. 



\begin{rem} We state some results concerning flat modules. \\
\begin{enumerate}
\item $R$ is flat:
\begin{center}
\begin{tikzcd}
0 \arrow[r] & A \arrow[r] & B
\end{tikzcd}
\end{center}
$\Rightarrow$
\begin{center}
\begin{tikzcd}
0 \arrow[r] & A \otimes R \arrow[r, "\phi \otimes Id"'] & B \otimes R
\end{tikzcd}
\end{center}
=
\begin{center}
\begin{tikzcd}
0 \arrow[r] & A \arrow[r] & B
\end{tikzcd}
\end{center}
is exact. 

\item If $M = M_1 \oplus M_2$, then $M$ is flat if and only if both $M_1,M_2$ are flat. 

\begin{proof}
Let $0 \rightarrow A \rightarrow_\phi B$ be exact. Then:
$$
0 \rightarrow A \tens M \rightarrow_{\phi \tens Id} B \tens M
$$ is isomorphic to
$$
0 \to (A \tens M_1) \oplus (A \tens M_2) \to_{\tilde{\phi}} (B \tens M_1) \oplus (B \tens M_2).
$$
The things on the left hand side of the $\oplus$ are connected by $\phi \tens Id_{M_1}$ and RHS by $\phi \tens Id_{M_2}$. And $\tilde{\phi} = (\phi_1,\phi_2)$ and $\tilde{\phi}$ is injective if and only if $\phi_1,\phi_2$ are. 
\end{proof}

\item So $R^n$ (free module of finite rank) is flat. The finiteness is not necessary, it's an exercise in the book. 
\item If $M$ is a direct summand of a free module, i.e. there exists $M$ s.t. $M \oplus N$ is free, then $M$ is flat. 

\item If $M_1,M_2$ are flat, then $M_1 \tens M_2$ is flat. 

\begin{proof}
$\tens M_1$ is exact as a functor, and $\tens M_2$ is exact, so $\tens (M_1 \tens M_2) = (\tens M_1) \tens M_2$ is exact. 
So if we have:
$$
0 \to A \to B
$$
 then
 $$
 0 \to A \tens M_2 \to B \tens M_2
 $$
  then 
  $$
  0 \to A \tens (M_1 \tens M_2) \to B \tens M_1 \tens M_2.
  $$

\end{proof}

\item If $M$ is flat and $I$ is an ideal in $R$, then $I \tens M \to IM$ is an isomorphism. This is standard mapping which maps $a \tens u \mapsto au$. 

\begin{proof}
$0 \to I \to R$ is exact, so:
$$
0 \to I \tens M \to R \tens M \cong M
$$
 is exact. And this is a mapping that maps $a \tens u \mapsto a \tens u \mapsto au$. So $I \tens M \to M$ is injective. The inverse of this mapping is just $IM$. So actually this is an isomorphism of modules. 
\end{proof}

\item Assume $R$ is an integral domain. Then if $Tor(M) \neq 0$, then $M$ is not flat. 

\begin{proof}
$0 \to R \to Q$-the field of fractions. 
\end{proof}

\end{enumerate}

\end{rem}

\textbf{Thursday, February 1st}

\begin{lem}\label{lem10.148}
If $R$ is an integral domain and $M$ is a flat $R$-module, then $Tor(M) = 0$. 
\end{lem}

\begin{proof}
Let $Q$ be the field of fractions of $R$. Then:
$$
0 \to R \to Q
$$
 is exact. So:
$$
0 \to R \tens M \to Q \tens M
$$
 is exact. But $R \tens M \cong M$ under the isomorphism $\phi:1 \tens u \to u$., where $ker\phi = Tor(M)$. So if $u \neq 0$ is in $Tor(M)$, then $1 \tens u \neq 0 \in R \tens M$, but is zero in $Q \tens M$, so $R \tens M \to Q \tens M$ is not injective, which is a contradiction since we said the above sequence is exact. 
\end{proof}

\begin{lem}
The converse of Lemma \ref{lem10.148} is not true: if $Tor(M) = 0$, it may not be flat. 
\end{lem}
We give a counterexample:
\begin{Ex}
Let $R = F[x,y]$ and let $M = I = (x,y)$. Then $M$ is torsion-free, but:
$$
I \tens M \to IM
$$ 
is not an isomorphism. $x \tens y - y \tens x \mapsto 0$. It was one of the properties of flat modules that for any ideal in $R$, the above map must be an isomorphism, thus $M$ is not flat. 
\end{Ex}

Flatness is related to torsion. 

\begin{lem}\label{lem10.151}
If $R$ is an integral domain and $Q$ is its field of quotients, then $Q$ is a flat $R$-module. 
\end{lem}

You can take $S^{-1}R$ for any multiplicatively closed set and this will be a flat $R$-module.

\begin{proof}
The reason for this is that $Q$ is a union of free $R$-modules, copies of $R$. It consists:
$$
Q = \bigcup_{d \neq 0}d^{-1}R.
$$
Let:
$$
0 \to A \to B,
$$
be exact. So $\phi:A \to B$ is injective. Then $(R^*)^{-1}A \cong A \tens Q \to_{\phi \tens Id} B \tens Q \cong (R^*)^{-1}B$. So we have:
$$
\phi \tens Id(\fracc{u}{r} = \fracc{\phi(u)}{r}, u \in A,r \in R.
$$
And $\fracc{\phi(u)}{r}=0$ if and only if $a \phi(u) = 0$ for some $a \neq 0$. Then $\phi(au) = 0$, and since $\phi$ is injective, $au = 0$, so $\fracc{u}{r} = 0$ is in $(R)^*)^{-1}A$. So $\phi \tens Id$ is injective. 
\end{proof}


Refer to Remarks \ref{exact1},\ref{exact2}. For equivalent definitions of exactness. 

We discuss \textbf{projective and injective modules.} 

\begin{Def}
Let $R$ be commutative unital. Let $M$ be an $R$-module. 

Functors: $Hom_R(m,\cdot)$ and $Hom_R(\cdot,M)$. 

For any $R$-module $A$, we have new modules $Hom_R(M,A)$ and $Hom_R(A,M)$. 
\end{Def}

If $\phi:A \to B$ is a hom-sm, then we have a hom-sm $Hom(M,A) \to Hom(M,B)$. How is it defined? We have:
\begin{center}
\begin{tikzcd}
A \arrow[rr, "\phi"'] &  & B \\
 & M \arrow[lu, "f"] \arrow[ru, "\phi \circ f"'] & 
\end{tikzcd},
\end{center}
so $f \to \phi \circ f$. And if we have one more module, we have:
\begin{center}
\begin{tikzcd}
A \arrow[rr, "\phi"'] &  & B \arrow[r, "\psi"] & C \\
 & M \arrow[lu, "f"] \arrow[ru, "\phi \circ f"] \arrow[rru, "\psi \circ \phi \circ f"'] &  & 
\end{tikzcd}.
\end{center}
So:
\bee
Hom(M,A) &\to Hom(M,B) \to Hom(M,C)\\
f & \mapsto \phi \circ f \mapsto \psi(\phi \circ f) = (\psi \circ \phi) \circ f.
\eee

\begin{theorem}
If $0 \to A \to B \to C \to 0$ is exact, then:
$$
0 \to Hom(M,A) \overset{\tilde{\phi}}{\to} Hom(M,B) \overset{\tilde{\psi}}{\to} Hom(M,C)
$$
is exact, i.e. the functor $Hom(M,\cdot)$ is left exact, but not exact, since exactness is not preserved on the right. 
\end{theorem}

\begin{proof}
We have:
\begin{center}
\begin{tikzcd}
0 \arrow[r] & A \arrow[r, "\phi"] & B \arrow[r, "\psi"] & C \arrow[r] & 0 \\
 &  & M \arrow[lu, "f"] \arrow[u, "\phi \circ f"] &  & 
\end{tikzcd}. 
\end{center}
Assume that $\phi \circ f = 0$. $\phi$ is injective by definition of exactness. And we have $\phi(f(a)) = 0, \forall u \in M$, so $f(u) = 0, \forall u \in M$, since $\phi$ is injective, so $f = 0$. Thus we have proved that $\tilde{\phi}$ is injective.. 

 Now consider $f \in Hom(M,A) \mapsto \phi \circ f \in Hom(M,B) \mapsto \psi \circ \phi \circ f \in Hom(M,C)$. And $\psi \circ \phi \circ f = 0$ since $\psi \circ \phi = 0$ by exactness. So  $Image(\tilde{\phi}) \sub ker(\tilde{\psi})$. Now let $g \in ker(\tilde{\psi})$, that is, $\psi \circ g = 0$. Then:
$$
\psi\rvert_{g(M)} = 0.
$$
So $g(M) \sub ker\psi$, so $g(M) \sub \phi(A)$. Then we have $f: M \to A$ defined by $f(u) = \phi^{-1}(g(u))$. So $g = \phi \circ f = \phi^{-1}(f)$. The inverse is well defined since $\phi$ is injective. 
\end{proof}

We give counterexample to show that it is not exact on the right. 

\begin{Ex}
Let $M = \z_2$. Note $\z \to \z_2 \to 0$ is exact, we have a map $h: \z_2 \to \z_2$, the identity map, but there's no map $g$ from $\z_2$ to $\z$ s.t. $h = \psi \circ g$, where $\psi$ is the map from $\z to \z_2$. 
\end{Ex}

\begin{Def}
$M$ is \textbf{projective} if $Hom(M,\cdot)$ is exact: $\forall$ surjective $\psi:B \to C$ and $h:M \to C$ there exists $g:M \to B$ s.t. $h = \psi \circ g$:
\begin{center}
\begin{tikzcd}
B \arrow[r] & C \arrow[r] & 0 \\
 & M \arrow[u, "h"] \arrow[lu, "g"] & 
\end{tikzcd}. 
\end{center}
So we know $Hom(M,B) \to Hom(M,C) \to 0$ is exact. 
\end{Def}

\begin{rem}
We list some properties of projective $R$-modules. 

\begin{enumerate}
\item If $M = M_1 \oplus M_2$, then $M$ is projective if and only if $M_1$ and $M_2$ are. Proof is easy apparently. 

\item $R,R^n$ are projective, and any free module is projective. This follows from the first property for free modules of finite rank. 
\begin{proof}
Take $e_i \to c_i$. Find $b_i \in B$ s.t $\psi(b_i) = c_i$ for all $i$, and define $g(e_i) = b_i$. Done. 
\end{proof}

\item If $M$ is a direct summand of a projective module, then it is projective. This is just a reformulation of the first property. And this is a criterion. 
\end{enumerate}
\end{rem}

\begin{theorem}
$M$ is projective if and only if $M$ is a direct summand of a free module: $ \exists N$ s.t. $M \oplus N$ is free. 
\end{theorem}

\textbf{Friday, February 2nd}

\begin{Def}
Recall that $M$ is \textbf{projective} if $\forall$ exact $B \overset{\phi}{\to} C \to 0$ and $h:M \to C$ there exists $g:M \to B$ s.t. $h = \phi \circ g$:
\begin{center}
\begin{tikzcd}
B \arrow[r] & C \arrow[r] & 0 \\
 & M \arrow[u, "h"] \arrow[lu, "g"] & 
\end{tikzcd}. 
\end{center}
\end{Def}

\begin{Def}
$Hom(M,\cdot)$ is an \textbf{exact functor:} if $0 \to A \to B \to C \to 0$ is exact, then $0 \to Hom(M,A) \to Hom(M,B) \to Hom(M,C) \to 0$ is exact. (Equivalent defn to above). 
\end{Def}


\begin{rem}
If $M$ is projective and $B \overset{\phi}{\to} $ is surjective then $\exists$ a section of $\phi:$ $s:M \to B$ s.t. $\phi \circ s = Id_M$. Indeed, we have: 
\begin{center}
\begin{tikzcd}
B \arrow[r, "\phi"] & M \arrow[r] & 0 \\
 & M \arrow[lu, "s"] \arrow[u, "Id_M"] & 
\end{tikzcd}
\end{center}
so there exists $s$ s.t. $\phi\circ s = Id_M$. Recall that a section is a map from $M \to B$ s.t. it maps the image of an element from surjective hom-sm back to the same element. 
\end{rem}

\begin{rem}
If $M$ is projective, then any short exact sequence $0 \to A \to B \to M \to 0$ splits s.t. $B \cong A \oplus M$. 
\end{rem}

In particular, since $M$ is a quotient of a free module $F \to M \to 0$, if $M$ is projective, then $M$ is a direct summand of a free module, since we have a section from $M \to F$ that makes it a direct summand. Conversely, if $M$ is a direct summand of a free module, it is projective: $F = M \oplus N$. 

\begin{Def}
$0 \to A \overset{\phi}{\to} B \overset{\psi}{\to } C \to 0$ splits "from the left" if $\exists \pi:B \to A$ s.t. $\pi\circ \phi = Id_A$. 
\end{Def}
\begin{proof}
 Let $C' = ker\pi$. We claim $\psi|_{C'}$ is isomorphic $C' \cong C$. Indeed $\psi$ is surjective (because short exact), and if $b \in C'$ and $\psi(b) = 0$. Then $ker\psi = \phi(A)$, by short exact, so if $b \in ker\psi$, then $b = \phi(a)$ for some $a \in A$. Then $\pi(b) = a$ by definition of section. But if $b \in C'$, $\pi(b) = 0$, so $a = 0$, so $b = 0$. So we proved that $b \in ker\pi \Rightarrow b = 0$. So $\pi$ is injective. Now we claim that $\phi(A) + C' = B$. So since $\phi$ is a bijection, we know $\phi(A) \cong A$. 
 \begin{proof}
 Let $b \in B$, let $a = \pi(b)$. Then $b - \phi(a) \in C'$, since $\pi(b - \phi(a)) = a - a = 0$. So $b \in \phi(A) + C'$. 
 \end{proof}
 Now claim $C' \cap  \phi(A) = 0$. If $b \in C'$, then $\pi(b) = 0$, and if $b = \phi(a)$, then $\pi(b) = \pi(\phi(a)) = a = 0$, so $b = 0$. More work to be done here. 
\end{proof}


We discuss injective modules. Which is related to $Hom(\cdot,M)$. Fix $M$. Let's consider this functor. You have module $A \Rightarrow$ module $ Hom(A,M)$. If you have $A \overset{\phi}{\to} B$, then you have $Hom(B,M) \overset{\tilde{\phi}}{\to} Hom(A,M)$. 
\begin{center}
\begin{tikzcd}
A \arrow[rd, "f"] \arrow[rr, "\phi"] &  & B \arrow[ld, "g"] \\
 & M & 
\end{tikzcd}
$f = g \circ \phi$. We have $\tilde{\phi} = g \circ \phi$. This is a \textbf{contravariant functor} - it inverts arrows (morphisms). What we had before was called a \textbf{covariant} functors ("normal" ones). 
\end{center}
\begin{Def}
\textbf{Covariant: } $A \Rightarrow F(A)$:
$$
A \to B \Rightarrow F(A) \to F(B).
$$
\end{Def}

\begin{Def}
\textbf{Contravariant: } $A \Rightarrow F(A)$:
$$
A \to B \Rightarrow F(B) \to F(A).
$$
Note $F(B)$ is first here on the right side. 
\end{Def}

\begin{theorem}
$Hom(\cdot,M)$ is left exact: 
if $0 \to A \to B \to C \to 0$ is exact, then
$0 \to Hom(C,M) \to Hom(B,M) \to Hom(A,M)$ is exact. 
\end{theorem}

\begin{proof}
Left as an exercise to the reader, it is straightforward. (wtf)
\end{proof}

It may not be exact, tho. If $0 \to A \overset{\phi}{\to}B$ is exact ($\phi$ is injective):
\begin{center}
\begin{tikzcd}
0 \arrow[r] & A \arrow[r, "\phi"] \arrow[d, "f"] & B \arrow[ld, "g"] \\
 & M & 
\end{tikzcd}
\end{center}
and we have $f: A \to M$ we need $g:B \to M$ s.t. $f = g \circ \phi$. 
We give a counterexample:
\begin{Ex}
Consider: 
\begin{center}
\begin{tikzcd}
0 \arrow[r] & \mathbb{Z} \arrow[r, "\cdot 2"] \arrow[d, "Id"] & \mathbb{Z} \arrow[ld, "g"] \\
 & \mathbb{Z} & 
\end{tikzcd}
\end{center}
Note $2\z \sub \z$ but we have no such $g$. We have no map going from $2n \to n$. 
\end{Ex}

\begin{Def}
$M$ is \textbf{injective} if $\forall 0 \to A \overset{\phi}{\to} B$ (exact) and $\forall f: A \to M$ there exists $g: B \to M$ s.t. $f = g \circ \phi$. 
\end{Def}

\begin{rem}
$M$ is injective if and only if $Hom(\cdot,M)$ is an exact functor. 
\end{rem}

\begin{rem}
\begin{enumerate}
\item $M = M_1 \oplus M_2$ is injective if and only if both $M_1$ and $M_2$ are injective. 
\item $R$ is not injective, generally speaking. ($\z$ is not injective $\z$-module)
\item $Q$ is an injective $\z$-module. 
\end{enumerate}
\end{rem}

\begin{lem}
Any module is a quotient of a free module, so, of a projective module. 
\end{lem}

\begin{theorem}
Any module is a submodule of an injective module. 
\end{theorem}


\textbf{Monday, February 5th}


\begin{Def}
A module $M$ is \textbf{divisible} if $\forall$ nonzero divisor $a \in R$, $\forall u \in M$, there is a $v \in M$ s.t. $av = u$. That is, $M  \to M$, where $v \mapsto av$ is surjective. So if $R$ is an integral domain it is always true?
\end{Def}

\begin{rem}
If $M$ is injective, then $M$ is divisible. 
\end{rem}

\begin{rem}
Let $R$ be an integral domain, if $M$ is divisible, and either $M$ is torsion free or $R$ is a PID, then $M$ is injective. 
\end{rem}

\begin{lem}[\textbf{Baer's criterion}]
$M$ is injective if $\forall$ ideal $I$ of $R$, $\forall f: I \to M$ there exists $g:R \to M$ s.t $g|_I = f$. So this proves that the field of fractions is injective. 
\end{lem}

\begin{rem}
$M$ is projective if and only if: if $M$ is a quotient module of some module $B$, $B \to M \to 0$, then we have $B \cong N \oplus M$. And in fact we have $0 \to N \to B \to M \to 0$. 
\end{rem}

\begin{rem}
Injective if and only if $0 \to M \to B$ implies $B \cong N \oplus M$. $M \sub B$. Then there exists $N \sub B$ s.t. the above is true. So $M$ is injective if and only if if $M$ is a submodule of $B$ then $M$ is a direct summand of $B$. 
\end{rem}

\begin{rem}
$\prod^\infty \Q \cong \bigoplus_{\alpha \in \Lambda}\Q$. This is only because on the left, that is a vector space. And thus it has a basis. 
\end{rem}

\begin{lem}
$M = \prod_{i = 1}^\infty \z$ is not free. 
\end{lem}

\begin{proof}
Let $N = \bigoplus_{I = 1}^\infty \z \sub M$. Assume that $M$ is free, let $B$ be a basis. There exists $B' \sub B$ which is countable s.t. $N \sub N' = \z B'$. For $u \in N$, let$B_u \sub B$ finite be s.t. $u \in RB_u$. Then:
$$
B' = \cup_{u \in N}B_u.
$$
which is countable. Let $\tilde{M} = M/N'$ a free module, $\cong R(B/B')$. Note $B$ is uncountable. If $K$ is a free $\z$-module, then $K$ is not divisible: 
$\forall v \in K$, $v = (0,...,n_i,...,n_j,0,...)$ $v$ is only divisible by $\gcd(N_i,...,n_j)$. So no element of $k$ is divisible if it is nonzero. Recall that $v$ is divisible if and only if $\forall k \neq 0$, there exists $w$ s.t. $kw = v$. We claim $\tilde{M}$ has divisible elements, so we have contradiction. Take:
$$
u = (\pm 1,\pm 2!,\pm 3!,\pm 4!,...) \in M,
$$
In $\tilde{M}$, $N = 0$, so take:
$$
\bar{u} = (0,0,...,0,k!,(k + 1)!,...) \in \tilde{M},
$$
 which is divisible by $k$ for all $k$. We have uncountably many of such $u$, not all of them are in $N'$, so there exists such $u$ with $\bar{u} \neq 0$ in $\tilde{M}$. 
\end{proof}


















\section*{10.5 Exercises}

\begin{enumerate}[label=\arabic*.]
\item \textit{Suppose that: }
\begin{center}
\begin{tikzcd}
A \arrow[r, "\psi"] \arrow[d, "\alpha"] & B \arrow[r, "\phi"] \arrow[d, "\beta"] & C \arrow[d, "\gamma"] \\
A' \arrow[r, "\psi'"] & B' \arrow[r, "\phi'"] & C'
\end{tikzcd}
\end{center}
\textit{is a commutative diagram of groups and that the rows are exact. Prove that: }
\begin{enumerate}
\item \textit{If $\phi$ and $\alpha$ are surjective, and $\beta$ is injective then $\gamma$ is injective. }
\begin{proof}
Let $c \in ker\gamma$. Then we know there exists $b \in B$ s.t. $\phi(b) = c$, since $\phi$ is surjective. Note that $\phi'(\beta(b)) = \gamma(\phi(b)) = \gamma(c) = 0$ since it is a commutative diagram. So we know $\beta(b) \in ker\phi' = \psi'(A')$ since the bottom row is exact, so we know there exists $a' \in A'$ s.t. $\psi'(a') = \beta(b)$. And since $\alpha$ is surjective, we know there exists $a \in A$ s.t. $\alpha(a) = a'$. Then since $\psi'(\alpha(a)) = \beta(b)$, and the diagram is commutative, we know we must have $\beta(\psi(a)) = \psi'(\alpha(a))  = \beta(b)$. Now since $\beta$ is injective, we know $b = \psi(a)$. But recall that $c = \phi(b) = \phi(\psi(a)) = 0$ since the top row is exact. Thus since $ker\gamma = 0 \in C$, we know that $\gamma$ is injective. 
\end{proof}

\item \textit{If $\psi',\alpha$, and $\gamma$ are injective, then $\beta$ is injective. }

\begin{proof}
Let $\beta(b) = 0$ for some $b \in B$. Then we have $\phi'(\beta(b)) = 0 = \gamma(\phi(b))$ by commutativity. Since $\gamma$ is injective, we know $\phi(b) = 0$, so $b \in \psi(A)$. So there exists $a \in A$ s.t. $\psi(a) = b$. Now note that since we have commutativity we know $\beta(\psi(a)) = 0 = \psi'(\alpha(a))$. But since $\alpha$ and $\psi'$ are both injective, we know $a = 0$, hence $\psi(a) = b = 0$, and $\beta$ is injective. 
\end{proof}

\item \textit{If $\phi,\alpha$ and $\gamma$ are surjective, then $\beta$ is surjective. }

\begin{proof}
Let $b' \in B$. Then $\phi'(b') \in C'$. So there exists $c \in C$ s.t. $\gamma(c) = \phi'(b')$ since $\gamma$ is surjective and there exists $b \in B$ s.t. $\phi(b) = c$ since $\phi$ is surjective. So we know $\gamma(\phi(b)) = \phi'(\beta(b)) = \phi'(b')$. So $\phi'(\beta(b) - b') = 0$, so $\beta(b) - b' \in ker\phi' = \psi'(A')$. So we know there exists $a' \in A'$ s.t. $\psi'(a') = \beta(b) - b'$. But since $\alpha$ is surjective and $a' \in A'$, we know there exists $a \in A$ s.t. $\psi'(\alpha(a)) = \psi'(a') = \beta(b) - b'$. So we must have that $\beta(\psi(a)) = \beta(b) - b'$ by commutativity. For $b-\psi(a)$ we then have $\beta(b-\psi(a))=\beta(b)-\beta(b)+b'=b'$, which proves that $\beta$ is surjective.
\end{proof}

\item \textit{If $\beta$ is injective, $\alpha$ and $\phi$ are surjective, then $\gamma$ is injective. }

\begin{proof}
Let $c \in C$ s.t. $\gamma(c) = 0$. Then since $\phi$ is surjective we have $b \in B$ s.t. $\phi(b) = c$. Now take $\psi'(\beta(b)) = 0$ by commutativity since $\gamma(\phi(b)) = 0$. Then we know $\beta(b) \in ker\phi' = \psi'(A)$ so we have $a' \in A'$ s.t. $\psi'(a') = \beta(b)$. And since $\alpha$ is surjective we have $a \in A$ s.t. $\alpha(a)  = a'$. So we have $\psi'(\alpha(a)) = \beta(b) = \beta(\psi(a))$ by commutativity. But since $\beta$ is injective we know $\psi(a) = b$ But then $b \in \phi(A) = ker\phi$ so $\phi(b) = 0 = c$. So $\gamma$ is injective. 
\end{proof}

\item \textit{If $\beta$ is surjective, $\gamma$ and $\psi'$ are injective, then $\alpha$ is surjective. }

\begin{proof}
Let $a' \in A$. Then $\phi'(\psi'(a)) = 0 \in C'$. Also since $\beta$ is surjective we have $b \in B$ s.t. $\beta(b) = \psi'(a')$. Now $\gamma(\phi(b)) = \phi'(\beta(b))$ by commutativity, but $\phi'(\beta(b)) = \phi'(\psi'(a')) = 0$, so $\gamma(\phi(b)) = 0$, and since $\gamma$ is injective, $\phi(b) = 0$. Thus by exactness, we have $a \in A$ s.t. $\psi(a) = b$. Now take $\psi'(\alpha(a)) = \beta(\psi(a)) = \psi'(a')$, and by injectivity of $\psi'$, we know $\alpha(a) = a'$. So $\alpha$ is surjective. 
\end{proof}
\end{enumerate}

\end{enumerate}








\chapter{Vector spaces}

\section{Definitions and basic theory}

\textbf{Thursday, February 8th}

\begin{Def}
Recall that a \textbf{linear transformation} is just an $R$-module homomorphism when the module is in fact a vector space. 
\end{Def}

Let $R$ be a commutative unital ring. We know:
\bee
Hom_R(M_1 \oplus M_2,N) &\cong Hom_R(M_1,N) \oplus Hom_R(M_2,N)\\
Hom_R(M,N_1 \oplus N_2) &\cong Hom_R(M,N_1) \oplus Hom_R(M,N_2)
\eee
So we have $\phi\leftrightarrow(\phi \circ \xi_1, \phi \circ \xi_2)$, where $\xi:M_1 \to M_1 \oplus M_2$ and $\xi_2:M_2 \to M_1 \oplus M_2$. Applying $\phi$ to these maps the stuff in $M_1 \oplus M_2$ to $N$. And we also have for the bottom one: $\phi \leftrightarrow(\pi \circ \phi, \pi \circ \phi)$, where $\pi_1:N_1 \oplus N_2 \to N_1$ and $\pi_2:N_1 \oplus N_2 \to N_2$. In the second case, this homomorphism has coordinates, and we get them by just projecting the output of $\phi$ using $\pi_1,\pi_2$. 

\begin{lem}
$Hom_R(R,R) \cong R$, proved in homework. 
\end{lem}
\begin{Cor}\label{cor113}
$Hom(R^m,R^n) \cong R^{mn}$. 
\end{Cor}

$\forall M$, we have $Hom_R(R,M) \cong M$. $\phi \mapsto \phi(1) = u$. Then $\phi(a) = au$ for all $a$. How do we construct this mapping? If we have $\phi: R^m \to R^n$, then we have $mn$ new homomorphisms. So let $\phi:R^m \to R^n$. $\forall i,j$ let $\phi_{i,j}: R \to R$ be given by $\phi_{i,j} = \pi_i \circ \phi \circ \xi_j \leftrightarrow a_{i,j} \in R$. What is this $a_{i,j}$? So $\phi_{i,j}(b) = a_{i,j}b$. Any $\psi:R\ to R$ $\psi \leftrightarrow \psi(1) = a$, and $\psi(b) = ab$. For a single homomorphism we have $m \times n$ numbers which define these $\phi$:
$$
A = \lpar 
\begin{matrix}
a_{11}& \cdots &a_{1m}\\
\vdots & & \vdots\\
a_{n1}& \cdots & a_{nm}
\end{matrix} \rpar 
$$
which is called the matrix of $\phi$. $\pi_i: R^n \to R$ and maps to $(a_1,..,a_n) \mapsto a_i$. And $\xi_j:R\to R^m$ maps $a \mapsto (0,...,0,a,0,...,0)$, where the $a$ is in the $j$-th column. The $j$-th column:
$$
\lpar 
\begin{matrix}
a_{1j}\\
\vdots\\
a_{nj}
\end{matrix} \rpar  \phi(v_j).
$$

where:
$$
v_j = 
\lpar 
\begin{matrix}
0\\
\vdots\\
1\\
\vdots\\
0
\end{matrix} \rpar.
$$
The $i$-th row $(a_{i1},...,a_{im})$ - the $i$th component of $\phi$, the matrix of $\pi_i \circ \phi$. We also have $\phi + \psi \leftrightarrow A + B$ where they are the matrices of $\phi,\psi$. And $\forall c \in R$, we have $c \phi \leftrightarrow cA$. 

Let $\phi:R^m \to R^n$, $\psi:R^n \to R^k$. Then we have $\psi \circ phi:R^m \to R^k$. Let $A$ be the matrix of $\phi$ and $B$ be the matrix of $\psi$. Note $A$ is $m \times n$ and $B$ is $k \times n$. The matrix of $\psi \circ \phi$ is called the product of $B$ and $A$ and is denoted by $BA$. Note $BA$ is $k \times m$. 

Let $M$ be a free module. 

Let $M$ be a free module, $M \cong R^m$. Let $\Set{u_1,...,u_m}$ be the basis in $M$ corresponding to this isomorphism. Then $\forall u \in M$, $u = c_1u_1 + \cdots + c_mu_m$:
$$
u \leftrightarrow \lpar 
\begin{matrix}
c_1\\
\vdots\\
c_m
\end{matrix} \rpar \in R^m.
$$

Let $\Set{v_1,...,v_m}$ be another basis in $M$. If $R$ is commutative, then any two bases have the same number of elements. Now:
 $$
 u \leftrightarrow \lpar 
 \begin{matrix}
 b_1\\
 \vdots\\
 b_m
\end{matrix}  \rpar  \in R^m.
$$
 in this basis. So $u = b_1v_1 + \cdots + b_mv_m$. So we do a change of basis or something like that. So we have an isomorphism from $M \to R^m$ by our first basis $\Set{u_i}$ and another by our second basis $\Set{v_i}$. Call the first one old and the second one new:
 \begin{center}
 \begin{tikzcd}
 & R^m \arrow[dd] \\
M \arrow[ru, "\cong"] \arrow[rd, "\cong"] &  \\
 & R^m
\end{tikzcd}
\end{center}
The matrix of the isomorphism between the old and new is called the \textbf{transition matrix}. 

\vspace{5mm}

\section{The matrix of a linear transformation}
\textbf{Friday, February 9th}
\vspace{5mm}

So we study free modules of finite rank. We know if we have: $\phi:R^m \to R^n \Rightarrow$ an $n \times m$ matrix:
$$
A = \lpar 
\begin{matrix}
a_{11}& \cdots &a_{1m}\\
\vdots & & \vdots\\
a_{n1}& \cdots & a_{nm}
\end{matrix} \rpar. 
$$
Let $u = (c_1,...,c_m)$,
Note $\phi(u)$ is part of a mapping:
$$
1 \to u = \lpar 
\begin{matrix}
c_1\\
\vdots\\
c_n
\end{matrix} \rpar \mapsto \phi(u).
$$
So the matrix of $1 \mapsto \phi(u)$ is:
$$
\lpar 
\begin{matrix}
b_1\\
\vdots\\
b_n
\end{matrix} \rpar = A
\lpar 
\begin{matrix}
c_1\\
\vdots\\
c_n
\end{matrix} \rpar. 
$$
which are the coordinates of $\phi(u)$. We map $R \to R^m$ sending $1 \mapsto u$, and then $u \mapsto \phi(u) \in R^n$. Remember that $A$ is the matrix representation of the homomorphism $\phi$. 

So $Hom(R^m,R^n) \leftrightarrow Mat_{m \times n}(R) \cong R^{mn}$. The basis is given by: $\Set{E_{ij}}$ where:
$$
E_{ij} = \lpar 
\begin{matrix}
0 & \cdots & 0\\
\vdots & 1 &\vdots\\
0& \cdots & 0
\end{matrix} \rpar.
$$
with a $1$ as the $i,j$-th entry and zeroes elsewhere. So we have a basis $\phi_{ij}$ where $\phi_{ij}(u_j) = v_i$ and $\phi_{ij}(u_k) = 0$ $\forall k \neq j$. Where $u_1 = \lpar \begin{matrix}
1\\
0\\
\vdots\\
0
\end{matrix} \rpar \in R^m$, and $u_1 = \lpar \begin{matrix}
1\\
0\\
\vdots\\
0
\end{matrix} \rpar \in R_n$. 

Now $M \cong R^m$, we have an "old" basis $\Set{u_1,...,u_m}$ and a new basis $\Set{v_1,...,v_m}$. Then we have: 
\begin{center}
 \begin{tikzcd}
 & R^m \arrow[dd, "\cong"] \\
M \arrow[ru, "\cong"] \arrow[rd, "\cong"] &  \\
 & R^m
\end{tikzcd},
\end{center}
where the top is the "old" and the bottom is the "new". This implies a transition matrix $P$, and we have:
$$
u = c_1u_1 + \cdots + c_mu_m = b_1v_1 + \cdots + b_mv_m.
$$
Where:
$$
\lpar 
\begin{matrix}
b_1\\
\vdots\\
b_m
\end{matrix} \rpar = P\lpar 
\begin{matrix}
c_1\\
\vdots\\
c_m
\end{matrix} \rpar \Rightarrow \lpar 
\begin{matrix}
c_1\\
\vdots\\
c_m
\end{matrix} \rpar = P^{-1}\lpar 
\begin{matrix}
b_1\\
\vdots\\
b_m
\end{matrix} \rpar. 
$$

So we have $\phi:M \to M$ where $M \cong R^m$ and $N \cong R^n$, which gives us a matrix $A$ of $\phi$, corresponding to chosen bases. Change bases in $M$, and $N$, with transition matrices $P$ and $Q$:
\begin{center}
\begin{tikzcd}
 & R^m \arrow[r, "A"] \arrow[dd, "{P, \cong}", bend left] & R^n \arrow[d, "\cong", no head] \arrow[dd, "{Q,\cong}", bend left=49] \\
M \arrow[ru, "\cong", no head] \arrow[rd, "\cong", no head] \arrow[rr, "\phi", bend left] &  & N \arrow[d, "\cong", no head] \\
 & R^m \arrow[r, "A'"] & R^n
\end{tikzcd}.
\end{center}
In new coordinates, the matrix of $\phi$ is $QAP^{-1} = A'$. 

\begin{Def}
If $\phi: M \to M$, then after change of coordinates, with transition amtrix $P$, the "new" matrix of $\phi$ is $PAP^{-1}$. It is called \textbf{similar matrix of $A$} (or \textbf{conjugate}). 
\end{Def}



\textbf{Monday, February 12th}

We review matrices: 
\begin{center}
\begin{tikzcd}
 & R^n \arrow[dd] \\
V \arrow[rd, "\cong"] \arrow[ru, "\cong"] &  \\
 & R^n
\end{tikzcd}
\end{center}
Recall that the vertical arrow is the map that we are representing as a matrix. We say the columns are the coordinates of elements of the old basis in the new basis. But the book's definition has the columns as coordinates of the new basis in the old basis. 

Observe: 
\begin{center}
\begin{tikzcd}
R^m \arrow[rd, "\cong", no head] \arrow[dd, "P"] \arrow[rrr, "A"] &  &  & R^n \arrow[dd, "Q"] \\
 & M \arrow[r, "\phi"] & N \arrow[ru, "\cong", no head] \arrow[rd, "\cong", no head] &  \\
R^m \arrow[ru, "\cong", no head] \arrow[rrr, "A'"] &  &  & R^n
\end{tikzcd}
\end{center}
where $A' = QAP^{-1} = PAP^{-1}$. 

Now what about finite-dimensional vector spaces? These are free modules of finite rank over a field $F$. 
\begin{rem}
All modules over a field are free. 
\end{rem}

What can we say about them? They are all free, so they are all projective, and injective. 

\begin{rem}
All finite-dimensional vector spaces are both projective, and injective. Thus any short exact sequence splits. i.e. any subspace or quotient space is a direct summand. 
\end{rem}

\begin{proof}
Let $W$ be a subspace of $V$. Find a basis $B$ in $W$. Any vector space is a free module: has a basis. Then we want to construct a basis $A$ in $V$, so that $B \sub A$. Put $U = F(A \setminus B)$. Then $V  = W \oplus U$. 

For finite dimensional vector spaces, $W = F\Set{u_1,...,u_k}$. Let $B = \Set{u_1,...,u_k}$ be a basis in $W$. We find $u_{k + 1},...,u_n$ s.t. $\Set{u_1,...,u_n}$ is a basis in $V$. Then $V = W \oplus U$, where $U = F\Set{u_{k + 1},...,u_n}$. 
\end{proof}

\begin{rem}
If $W \sub V$ then $\dim W \leq \dim V$. If $W \subset V$, then $\dim W < \dim V$. 
\end{rem}

\begin{Ex}
We give an example that it is not true for all modules, must be vector spaces. 
$2\z \subset \z$ with the same rank. 
\end{Ex}

\begin{rem}
If $V = W \oplus U$, then $\dim V = \dim W + \dim U$. 
\end{rem}


\begin{rem}
If $\dim V = n$. If $C$ is a linearly independent set in $V$, then $|C| \leq n$. If $|C| = n$, then $C$ is a basis of $V$. 
\end{rem}

\begin{rem}
If $V  =FC = Span(C)$, then $|C| \geq n$. If $|C| = n$, then $C$ is a basis of $V$. Note here that $C$ is not necessarily a linearly independent set in this remark. 
\end{rem}


\begin{Def}
Let $V$ be $m$ dimensional, and $W$ be $n$ dimensional. Given $\phi:V \to W$, the \textbf{rank of a linear tranformation} $Rank\phi = \dim \phi(V)$. 
\end{Def}

\begin{Def}
Then \textbf{rank of matrix }$A$ = rank of corresponding homomorphism = dimension of column space of $A$ = subspace of $F^n$ generated (spanned) by the columns of $A$. 
\end{Def}

\begin{lem}
$Rank\phi = \dim\phi(V) = \dim V = \dim (ker\phi)$. 
\end{lem}

\begin{proof}
$V = ker\phi \oplus U$, $U \cong \phi(V)$:
\begin{center}
\begin{tikzcd}
0 \arrow[r] & ker\phi \arrow[r] & V \arrow[r] & \phi(V) \arrow[r] \arrow[l, bend left] & 0
\end{tikzcd}. 
\end{center}
\end{proof}

\begin{lem}
The rank of the column space is equal to the rank of the row space. 
\end{lem}

\begin{rem}
Rank is basis-invariant. 
\end{rem}

\begin{rem}
If two matrices are \textbf{similar}, then there is a linear transformation to get from one to the other. 
\end{rem}

\begin{Def}
$\phi:V \to V$. $u$ is an \textbf{eigenvector} of $\phi$ if $\phi(u) = cu$ for some $c \in F$. $c$ is called the \textbf{eigenvalue} of $u$, and of $\phi$. 
\end{Def}

\begin{rem}
We discuss what it means for two modules to be "equal" as opposed to "isomorphic". Consider $V = \R_{x,y,z}^3$. Let $W_1 = \R_{x,y}^2$, and let $W_2 = \R_x$. Then $V \cong W_1 \oplus W_2$. But they are not equal, since they live in different spaces. (isomorphic to the external direct sum), proving equality is proving "equality" with respect to the internal direct sum object. 
\end{rem}




\section*{11.2 Exercises}

\begin{enumerate}[label=\arabic*.]

\item \textit{Let $V$ be the collection of polynomials with coefficients in $\Q$ in the variable $x$ of degree at most 5. Determine the transition matrix from the basis $1,x,x^2,...,x^5$ ("old basis") for $V$ to the basis $1, 1 + x, 1 + x + x^2, ...,1 + x+ x^2 + x^3 + x^4 + x^5$ ("new"basis) for $V$. }

Transition matrix (expressing the "new" basis as linear combinations of "old" basis: 
$$
\lpar 
\begin{matrix}
1 & 1 & 1 & 1& 1 & 1\\
0 & 1 & 1 & 1& 1 & 1\\
0 & 0 & 1 & 1& 1 & 1\\
0 & 0 & 0 & 1& 1 & 1\\
0 & 0 & 0 & 0& 1 & 1\\
0 & 0 & 0 & 0& 0 & 1\\
\end{matrix} \rpar. 
$$

\item \textit{$V$ is the same as above. $\phi:V \to V$, $\phi(p) = p'$ (differentiate). We find the matrices of $\phi$ with respect to the two bases from the above exercise. }

Matrices of $\phi$:
\bee
\phi(1) &= 0\\
\phi(x) &= 1\\
\phi(x^2) &= 2x\\
&\vdots
\eee
So we have: 
$$
A = \lpar 
\begin{matrix}
0 & 1 & 0 & 0 & 0 & 0\\
0 & 0 & 2 & 0 & 0 & 0\\
0 & 0 & 0 & 3 & 0 & 0\\
0 & 0 & 0 & 0 & 4 & 0\\
0 & 0 & 0 & 0 & 0 & 5\\
0 & 0 & 0 & 0 & 0 & 0\\
\end{matrix} \rpar .
$$
Now for the next basis we have:
\bee
\phi(1) &= 0\\
\phi(1 + x) &= 1\\
\phi(1 + x + x^2) &= 1 + 2x = -1 + 2(1 + x)\\
\phi(1 + x + x^2+ x^3) &= 1 + 2x + 3x^2 = -1 - 1(1 + x) + 3(1 + x + x^2)\\
&\vdots
\eee
So we have:
$$
B = \lpar 
\begin{matrix}
0 & 1 & -1 & -1 & -1 & -1\\
0 & 0 & 2 & -1 & -1 & -1\\
0 & 0 & 0 & 3 & -1 & -1\\
0 & 0 & 0 & 0 & 4 & -1\\
0 & 0 & 0 & 0 & 0 & 5\\
0 & 0 & 0 & 0 & 0 & 0\\
\end{matrix} \rpar .
$$

\setcounter{enumi}{7}

\item 

\begin{proof}
$\Set{u_1,...,u_n}$ - eigenvalues of $\phi$. Meaning that $u_i$ are eigenvectors for all $i$. Then matrix of $\phi$ in this basis is:
$$
A = \lpar 
\begin{matrix}
c_1 & 0 & 0 & 0 & 0 & 0\\
0 & \ddots & 0 & 0 & 0 & 0\\
0 & 0 & \ddots & 0 & 0 & 0\\
0 & 0 & 0 & \ddots & 0 & 0\\
0 & 0 & 0 & 0 & \ddots & 0\\
0 & 0 & 0 & 0 & 0 & c_n\\
\end{matrix} \rpar .
$$
\end{proof}


\textit{If the matrix $A$ if $\phi$ is similar to a diagonal matrix, then $\phi$ has an \textbf{eigenbasis}. }

\begin{proof}
$A = PA'P^{-1}$, $A'$ is diagonal. Use $P$ as a transition matrix - construct a new basis using $P$. Set:
$$
v_i = Pu_i.
$$
for all $i$. In this basis, the matrix of $\phi$ is $A'$. So $\phi(v_i) = c_iv_i$ for all $i$. So we have: 
$$
A = \left[ 
\begin{matrix}
c_1 & 0 & 0 & 0 & 0 & 0\\
0 & \ddots & 0 & 0 & 0 & 0\\
0 & 0 & \ddots & 0 & 0 & 0\\
0 & 0 & 0 & \ddots & 0 & 0\\
0 & 0 & 0 & 0 & \ddots & 0\\
0 & 0 & 0 & 0 & 0 & c_n\\
\end{matrix} \right] . 
$$
\end{proof}


\setcounter{enumi}{10}

\item \textit{Let $\phi$ be a linear transformation from the finite dimensional vector space $V$ to itself such that $\phi^2 = \phi$. }

\begin{enumerate}
\item \textit{Prove that $Image(\phi) \cap ker\phi = 0$. }

\begin{proof}
Note that $\phi:V \to V$. Let $I = Image(\phi)$ and let $K = ker\phi$. Let $a \in K$. Then $\phi(a) = 0$. Then let $a \in I$. Then there exists $b \in V$ s.t. $\phi(b) = a$. But then note:
$$
\phi^2(b) = \phi(\phi(b)) = \phi(a) = 0 = \phi(b) = a.
$$
So $a = 0$, hence $K \cap I = 0$. 
\end{proof}

\item \textit{Prove that $V = Image(\phi)\oplus ker\phi$. }

\begin{proof}
We prove that $V = Image\phi + ker\phi$. Since $Image\phi \sub V$ and $ker\phi \sub V$, we know that if $v \in Image\phi$ and $w \in ker\phi$, then $v,w \in V$, so $v + w \in V$. So $Image\phi + ker\phi \sub V$. We prove the other inclusion. Now let $a \in V$. If $a \in ker\phi$ then we are done. So let $a \notin ker\phi$. Then $\phi(a) = b \neq 0 \in V$. Then we have: 
\bee
\phi(b - a) &= \phi(b) - \phi(a) = \phi(b) - \phi^2(a)\\
 &= \phi(b) - \phi(\phi(a)) = \phi(b) - \phi(b) = 0.
\eee
So we know that $b - a \in ker\phi$. So then $a - b \in ker\phi$ since $\phi$ is a linear transformation. Now note: 
$$
\phi(a) + (a - b) = b + a - b = a.
$$
and since $\phi(a) \in Image(\phi)$ and $a - b \in ker\phi$, we have shown $V \sub Image\phi + ker\phi$. Thus $V = Image\phi + ker\phi$, and since we showed they have zero intersection in the last part, we have proved $V = Image\phi \oplus ker\phi$. 
\end{proof}



\item \textit{Prove that there is a basis of $V$ s.t. the matrix of $\phi$ with respect to this basis is a diagonal matrix whose entries are all $0$ or $1$. }

\begin{proof}
Let $A = \Set{v_1,...,v_k}$ be a basis for $\phi(V)$. Then let $B = \Set{v_{k + 1},...,v_n}$ be a basis for $ker\phi$. We know this basis must have $n - k$ elements since $A \cup B$ must be a basis for $V$ since we proved the direct sum in the last part. Now recall that the coefficient matrix of $\phi$ with respect to any basis $C$ is given by $(a_{ij})$ where $\phi(c_i) = \sum_j a_{ij}c_j$. So we find the matrix of $\phi$ with respect to $A \cup B$. Let $v_i \in A \cup B$. Suppose $v_i \in A$. Then $v_i = \phi(w)$ for some $w \in V$. So we have $\phi(v_i) = \phi^2(w) = \phi(w) = v_i$. So the $i$-th column of the $i$-th row must be a $1$ and all other entries in that column are zero. And since $v_i \in A$, we know that $i \leq k$. Now let $v_i \in B$. Remember they are disjoint by part (a). Then $\phi(v_i) = 0$, so the $i$-th column is all zeroes. Thus we have constructed the matrix of $\phi$ with respect to the basis $A \cup B$, and it is a diagonal matrix with only ones and zeroes along the diagonal.  
\end{proof}
\end{enumerate}

\end{enumerate}

\section{Dual vector spaces}


\textbf{Friday, February 9th}


\begin{Def} \label{def11.5}
If $M$ is an $R$-module, then the \textbf{dual module} of $M$ is $M^* = Hom_R(M,R) = \Set{f:M \to R}$. These elements are called "linear forms" on $M$. 
\end{Def}

Sometimes this dual module is just zero. 

\begin{Ex}
\begin{enumerate}
\item $\z_n^* = Hom(\z_n,\z) = 0$. 
\item $R^* = Hom_R(R,R) \cong R$. By $f \mapsto f(1)$.
\item $(R^n)^* =Hom_R(R^n,R) \cong R^n$. "the dual module of a free module of finite rank is free". This is by $f \leftrightarrow(f(u_1),...,f(u_n))$, where $u_i$ are elements of the standard basis. 
\item If $M = \bigoplus_{i = 1}^\infty R$. This is $Hom(M,R) = M^*$. Note since this is a free module, so any homomorphism is defined by its actions on the basis elements. For any choice of $f(u_i) \in R$, we have $f \in M^*$. So we can choose any sequence of elements of $R$. We could do the same thing with uncountably many. So $M^* \cong \prod_{i = 1}^\infty R = \Set{(a_1,...):a_i \in R}$. This is by $f \leftrightarrow(f(u_1),f(u_2),...)$. 
\end{enumerate}
\end{Ex}

\begin{rem}
If $R$ is noncommutative and $M$ is a left-module, then $M^*$ is a right module. We define $f(a)(u) = f(au)$. We can check that this satisfies the structure of a right module. 
\end{rem}


$M^* \Rightarrow M^{**} = (M^*)^*$. And we have a natural homomorphism $M \to M^{**}$. An element $u \mapsto F_u$ where $F_u(f) = f(u)$. What the fuck is $f$. For $f \in M^*$. Note $u$ acts on linear forms. We have a pairing: $u \in M$, $f \in M^*$, then $\langle f,u \rangle = f(u) \in R$. Elements of $M^*$ are called \textbf{"covectors"}. And this mapping is linear with respect to $u$ and linear with respect to $f$. This natural mapping is always defined, but it is not necessarily an isomorphism, since we could have that $M^* = 0$, so $M^{**} = 0$, so not injective. And we could also have that $M^*$ and double star are much much larger than $M$, as in the case of infinite direct sums. Then we lose surjectivity. 

We can also just write $u(f) = f(u)$. 

\begin{rem}
$M \to M^{**}$ is not necessarily an isomorphism. 
\end{rem}

\begin{lem}
But if $M \cong R^n$, then $M \to M^{**}$ is a natural isomorphism. 
\end{lem}
\begin{proof}
This is because it maps basis vectors to basis vectors. Let $\Set{u_1,...,u_n}$ be a basis of $M$. 

\begin{Def}
We have a \textbf{dual basis} in $M^* \cong R^n$. Note:
$$
\Set{f_1,...,f_n}:f_i(u_j) = \begin{cases}
1 & i = j\\
0 & i \neq j
\end{cases} = \delta_{ij}.
$$
Note also that $R^n \to R$ matrices are $(a_1,...,a_n)$, $f(u_i) = a_i$. Then $f_1,...,f_n$ are the linear forms whose matrices are:
$$
(1,0,...,0),(0,1,0,...,0),...,(0,...,0,1).
$$
In the book, the dual basis is $\Set{u_1^*,...,u_n^*}$. Note $f_1$ is a mapping that maps $u_1$ to $1$ and maps all other basis vectors to zero!! So the $f_i$s are a basis of $M^*$. These are the dual basis. 
\end{Def}
I have no idea if we actually finished this proof or not. But it seems that we have moved on. 


\begin{Def}
Formally: 
$$
M^{**} = Hom(Hom(M,R),R).
$$
\end{Def}

Note if you have $Hom(R^m,R^n)$, then we have a natural homomorphism that maps $i$-th basis vector to $j$-th basis vector in the image and all other basis vectors to zero. 

"Why even bother with this dual, never mind the dual of the dual?" -Ryan

"In functional analysis, it's popular. What is the dual of a measure? Something that acts on measures... There are reflexive spaces where you just return to the... $M^{**} = M$. Otherwise...(unitelligible). " - Leibman

\begin{rem}
Note that the dual basis of the dual basis is the original basis!

If $\Set{f_1,...,f_n}$ is the dual basis of $\Set{u_1,...,u_n}$, then images of $\Set{u_1,...,u_n}$ in $M^{**}$ (they are $F(u_i)$) is the dual basis of $\Set{f_1,...,f_n}$:
$$
\langle f_i,u_j\rangle = \delta_{ij}.
$$
\end{rem}

And from this, we have the conclusion of the proof from earlier. Since basis vectors go to basis vectors, we are done. We have $M \overset{F}{\to} M^{**}$ is an isomorphism. 
\end{proof}

Now assume that we have two modules $\phi:M \to N$ be a homomorphism, then we have the \textbf{dual homomorphism} $\phi^*:N^* \to M^*$ defined by $\phi^*(f) = f \circ \phi$:
\begin{center}
\begin{tikzcd}
M \arrow[rd, "\phi^*(f) = f \circ \phi"'] \arrow[r, "\phi"] & N \arrow[d, "f"] \\
 & R
\end{tikzcd}.
\end{center}
So $*$-duality is a contravariant functor. 

\bb
\textbf{Tuesday, February 13th}
\bb

Consider $V$, and $W^* \sub V^*$. Take a basis here:
$$
\Set{f_1,...,f_k,f_{k + 1},...,f_n}.
$$
 Where up to $k$ is a basis for $W^*$. So this gives us a basis $\Set{u_1,...,u_n}$ in $V$, somehow... 
 
 \begin{Def}
 We review the \textbf{dual of a basis}. If we have a basis $\Set{u_1,...,u_n}$ in $V$, the \textbf{dual basis} in $V^*$ is $\Set{f_1,...,f_n}$ s.t. $\forall i$, $f_i(u_j) = \delta_{ij}$. Note any $f \in V^*$ is defined by $f(u_j)$ for $j = 1,...,n$. Thus each $f_i$ is uniquely defined by this definition. So matrices of $f_1,...,f_n$ in basis $\Set{u_1,...,u_n}$ are:
 \bee
 (1,0,.&..,0)\\
 &\vdots\\
 (0,...,&0,1)\\
 \eee
 \end{Def}
 


\begin{rem}
$V^{**} \cong V$ by $u(f) = f(u)$ (this is how we are defining the action of $u$) and thus the dual basis of $\Set{f_1,...,f_n}$ is $\Set{u_1,...,u_n}$. This is because $u_j(f_i) = \delta_{ij},\forall i,j$. 
\end{rem}

Consider $\phi:V \to W$ a linear transformation. You have basis $\Set{u_1,...,u_m}$ in $V$ and $\Set{v_1,...,v_n}$ in $W$. Let $A$ be the matrix of $\phi$ in these bases. We have the dual homomorphism $\phi^*:W^* \to V^*$ defined by $\phi^*(g) = g\circ \phi$:

\begin{center}
\begin{tikzcd}
V \arrow[r, "\phi"] \arrow[rd, "\phi^*(g) = g \circ \phi"'] & W \arrow[d, "g"] \\
 & F
\end{tikzcd}.
\end{center}
And we have dual bases $\Set{f_1,...,f_m}$ in $V^*$ and $\Set{g_1,...,g_n}$ in $W^*$. So what is $A^*$, the matrix of $\phi^*$ in these bases? Note $A = (a_{ij})$, where $a_{ij} = g_i(\phi(u_j))$. So $g_i(u)$ is actually the $i$-th coordinate of $u$., since $\phi$ maps to $W$ and $g_i$ maps from $W$ to $F$. Now $A^* = (b_{ji})$, then $b_{ij} = u_j(\phi^*(g_i)) = u_j(g_i \circ \phi) = (g_i \circ \phi)(u_j) = a_{ij}$ (we use the definition of the action of $u$). 

\begin{rem}
So $A^* = A^T$. 
\end{rem}

\begin{lem}
From this it immediately follows that $(BA)^T = A^TB^T$. 
\end{lem}
\begin{proof}
Observe:
\begin{center}
\begin{tikzcd}
V \arrow[r, "A"] \arrow[rr, "BA", bend left] & W \arrow[r, "B"] & U \\
V^* & W^* \arrow[l, "A^T"'] & U^* \arrow[l, "B^T"'] \arrow[ll, "(BA)^T = A^TB^T", bend left]
\end{tikzcd}.
\end{center}
\end{proof}

\begin{rem}
Row rank$(A)$ = column rank$(A^*)$ = $rank\phi^* = rank\phi$. So yes, row rank of $A$ is column rank of $A$. But we didn't prove this last equality yet. 
\end{rem}

\begin{rem}
$*$ is a contravariant functor. 
\end{rem}


\section*{11.3 Exercises}

\begin{enumerate}[label=\arabic*.]

\item \textit{Let $V$ be a finite dimensional vector space. Prove that the map $\phi\mapsto \phi^*$ in Theorem 20 from Dummit and Foote gives a ring isomorphism of End$(V)$ with End$(V^*)$. }

\begin{proof}
This is an isomorphism of vector spaces, but not of rings. Note $\phi + \psi \mapsto \phi^* + \psi^*$ since:
\bee
(\phi^* + \psi^*)(g) &= g \circ(\phi + \psi)\\
&= g \circ \phi + g \circ \psi\\
&= \phi^*(g) + \psi^*(g).
\eee
Isomorphism since $\phi^* \mapsto \phi^{**} = \phi$ is the inverse. Why is it not an isomorphism of rings? Note $(\phi\psi)^* = \psi^*\phi^*$, not $\phi^*\psi^*$. So the multiplication is not preserved.

Note $\phi^{**}(u) = u \circ \phi^*$. So:
$$
(u \circ \phi^*)(g) = u(\phi^*(g)) = u(g\circ \phi) = g(\phi(u)) = \phi(u)(g).
$$
Thus $u \circ \phi^* = \phi(u)$. And thus $\phi^{**}(u) = \phi(u)$.  
\end{proof}

\item \textit{Let $V$ be the collection of polynomials with coefficients in $\Q$ in the variable of $x$ of degree at most $5$ with $1,x,x^2,...,x^5$ as basis. Prove that the following are elements of the dual space of $V$ and express them as linear combinations of the dual basis: }

\begin{enumerate}
\item \textit{$E:V \to \Q$ defined by $E(p(x)) = p(3)$, i.e. evaluation at $x = 3$. }


Note $E \in V^*$ since it acts on polynomials, the action is linear, this is a homomorphism from $V$ to $\Q$ and $V^* = Hom(V,\Q)$. Coordinates of $E$ is the dual basis of $\Set{1,x,x^2,...,x^5}$. Dual basis of $\Set{1,x,x^2,...,x^5}$ is $\Set{f_0,f_1,...,f_5}$ where $f_i(a_0 + a_1x + \cdots + a_5x^5) = a_1$. The coordinates of linear form in the dual basis are (to read the $i$-th coordinate, you apply the $i$-th element of the basis to this form) $u_i(E) = E(u_i)$. Recall that $u_i$ is the $i$-th element in $\Set{1,x,x^2,...,x^5} = \Set{u_0,u_1,...,u_5}$. So $E(\Set{1,x,x^2,...,x^5}) = (1,3,3^2,...,3^5)$. And:
$$
E = f_0 + 3f_1 + 9f_2 + \cdots + 243f_5.
$$
And we can check that this is correct by applying the definition of $f_i$. 

\item \textit{$\phi:V \to \Q$ defined by $\phi(p(x)) = \int_0^1p(t)dt$. Where $\phi \in V^*$. }

Note $\phi \leftrightarrow (1,\fracc{1}{2},\fracc{1}{3},...,\fracc{1}{6})$. 
\end{enumerate}

\setcounter{enumi}{2}

\item \textit{Let $S$ be any subset of $V^*$ for some finite dimensional space $V$. Define $Ann(S) = \Set{v \in V:f(v) = 0,\forall f \in S}$. ($Ann(S)$ is called the annihilator of $S$ in $V$. }

\begin{enumerate}
\item \textit{Prove that $Ann(S)$ is a subspace of $V$. }

\begin{proof}
Recall Definition \ref{def11.5}. Let $v,w \in Ann(S)$. Then $f(v) = f(w) = 0$ $\forall f \in S \sub Hom(V,F)$, where $V$ is a vector space over the field $F$. Then $f(v + w) = f(v) + f(w) = 0 + 0 = 0$ since $f$ is a homomorphism. So $v + w \in Ann(S)$. Now let $r \in F$. Then $f(rv) = rf(v) = r\cdot 0 = 0$ since again $f$ is a homomorphism. So $rv \in Ann(S)$. Thus $Ann(S)$ is a subspace by definition. 
\end{proof}



\item \textit{Let $W_1$ and $W_2$ be subspaces of $V^*$. Prove that $Ann(W_1 + W_2) = Ann(W_1) \cap Ann(W_2)$ and $Ann(W_1 \cap W_2) = Ann(W_1) + Ann(W_2)$. }

\begin{proof}
Recall: 
$$
Ann(W_1 + W_2) = \Set{v \in V:(f + g)(v)  =0,\forall f + g \in W_1 + W_2}.
$$
So let $v \in Ann(W_1 + W_2)$. Then with $g = 0$, we have $(f + g)(v) = f(v) = 0$, for all $f \in Ann(W_1)$. Now let $f = 0$, by same argument, $g(v) = 0$ for all $g \in W_2$, so $v \in Ann(W_1)$, so $v \in Ann(W_1) \cap Ann(W_2)$. Now let $v \in Ann(W_1) \cap Ann(W_2)$. Then $f(v) = 0$ and $g(v) = 0$ for all $f \in W_1,g \in W_2$. Then for arbitrary $f + g \in W_1 + W_2$. We have $(f + g)(v) = f(v) + g(v) = 0 + 0 = 0$. So $v \in Ann(W_1 + W_2$. So we have proved both inclusions: $Ann(W_1 + W_2) = Ann(W_1) \cap Ann(W_2)$. 

Now we prove the second equality: recall:
$$
Ann(W_1 \cap W_2) = \Set{v \in V: f(v) = 0,\forall f \in W_1\cap W_2}.
$$
Let $u \in Ann(W_1)$ and $v \in Ann(W_2)$. Then for any $f \in W_1 \cap W_2$, $f(u) = 0$ and $f(v) = 0$, so $f(u + v) = f(u) + f(v) = 0 + 0 = 0$, since $f$ is a homomorphism. So $u + v \in Ann(W_1 \cap W_2)$, so $Ann(W_1) + Ann(W_2) \sub Ann(W_1 \cap W_2)$. 

Now we apply the result of part (c). We want to show $Ann(W_1 \cap W_2) \sub Ann(W_1) + Ann(W_2)$. By this result we know this is equivalent to showing:
$$
Ann(Ann(W_1 \cap W_2)) = W_1 \cap W_2 \sub Ann(Ann(W_1) + Ann(W_2)).
$$
So let $B_V$ be a basis for $V$, and let $B_{V^*}$ be a basis for $V^*$. Then let $\Set{f_1,...,k}$ be a basis for $W_1$ and define $\Set{f_l,...,f_m}$ as basis for $W_2$, without loss of generality, where $m,k \leq n = \dim V = \dim V^*$. Then by part (f) we know $Ann(W_1) = F(B_V \setminus \Set{f_1,...,f_k})$ and $Ann(W_2) = F(B_V \setminus \Set{f_l,...,f_m})$. So $Ann(W_1) + Ann(W_2) = A =  F(B_V \setminus (\Set{f_l,...,f_m}\cap \Set{f_1,...,f_k}))$. And by part (f) again we know $Ann(A) = F(B_{V^*} \setminus (B_{V^*} \setminus F(\Set{f_l,...,f_m}\cap \Set{f_1,...,f_k}))) = W_1 \cap W_2$. So we have proved the other inclusion, and we are done.  
\end{proof}
\bb
\item \textit{Let $W_1$ and $W_2$ be subspaces of $V^*$. Prove that $W_1 = W_2$ if and only if $Ann(W_1) = Ann(W_2)$. }
\bb

\begin{proof}
Let $\Set{g_1,...,g_n}$ be a basis of $V^{**}$. And we have the natural isomorphism which sends $g_i \mapsto v_i \in B_V$, the basis of $V$. So $V \cong V^{**}$. So $V^*$ must have a basis $\Set{f_1,...,f_n}$ and let $\Set{f_1,...,f_k}$ be a basis for $W_1$. By part (f), we know 
$$Ann(Ann(W_1)) = Ann(F\Set{v_{k + 1},...,v_n}).
$$
  But again by part $F$ and since $v_i(f_j) = f_j(v_i) = 0,\forall i\neq j$, we know $ Ann(F\Set{v_{k + 1},...,v_n}) = F\Set{f_1,...,f_k}$. But this is exactly $W_1$, so $Ann(Ann(W)) = W$, and so since $Ann(W_1) = Ann(W_2)$, we know $Ann(Ann(W_1)) = Ann(Ann(W_2)) \Rightarrow W_1 = W_2$. 
\end{proof}

\bb

\item \textit{Prove that the annihilator of $S$ is the same as the annihilator of the subspace of $V^*$ spanned by $S$. }

\bb

\begin{proof}
Note $Ann(S) = \Set{v \in V: f(v) = 0, \forall f \in S}$. And note that 
$$
Ann(FS) = \Set{v \in F:f(v) = 0,\forall f \in FS}.
$$
 Now $V^*$ is finite dimensional since we know how to generate the dual basis, and the dimension of $V^*$ is the same as the dimension of $V$. So $S$ has a finite maximal linearly independent set $B_S = \Set{f_1,...,f_k}$. Let $v \in Ann(FS)$. Then since $1 \in F$, we know $S \sub FS$, so $f(v) = 0,\forall f \in S$, so $Ann(FS) \sub Ann(S)$. 
 
 
 
 
 Now let $v \in Ann(S)$. Then since $B_S \sub S$, we know $v \in Ann(B_S)$. Then 
 $$
 FS \sub FB_S = F\Set{f_1,...,f_k} = \Set{r_1f_1 + \cdots + r_kf_k:r_i \in F, f_i \in B_S}.
 $$
  Then $f_i(v) = 0$ for all $i$ since they are in $B_S$, and $r_i \cdot 0 = 0$, so $v \in Ann(FB_S) \sub Ann(FS)$ since $FS \sub FB_S$. Hence $Ann(S) \sub Ann(FS)$, and so they are equal. 
\end{proof}

\bb

\item \textit{Assume $V$ is finite dimensional with basis $v_1,...,v_n$. Prove that if $S = \Set{v_1^*,...,v_k^*}$ for some $k \neq n$, then $Ann(S)$ is the subspace spanned by $\Set{v_{k + 1},...,v_n}$. }

\bb

\begin{proof}
Note that $S$ is some subset of the dual basis, so let's change notation to be consistent with lecture. Let $S = \Set{v_1^*,...,v_k^*} = \Set{f_1,...,f_k}$. Note since $k \neq n$, $\Set{v_{k + 1},...,v_n}$ is nonempty. Let $v = r_1v_1 + \cdots + r_nv_n \in Ann(S)$. Then $f_i(v) = 0$, $1 \leq i \leq k$. We want to show $v \in F\Set{v_{k + 1},...,v_n}$. Suppose $v \notin  F\Set{v_{k + 1},...,v_n}$, then since $v \in V$, we know there exists $i \leq k$ s.t. the coefficient of $v_i$ in $r_1v_1 + \cdots + r_nv_n$ is nonzero. But if this is true, we would have $f_i(r_1v_1 + \cdots + r_nv_n) \neq 0$ since each of the basis vectors is linearly independent. This is a contradiction, since $f_i(v) = 0$ for all $v \in Ann(S)$. So we must have that $v \in  F\Set{v_{k + 1},...,v_n}$. And hence $Ann(S) \sub F\Set{v_{k + 1},...,v_n}$. 

Now let $v \in F\Set{v_{k + 1},...,v_n}$. Then $v = r_{k + 1}v_{k + 1} + \cdots + r_nv_n$. Chose arbitrary $f_i \in S$. Then $i \leq k$, so $f(r_jv_j) = r_jf(v_j) = f_j\cdot 0 = 0$ for all $j > k$, by definition of $f_i$, since $i \neq j$. Thus $f_j(v) = 0$ since $j > k$ for all $v_j \in \Set{v_{k + 1},...,v_n}$. So since this holds for all $f_j \in S$, $v \in Ann(S)$, so $F\Set{v_{k + 1},...,v_n} \sub Ann(S)$. 
\end{proof}

\bb

\item \textit{Assume $V$ is finite dimensional. Prove that if $W^*$ is any subspace of $V^*$, then $\dim Ann(W^*) = \dim V - \dim W^*$. }

\bb

\begin{proof}
We have a basis of $\Set{v_1,...,v_n}$ of $V$.  Let $\Set{f_1,...,f_n}$ be the corresponding basis of the finite dimensional $V^*$ (since $V$ is finite dimensional), and without loss of generality, let $\Set{f_1,...,f_k}$ be a basis for $W^*$, which we know has a basis since it is a subspace. By the previous exercise, $Ann(W^*) = F\Set{v_{k + 1},...,v_n}$. So it has dimension $n - k$, and since $\dim V = n$ and $\dim W^* = k$, we are done. 
\end{proof}

\bb

Note that parts (a),(d),(b)(i,ii,iii) are all easy. 

Below is only true for finite dimensional vector spaces. 
Recommend: (e)$\Rightarrow$(f)$\Rightarrow$ $Ann(Ann(W)) = W \Rightarrow$ (c) $\Rightarrow$ (b)(iv). 

\end{enumerate}
\end{enumerate}

\section{Determinants}

\begin{rem}
$\det AB = \det A \det B$. 
\end{rem}

\begin{rem}
$\det A = 0$ if and only if columns of $A$ are linearly dependent. 
\end{rem}

\begin{rem}
$\det(PAP^{-1}) = \det A$. 
\end{rem}

\begin{Def}
$$
\det A = \left|\begin{matrix}
a_{11} & \cdots & a_{1n}\\
\vdots & & \vdots\\
a_{n1} & \cdots & a_{nn}
\end{matrix} \right| = \sum_{\sigma \in S_n} sign\sigma \prod_{i = 1}^n a_{i,\sigma(i)}.
$$
\end{Def}

\begin{rem}
Observe:
\begin{center}
\begin{tikzcd}
\phi:R^n \arrow[r] & R^n \\
\Lambda^n\phi:\Lambda^n(R^n) \arrow[d, "\cong"] \arrow[r] & \Lambda^n(R^n) \arrow[d, "\cong"] \\
R \arrow[r, "\cdot c = \det"] & R
\end{tikzcd}.
\end{center}
\end{rem}

\begin{rem}
$\det A = \det A^T$. 
\end{rem}

\begin{rem}
$\det A$ is an alternating $n$-linear function of columns of $A$ such that $\det I = 1$. This determines the determinant uniquely since $\Lambda\Tau^n(R^n) \cong R$. This space is one dimensional, so up to scaling, it is unique, and we normalize, so its completely unique. Note $n$-linear functions here are tensors, and for it to be alternating it must be in $\Lambda\Tau^n(R^n)$. And we have a canonical isomorphism:
$$
\Lambda\Tau^n(R^n) \cong \Lambda^n(R^n).
$$
\end{rem}

\section*{11.4 Exercises}

\begin{enumerate}[label=\arabic*.]
\setcounter{enumi}{1}
\item \textit{Let $F$ be a field and let $A_1,A_2,...,A_n$ be (column) vectors in $F^n$. Form the matrix $A$ whose $i$-th column is $A_i$. Prove that these vectors form a basis of $F^n$ if and only if $\det A \neq 0$. }

\begin{proof}
Recall Corollary 27 from Dummit and Foote, which states that if $R$ is an integral domain, then $\det A \neq 0$ for $A \in M_n(R)$ if and only if the columns of $A$ are $R$-linearly independent as elements of the free $R$-module of rank $n$. 


Now since $F^n$ is a vector space, we know that if we have a set of $n$ linearly independent vectors, it must be a basis. So let the column vectors $A_i$ form a basis of $F^n$. Then they must be linearly independent. So by the corollary, we know $\det A \neq 0$. Now let $\det A \neq 0$. Then by the corollary, we know $A_i$ are linearly independent over $F$ as elements of $F^n$, since $F$ is field, thus an integral domain. So then since $F^n$ is a vector space of $\dim F^n = n$, they must form a basis, since if they didn't, we would need some other linearly independent vector to generate the missing elements of $F^n$, which would contradict the fact that $\dim F^n = n$. 
\end{proof}

\item \textit{Let $R$ be any commutative ring with $1$, let $V$ be an $R$-module and let $x_1,...,x_n \in V$. Assume that for some $A \in M_{n \times n}(R)$,
$$
A\lpar 
\begin{matrix}
x_1\\
\vdots\\
x_n
\end{matrix} \rpar  = 0.
$$
Prove that $(\det A)x_i = 0$, for all $i \in \Set{1,2,...,n}$. }

\begin{proof}
Recall Theorem 30 from Dummit and Foote, which states that if $B$ is the transpose of the matrix of cofactors of $A$, then $AB = BA = (\det A) I$. So note:
$$
0 = B0 = BA\lpar 
\begin{matrix}
x_1\\
\vdots\\
x_n
\end{matrix} \rpar = (\det A)I\lpar 
\begin{matrix}
x_1\\
\vdots\\
x_n
\end{matrix} \rpar = (\det A)\lpar 
\begin{matrix}
x_1\\
\vdots\\
x_n
\end{matrix} \rpar.
$$
And this is zero if and only if $(\det A)x_i = 0$ for all $i$. 
\end{proof}



\end{enumerate}


\section{Tensor algebras, symmetric and exterior algebras}

\textbf{Thursday, February 15th}
\bb
\begin{Def}
Let $M$ be an $R$-module. Let $\mc{T}^k(M) = M \tens \cdots \tens M$ where we have $k$ copies of $M$. For the infinite case we have:
$$
 \mathcal{T}(M) = \bigoplus_{k = 0}^\infty \mathcal{T}^k(M)
 $$
and this $\mathcal{T}^k(M)$ is called the \textbf{tensor algebra of $M$}. 

We define $\mathcal{T}^0(M) = R$, and $\mathcal{T}^1 = M$. And we have:
$$
(u_1 \tens \cdots \tens u_k)\cdot (v_1 \tens \cdots \tens v_l) = u_1 \tens \cdots \tens u_k \tens v_1 \tens \cdots \tens v_l.
$$
\end{Def}


\begin{Def}
Now consider $\mc{S}(M) = \mathcal{T}(M)/C(M)$, where $C(M)$ is the ideal generated by tensors $u \tens v - v \tens u$ for $u,v \in M$. This object $\mc{S}(M)$ is called the \textbf{symmetric tensor algebra of $M$. } And $\mc{S}(M) = \oplus_{k =0}^\infty \mc{S}^k(M)$. 
\end{Def}

Note elements of the form $w \tens u \tens v - w \tens v \tens u \in C(M)$. 

So we have: 

\begin{Def}
\textbf{Graded ring:}
$$
R = R_0 \oplus R_1 \oplus R_2 \oplus \cdots.
$$
where these are submodules, not subrings. Note for $u \in R_i,v \in R_j$, then $uv \in R_{i + j}$. 

Note:
$$
u_1 \tens u_2 \tens u_3 = u_2 \tens u_1 \tens u_3 = u_2 \tens u_3 \tens u_1 = u_3 \tens u_2 \tens u_1 \in \mc{S}(M).
$$


\end{Def}

\begin{Def}
An ideal is a \textbf{graded ideal} if $I\oplus (I \cap R_k)$. 
\end{Def}

So $C(M) = \oplus (\mc{T}^k/C^k)$. 

\begin{rem}
We don't really need any of this stuff above, and possibly some of the stuff below as well. 
\end{rem}

\begin{Def}
An \textbf{exterior algebra} is:
$$
\Lambda(M) = \mc{T}(M)/\mc{A}(M),
$$
where $\mc{A}(M)$ is the ideal in $\mc{T}(M)$ generated by tensors of the form $u \tens u$ for $u \in M$. And:
$$
\Lambda(M) = \bigoplus_{k = 0}^\infty \Lambda^k(M).
$$
And $\tens$ in $\Lambda(M)$ is denoted by $\wedge$. So we have $u \wedge u = 0$ in $\Lambda(M)$ for all $u \in M$. 
\end{Def}

Now we have $\forall u,v \in M$, $u \wedge v = -v \wedge u$. And:
$$
0 = (u + v) \wedge (u + v) = u \wedge v + \cdots.
$$
$$
u \wedge(v \wedge w) = -v \wedge u \wedge w = (v \wedge w) \wedge u.
$$
And:
$$
\alpha \wedge \beta = (-1)^{kl} \beta \wedge \alpha
$$
 if $\alpha$ has order $k$ and $\beta$ has order $l$. 
 
 If $M$ is free of rank $n$, then $\forall k$ $\mc{T}^k(M)$ is free of rank $n^k$. If $\Set{u_1,...,u_n}$ is a basis in $M$, then:
 $$
 \Set{u_{i_1} \tens \cdots \tens u_{i_k}:i_1,...,i_k \in \Set{1,...,n}}
 $$
  is a basis in $\mc{T}^k(M)$. 
 
 \begin{rem}
 Basis in $\mc{S}^k(M)$ is:
 $$
 \Set{u_{i_1} \tens u_{i_2} \tens \cdots \tens u_{i_k}:i_1 \leq i_2 \leq ... \leq i_k}. 
 $$
 And the rank is $\lpar \begin{matrix}
 n + k - 1\\
 k - 1
\end{matrix}  \rpar$ (choose). 
 \end{rem}
 
 \begin{rem}
 The basis in $\Lambda^k(M)$ is:
 
 $$
 \Set{u_{i_1} \wedge u_{i_2} \wedge \cdots \wedge u_{i_k}: i_1 < i_2 < ... < i_k}.
 $$
 \end{rem}
 
 \begin{Ex}
 If $k = 3, n = 4$, then:
 $$
 u_1 \wedge u_2 \wedge u_3, u_1 \wedge u_2 \wedge u_4, u_1 \wedge u_3 \wedge u_4, u_2 \wedge u_3 \wedge u_4
 $$
 are the elements of $\Lambda^k(M)$. And the rank is $\binom{n}{k}$. If $k > n$ then $\Lambda^k(M) = 0$. 
 \end{Ex}
 
 \begin{Ex}
 If $k = n$, $rank = 1$ so $\Lambda^n(M) \cong R$. The only basis tensor in $\Lambda^n(M)$ is $u_1 \wedge u_2 \wedge ... \wedge u_n$. 
 Now $\forall v_1,...,v_n \in M$, we have:
 $$
 v_1 \wedge v_2 \wedge \cdots \wedge v_n = cu_1 \wedge u_2 \wedge \cdots \wedge u_n, c\in R.
 $$
 \end{Ex}
 
 \begin{Ex}
 Let $n = 2$, and $\Set{u_1,u_2}$. $v_1 = au_1 + bu_2$. And $v_2 = cu_1 + du_2$. Then:
 \bee
 v_1 \wedge v_2 &= (au_1 + bu_2) \wedge (cu_1 + du_2)\\
 &= acu_1 \wedge u_1 + adu_1 \wedge u_2 + bcu_2 \wedge u_1 + bdu_2 \wedge u_2\\
 &= (ad - bc)u_1 \wedge u_2.
 \eee
 \end{Ex}
 
 \begin{lem}
Let $M$ be a vector space.  If $\dim(M) = n$ then $v_1,...,v_n \in M$ are linearly independent if and only if $v_1 \wedge ... \wedge v_n \neq 0$. 
 \end{lem}
 
 \begin{proof}
 Assume $M$ is a vector space. If $v_1,...,v_n$ are linearly independent, they form a basis for $M$, so $v_1 \wedge ...\wedge v_n$ is a basis (so nonzero) element of $\Lambda^n(M)$. If they are linearly independent, they are contained in a subspace $W$ of dimension $n - 1$. So $v_1 \wedge ... \wedge v_n \in \Lambda^n(W) = 0$, since $n > \dim(W)$. 
 \end{proof}
 
\begin{Def}

$
\mc{S}^k(M^*)
$ - \textbf{Symmetric $k$-linear forms on $M$}. 

\bb
$
\Lambda^k(M^*)
$ - \textbf{alternating $k$-linear forms on $M$}. 
\end{Def}

So we have $\Phi:M^k \to R$ by $(v_1,...,v_k) \mapsto a$ such that:
\bee
\Phi(v_{\sigma(1)},...,v_{\sigma{k}}) &= \Phi(v_1,...,v_k)\\
\eee
for symmetric, and the following for alternating:
\bee
\Phi(v_{\sigma(1)},...,v_{\sigma{k}}) &= sign(\sigma)\Phi(v_1,...,v_k)\\
\eee
with $\sigma \in S_k$ a symmetric group. 
And we have something. 
\bb
\bb

\textbf{Friday, February 16th}

\begin{Def}
A tensor $w \in \mc{T}^k(M)$ is \textbf{symmetric} if it is invariant under permutation of components: 
$$
\sigma(w) = w, \forall \sigma \in S_k.
$$
Where $S_k$ acts on $\mathcal{T}^k(M)$ by:
$$
u_1 \tens ...\tens u_k \mapsto u_{\sigma(1)} \tens ... \tens u_{\sigma(k)}
$$
for $\sigma \in S_k$. 
\end{Def}

\begin{Ex}
In $\mc{T}^2$, $u \tens v + v \tens u$ is symmetric. 
\bb

In $\mc{T}^3$, $u \tens v \tens v + v \tens u \tens v + v \tens v \tens u$ is symmetric. 
\end{Ex}

\begin{rem}
Symmetric tensors form a submodule of $\mc{T}^k(M)$. But not a subalgebra. 
\end{rem}


\begin{Def}
There is a natural mapping called \textbf{symmetrization: }$\forall w \in \mc{T}^k(M)$, let:
$$
\tt{Sym}(w) = \fracc{1}{k!}\sum_{\sigma \in S_k} \sigma(w).
$$
Assume that $k!$ is a unit in $R$, it is the product of all its divisors. 

\end{Def}

\begin{rem}
$\tt{Sym}:\mc{T}^k(M) \to \Set{\text{Symmetric tensors in } T^k(M)} = \mc{ST}^k(M)$. 
\end{rem}

\begin{lem}
$\tt{Sym}$ defines an isomorphism $\mc{S}^k(M) \to \mc{ST}^k(M)$. Where $\mc{S}^k(M) = \mc{T}^k(M)/\mc{C}^k(M)$. 
\end{lem}

\begin{proof}
\text{Sym} is surjective since $\forall w \in \mc{ST}^k(M)$, we have:
$$
\tt{Sym}(w) = w.
$$
Observe:
$$
\mc{C}^k(M) = \Set{\sum w-1 \tens (u \tens v - v \tens u) \tens w_2: u,v \in M;w_1,w_2 \in \mc{T}(M)}.
$$
And $\tt{Sym}(\mc{C}^k(M)) = 0$, so $\mc{C}^k(M) \sub \ker(\tt{Sym})$. 
\end{proof}


\begin{rem}
$\forall w \in \mc{T}^k$:
\bee
w - \tt{Sym}(w) = \fracc{1}{k!}\sum_{\sigma \in S_k}(w - \sigma(w)).
\eee
Where $w - \sigma(w) \in \mc{C}^k(M)$. And if $w \in \ker(\tt{Sym})$, then $w = \fracc{1}{k!}\sum(w - \sigma(w)) \in \mc{C}^k(M)$. So $\ker(\tt{Sym}) = \mc{C}^k(M)$, so $\mc{S}^k(M) \cong \mc{ST}^k(M)$. 


Now why $w - \sigma(w)$? If $\sigma = \tau_1\tau_2 \cdots \tau_r$, transpositions, then 
$$
w - \sigma(w) = (w - \tau_r(w)) + (\tau_r(w) - \tau_{r - 1}\tau_r(w)) + \cdots + (\tt{    } - \sigma(w)).
$$
\end{rem}

Now we can do the same for alternating tensors and exterior algebra. Let $\sigma(w) = (\tt{sign}\sigma)w, \forall \sigma \in S_k$. 

\begin{rem}
They form a submodule of $\mc{T}^k(M)$. 
\end{rem}

\begin{rem}
We have a homomorphism $\tt{Alt}: \mc{T}^k(M) \to \Lambda\mc{T}^k$ by 
$$
\tt{Alt}(w) = \fracc{1}{k!}\sum_{\sigma \in S_k}\tt{sign}(\sigma)\sigma(w).
$$
And we call this \textbf{alternation}, or \textbf{sqew-symmetrization}. It induces an isomorphism $\Lambda\mc{T}^k(M) \to \Lambda^k(M)$. 
\end{rem}

Observe: 
\bee
u_1 \wedge u_2 &\leftrightarrow \fracc{1}{2} \lpar u_1 \tens u_2 - u_2 \tens u_1\rpar\\
u_1 \wedge u_2 \wedge u_3 &\leftrightarrow \fracc{1}{6}\bigg( u_1 \tens u_2 \tens u_3 - u_2 \tens u_1 \tens u_3 - u_1 \tens u_3 \tens u_2\\
&- u_3 \tens u_2 \tens u_2 + u_2 \tens u_3 \tens u_1 + u_3 \tens u_1 \tens u_2\bigg).
\eee

\begin{rem}
I have no idea what's going on here, where are these normalization constants coming from?
\end{rem}

\bb\bb

Let $M$ be free of rank $n$. Then $M^*$ is also free of rank $n$. So $\Lambda^n(M^*) \cong R$, so $\Lambda \Tau^n(M^*) \cong R$. 

\begin{rem}
$\forall k, \Lambda\tau^k(M^*)$ is the \textbf{space of alternating $k$-linear mappings $M^k \to R$. } Note this space is NOT $\Lambda^k(M^*)$. 
\end{rem}

Now define $\Phi:M^k \to R$ s.t. $\forall \sigma$:
$$
\Phi(u_{\sigma(1)},...,u_{\sigma(k)}) = \tt{sign}\Phi(u_1,...,u_k).
$$
And $k$-linear. 

\bb

Assuming that $k!$ is an unit in $R$:
$$
\Lambda\Tau^k(M) \cong \Lambda^k(M).
$$
And $\tt{Alt}:\Tau^k \to \Lambda\Tau^k$. 

\bb\bb
Now let $\phi: M \to M$ be a homomorphism. Then $\forall k$, 
$$
\phi^{\tens k}: \Tau^k(M) \to \Tau^k(M), \Lambda^k(M) \to \Lambda^k(M).
$$

\begin{rem}
For $k = n$:
\begin{center}
\begin{tikzcd}
\Lambda^k(M) \arrow[rr] \arrow[rd, "\cong", no head] &  & \Lambda^k(M) \\
 & R \arrow[ru, "\cong", no head] & 
\end{tikzcd}.
\end{center}


Where $w \mapsto cw$. Now $c$ is a constant in $R$ (according to Eric...)
\end{rem}

\begin{Def}
Take a basis $\Set{u_1,...,u_n}$ in $M$:
$$
u_1 \wedge ... \wedge u_n \mapsto \phi(u_1) \wedge ... \wedge \phi(u_n) = cu_1 \wedge ... \wedge u_n.
$$
Define $c =\det\phi$. 
\end{Def}

\begin{Cor}
The determinant doesn't depend on the choice of basis. 
\end{Cor}

\begin{Cor}
$\det(\phi\psi) = \det\phi \cdot \det \psi$. 
\end{Cor}

\begin{rem}
The determinant is an alternating $n$-linear mapping of 
$$
(\phi(u_1),...,\phi(u_n)),
$$ 
that is, columns of the matrix of $\phi$. So there exists only one such mapping since $\Lambda^n(M^*) \cong R$. 


\bb\bb

Note $\Phi:\lpar ||| \rpar \to R$ s.t. $\Phi(Id) = 1 \Rightarrow \Phi = \det$. 
\end{rem}

\bb\bb

\textbf{Monday, February 19th}

Let $M$ be free, $B = \Set{u_1,...,u_n}$ be a basis in $M$. Then:
$$
B^{ \tens k} = \Set{u_{i_1}\tens \cdots \tens u_{i_k}:i_1,...,i_k \in \Set{1,...,n}}
$$
 is a basis in $\mc{T}^k(M)$. 

\begin{lem}
Basis in $\mc{S}^k(M)$ is:
 $$
 \mc{S}^k(B) = \Set{u_{i_1} \tens u_{i_2} \tens \cdots \tens u_{i_k}:i_1 \leq i_2 \leq ... \leq i_k}. 
 $$
\end{lem}

\begin{proof}
Note that $\mc{S}^k(M)$ has a universal property: if $\Phi:M^k \to N$ is a $k$-linear symmetric mapping. Recall that symmetric means that $\Phi(u_{\sigma(1)},...,u_{\sigma(k)}) = \Phi(u_1,...,u_k) \forall \sigma \in S_k$. Then  there is a hom-sm $\beta:\mc{S}^k(M) \to N$ such that: 
 $$
 \beta(v_1 \tens \cdots \tens v_k) = \Phi(v_1,...,v_k), \forall v_i \in M.
 $$
 Now the word symmetric guarantees this homomorphism. Now we have $\alpha:\mc{T}^k(M) \to N$ s.t. $\alpha|_{\mc{C}^k(M)} = 0$. Recall that 
 $$
 \mc{C}^k(M) = \lpar v_1 \tens \cdots \tens v_i \tens v_{i + 1} \tens \cdots \tens v_k - v_1 \tens \cdots \tens v_{i + 1} \tens v_i \tens \cdots \tens v_k \rpar. 
 $$
So $\mc{S}^k(B)$ generates $\mc{S}^k(M)$. Note for $n = 2$, $k = 2$, we have $\{u_1 \tens u_1,u_1 \tens u_2,u_2 \tens u_1,u_2 \tens u_2\} = B^{\tens 2}$, and we have:
$$
\mc{S}^2(B) = \Set{u_1 \tens u_1,u_1 \tens u_2,u_2 \tens u_2}.
$$
So now we have to prove linear independence of this set. So we want to find a $2$-linear mapping which maps... something to something. First we have to choose  some element which we want to use. Let $i_1 \leq i_2 \leq ... \leq i_k$. Define a $k$-linear mapping from $M^k \to R$ by sending: 
$$
\Phi(u_{j_1},...,u_{j_k}) = \begin{cases}
1 & \text{ if }(j_1,...,j_k) = \sigma(i_1,...,i_k) \text{ for some }\sigma \in S_k\\
0 & \text{otherwise}
\end{cases}.
$$
So we have basis vectors $u_{j_1}\tens \cdots \tens u_{j_k}$ and we want to send them to 1 only if they are some permutation of our $i$'s. Then $\Phi$ induces a homomorphism $\beta:\mc{S}^k(M) \to R$ such that:
$$
\beta(u_{i_1} \tens \cdots \tens u_{i_k}) = 1.
$$
And $\beta(u_{j_1}  \tens \cdots \tens u_{j_k}) = 0$ $\forall j_1 \leq ...\leq j_k$ if $\neq (i_1,...,i_k)$. So we only define $\Phi$ on basis vectors and expand it to the whole space by $k$-linearity. Now we have a hom-sm which maps our chosen vector to 1 and all other vectors to zero. This implies that $u_{i_1}  \tens \cdots \tens u_{i_k}$ is not a linear combination of other vectors from $\mc{S}^k(B)$. If such a hom-sm exists it means that our chosen element is a nonzero linear combination. So what we proved is that any element from $\mc{S}^k(B)$ is not a linear combination of the others, so we proved this set is linearly independent and they are a basis consequently. This relies on the universal property (the induction of the hom-sm). 
\end{proof}

So the for the homework, make $\Phi$ alternating, and add $sign \sigma$ in front of the 1 in the definition of piecewise $\Phi$. 

\begin{lem}
 The basis in $\Lambda^k(M)$ is:
 
 $$
 \Set{u_{i_1} \wedge u_{i_2} \wedge \cdots \wedge u_{i_k}: i_1 < i_2 < ... < i_k}.
 $$
\end{lem}


\bb\bb

Now we discuss \textbf{alternatization} of tensors. Consider 
$$
Alt_2:w \mapsto \fracc{1}{k!}\sum_{\sigma \in S_k}sign(\sigma) \sigma(w),
$$ 
where $w \in \mc{T}^k(M)$. And we added the coefficient $\fracc{1}{k!}$. But it does not appear in the book. 

So we know $Alt|_{\mc{A}^k(M)} = 0$, so it factorizes to $\Lambda^k(M) \to \Lambda\mc{T}^k(M)$, which is the space of alternating tensors. And it turns out to be an isomorphism. Surjective because it sends alternating tensors into themselves, and the kernel is $\mc{A}^k(M)$. Recall that:
$$
\mc{A}^k(M) = \Tau^k(M) \cap \Set{\text{ ideal in }\Tau(M) \text{ generated by }u \tens u}.
$$
And:
$$
\Lambda^k(M) = \Tau^k(M)/\mc{A}^k(M).
$$
And $\sigma(w) = sigm(\sigma)w, \forall \sigma$. So the elements of $\Lambda\Tau^k(M)$ are just tensors with the property that if you change the order, it changes the sign. So $\mc{A}^k()$ is submodule of $\Tau^k(M)$ consisting of tensors having at least two equal components. So it is generated by elements of the form:
$$
u_1 \tens \cdots \tens u_{i - 1} \tens u_i \tens u_i \tens u_{i + 2} \tens \cdots \tens u_k.
$$

\begin{Ex}
For $k = 2$: $u_1 \wedge u_2 \mapsto \fracc{1}{2}(u_1 \tens u_2 - u_2 \tens u_1)$. 
\end{Ex}

So if $w \mapsto \sum sign(\sigma)\sigma(w)$, then for $k = 2$, $u_1 \tens u_2 - u_2 \tens u_1 \mapsto 2(u_1 \tens u_2 - u_2 \tens u_1)$. 

\begin{rem}
Still quite lost here. 
\end{rem}

We can consider this mapping without $\fracc{1}{k!}$, but it will not be an isomorphism. Let's try and think of any example. For symmetric tensors, if we don't have $sign(\sigma)$ and we just take it over $\z$, then the image will be $2\z$. Does this make sense? But for alternating tensors? I don't know. 

\begin{Def}
\textbf{Natural} means functorial. Whatever that means. So you can define it for each object, so that we have morphisms between objects and commutative diagrams and other nice stuff probably. Consider $V \cong V^*$ and $V \cong V^{**}$ but this first one is not natural. This is because we might need a different definition of the map for different specific vector space. But this second one is natural. We define it as $u \to \Phi_u$ by $\Phi_u(\phi) = \phi(u)$. It works for all objects in the category... I think. 
\end{Def}


\begin{theorem}
Everything is unnatural. 
\end{theorem}





\section*{11.5 Exercises} 

\begin{enumerate}[label=\arabic*.]
\setcounter{enumi}{4}
\item \textit{Prove that if $M$ is a free $R$-module of rank $n$, then $\Lambda^k(M)$ is a free $R$-module of rank $\binom{n}{k}$ for $k = 0,1,2,...$} 
Let $B = \Set{u_1,...,u_n}$ be a basis in $M$. Equivalently, we claim:
\begin{lem}
 The basis in $\Lambda^k(M)$ is:
 
 $$
 \Lambda^k(B) = \Set{u_{i_1} \wedge u_{i_2} \wedge \cdots \wedge u_{i_k}: i_1 < i_2 < ... < i_k}.
 $$
\end{lem}
Note this set has $\binom{n}{k}$ elements since we are choosing $k$ from $n$, since $|B| = n$. 
\begin{proof}
 Note that $\Lambda^k(M)$ has a universal property: If $\Phi:M^k \to N$ is a $k$-linear alternating mapping, then there is a hom-sm $\beta:\Lambda^k(M) \to N$ such that: 
 $$
 \beta(v_1 \wedge \cdots \wedge v_k) = \Phi(v_1,...,v_k), \forall v_i \in M.
 $$
 And since this basis is obtained from the natural projection of $B$, we know $\Lambda^k(B)$ generates $\Lambda^k(M)$.


Let $i_1 < i_2 < ... < i_k$. Define a $k$-linear mapping from $M^k \to R$ by sending: 
$$
\Phi(u_{j_1},...,u_{j_k}) = \begin{cases}
sign(\sigma) & \text{ if }(j_1,...,j_k) = \sigma(i_1,...,i_k) \text{ for some }\sigma \in S_k\\
0 & \text{otherwise}
\end{cases}
$$
So we have basis vectors $u_{j_1}\wedge \cdots \wedge u_{j_k}$ and we want to send them to $sign(\sigma)$ only if they are some permutation of our $i$'s. Then $\Phi$ induces a homomorphism $\beta:\Lambda^k(M) \to R$ such that:
$$
 \beta(v_1 \wedge \cdots \wedge v_k) = \Phi(v_1,...,v_k), \forall v_i \in M.
 $$
And $\beta(u_{j_1}  \wedge \cdots \wedge u_{j_k}) = 0$, $\forall j_1 < ...< j_k$ if $\neq (i_1,...,i_k)$. So we only define $\Phi$ on basis vectors and expand it to the whole space by $k$-linearity. Now we have a hom-sm which maps our chosen vector to $sign(\sigma)$ and all other vectors to zero. So suppose 
$$
s = r_1v_1 + \cdots + r( u_{i_1} \wedge u_{i_2} \wedge \cdots \wedge u_{i_k}) + \cdots + r_{\binom{n}{k}}v_{\binom{n}{k}} = 0.
$$
Then $\beta(s) = \beta(r( u_{i_1} \wedge u_{i_2} \wedge \cdots \wedge u_{i_k})) = 0$, since $\beta$ maps all other vectors to zero. So we must have $r \neq 0$ since our basis vector is nonzero. 
 This implies that $u_{i_1}  \wedge \cdots \wedge u_{i_k}$ is not a linear combination of other vectors from $\Lambda^k(B)$. So what we proved is that any element from $\Lambda^k(B)$ is not a linear combination of the others, so we proved this set is linearly independent and thus a basis. 
\end{proof}

\begin{comment}

\begin{proof}
Recall $T = \mc{T}^k(M) = M \tens \cdots \tens M$. Then we have a basis for $T$ given by: 
$$
B = \Set{m_{i_1} \tens \cdots \tens m_{i_n}: 1 \leq i_j \leq n},
$$
and $m_i = (0,...0,1,0,...,0) \in M$ where the $1$ is in the $i$-th position, the $i$-th standard basis vector of $M$. Recall: 
$$
L = \Lambda^k(M) = \mc{T}^k(M)/\mc{A}^k(M) = T/A.
$$
Now let $\pi:T \to L$ be the natural projection homomorphism given by:
$$\pi(v_1 \tens \cdots \tens v_n) = (v_1 \tens \cdots \tens v_n) \mod A.
$$
Now note that $B/A$ generates $L$. We conjecture that $B/A$ is linearly independent, and thus a basis for $L$. So we know that $B/A$ is just the set where all $i_{j_1} \neq i_{j_2}$ for all $j_1 \neq j_2$. 
 
\end{proof}
\end{comment}

\setcounter{enumi}{11}

\item 
\begin{enumerate}
\item \textit{Prove that if $f(x,y)$ is an alternating bilinear map on $V$ (i.e. $f(x,x) = 0$ for all $x \in V$) then $f(x,y) = -f(x,y)$ for all $x,y \in V$. }

\begin{proof}
Observe: 
\bee
0 &= f(x + y,x + y) = f(x + y,x) + f(x + y,y) \\
&= f(x,x) + f(y,x)  + f(x,y) + f(y,y) = f(y,x) + f(x,y).
\eee
So adding $-f(x,y)$ to both sides we have: 
$$
-f(x,y) = f(y,x).
$$
\end{proof}

\item \textit{Suppose that $-1 \neq 1$ in $F$. Prove that $f(x,y)$ is an alternating bilinear map on $V$ (i.e. $f(x,x) = 0$ for all $x \in V$) if and only if $f(x,y) = -f(y,x)$ for all $x,y \in V$. }

\begin{proof}
The forward direction follows from part (a). For the second direction, assume $f(x,y)  = -f(y,x)$. So we have $f(x,x) = -f(x,x)$. Since $-1 \neq 1$, we know $1 + 1 = r \neq 0 \in F$. So we have: 
$$
rf(x,x) = 0.
$$
Suppose $f(x,x) \neq 0 \in W$, where $W$ is the vector space which $f$ maps to. Then since $r \neq 0$ we have a contradiction since $\Set{f(x,x)}$ is linearly independent. So $f(x,x) = 0$ for all $x \in V$. 
\end{proof}

\item \textit{Suppose that $-1 = 1$ in $F$. Prove that every alternating bilinear form $f(x,y)$ on $V$ is symmetric (i.e. $f(x,y) = f(y,x)$ for all $x,y \in V$). Prove that there is a symmetric bilinear map on $V$ that is not alternating. [One approach: show that $\mc{C}^2(V) \sub \mc{A}^2(V)$ and $\mc{C}^2(V) \neq \mc{A}^2(V)$ by counting dimensions. Alternatively, construct an explicit symmetric map that is not alternating. ]}

\begin{proof}
For the first part, we use part (a), so we know:
$$
f(x,y) = -f(y,x) = f(y,x),
$$
 since $1 = -1$. For the second part, consider $f:V \to F$ given by $f(x,y) = x \cdot y$, the dot product. It is symmetric since addition in $F$ is abelian, but it is not alternating. Note $f(x,x) = 0$ if and only if $x = 0$. 
\end{proof}


\end{enumerate}

\end{enumerate}

\begin{rem}
Chapter 12 will be the primary part of the midterm, since it is practical, like linear algebra. We hope to finish it by next week. It might be only this stuff. 
\end{rem}





\chapter{Modules over principal ideal domains}


\section{The basic theory}



\bb\bb

\textbf{Tuesday, February 20th}


Let $V,W$ be finite-dimensional vector spaces over a field $F$. Let $\phi:V \to W$ be a linear mapping (homomorphism). Then if we choose a basis in $V,W$, we get a matrix for $\phi$. So can we choose a basis so that the matrix has an especially simple form?

We choose a basis $\Set{u_1,...,u_n}$ in $V$ s.t. $\Set{u_1,...,u_k}$ is a basis in $ker \phi$. Consider the vectors $\Set{\phi(u_{k + 1}),...,\phi(u_n)} \sub W$. They are linearly independent, and generate $\phi(V)$. How do we know they are linearly independent? It's just because that the first $k$ were picked to be in the kernel. We have: 
\bee
a_{k + 1}\phi(u_{k + 1}) + \cdots + a_n\phi(u_n) &= 0\\
\phi(a_{k + 1}u_{k + 1} + \cdots + a_nu_n) &= 0\\
a_{k + 1}u_{k + 1} + \cdots + a_nu_n &\in ker\phi\\
a_{k + 1}u_{k + 1} + \cdots + a_nu_n &= b_1u_1 + \cdots + b_ku_k,
\eee
for some $b_i$. So $a_{k + 1}= \cdots = a_n = 0$. So $\Set{\phi(u_{k + 1}),...,\phi(u_n)}$ is a basis in $\phi(V)$. Call them $v_1,...,v_{n - k}$ and add $v_{n - k + 1},...,v_m$ to get a basis in $W$. In the base $\Set{u_1,...,u_n}$,$\Set{v_1,...,v_m}$, the matrix of $\phi$ is:
$$
\lpar 
\begin{tabular}{ccc|ccc}
0 & $\cdots$ & 0 & 1 & 0 & 0\\
\vdots & & \vdots & 0 & $\ddots$ & 0\\
0 & $\cdots$ & 0 & 0 & 0 & 1\\
\hline
0 & $\cdots$ & 0 &0 & $\cdots$ & 0 \\
$\vdots$ & & $\vdots$ &$\vdots$ & & $\vdots$\\
0 & $\cdots$ & 0 &0 & $\cdots$ & 0 \\
\end{tabular}
\rpar.
$$

And if instead we choose $\Set{u_{k + 1},...,u_n,u_1,...,u_k}$, the matrix is:
$$
\lpar 
\begin{tabular}{ccc|ccc}
1 & 0 & 0 & 0 & $\cdots$ & 0 \\
0 & $\ddots$ & 0 & \vdots & & \vdots  \\
0 & 0 & 1 & 0 & $\cdots$ & 0 \\
\hline
0 & $\cdots$ & 0 &0 & $\cdots$ & 0 \\
$\vdots$ & & $\vdots$ &$\vdots$ & & $\vdots$\\
0 & $\cdots$ & 0 &0 & $\cdots$ & 0 \\
\end{tabular}
\rpar .
$$
In both, we have $m$ rows and $n$ columns, and the nonzero square has $n - k$ columns, so that is the rank of $\phi$. 

\begin{rem}
If $\phi:V \to W$ and $U \sub V$ is a subspace s.t. $\phi(U) \sub L$. Choose a basis $\Set{u_1,...,u_n}$ in $V$ s.t. $\Set{u_1,...,u_k}$ is a basis of $U$, and $\Set{v_1,...,v_m}$ in $W$ s.t. $\Set{v_1,..,v_l}$ is a basis in $L$. Then the matrix of $\phi$ has the form: 
$$
\lpar 
\begin{tabular}{c|c}
B & C\\
\hline
0 & D
\end{tabular}
\rpar ,
$$
where $\phi|_U:U \to L$ has matrix $B$. Also, $\phi$ induces a mapping $V/U \to W/L$. The matrix of this mapping is $D$, in bases $\Set{\bar{u_{k + 1}},...,\bar{u_n}}$, and $\Set{\bar{v_{l + 1}},...,\bar{v_m}}$. So $B$ maps $U \to L$, and $D$ maps $V/U \to W/L$. The bottom left hand corner is zero because we take a vector $\phi(u_1)$ and write it as a column vector in $W$, but actually it is in $L$, so after a certain point, all the rest of the entries of this vector are zero. 
\end{rem}

Consider $\phi:V \to V$. We ask the question of what is the simplest form of the matrix of $\phi$? Or given an $n \times n$ matrix $A$, what is the ``simplest" form of of $PAP^{-1}$ for all invertible $P$?

\begin{rem}
For $\phi:V \to W$, we consider all matrices $PAQ^{-1}$ for invertible $P,Q$. And $\forall A$, there exists $P,Q$ such that:
$$
PAQ^{-1} =  \lpar 
\begin{tabular}{ccc|ccc}
1 & 0 & 0 & 0 & $\cdots$ & 0 \\
0 & $\ddots$ & 0 & \vdots & & \vdots  \\
0 & 0 & 1 & 0 & $\cdots$ & 0 \\
\hline
0 & $\cdots$ & 0 &0 & $\cdots$ & 0 \\
$\vdots$ & & $\vdots$ &$\vdots$ & & $\vdots$\\
0 & $\cdots$ & 0 &0 & $\cdots$ & 0 \\
\end{tabular}
\rpar. 
$$
\end{rem}

\begin{rem}
if there is a basis $\Set{u_1,...,u_n}$ in $V$ s.t. $\phi(u_i) = \lambda_iu_i$ for all $i$ (\textbf{eigenbasis}), then in this basis, the matrix of $\phi$ is diagonal:
$$
\lpar 
\begin{matrix}
\lambda_1 & & 0\\
 & \ddots & \\
 0 & & \lambda_n
\end{matrix} \rpar. 
 $$
\end{rem}

\begin{Ex}
For the matrix:
$$
\lpar 
\begin{matrix}
0 & -1\\
1 & 0
\end{matrix} \rpar,
$$
there is no eigenbasis in $Mat_{2 \times 2}(\R)$. 
\end{Ex}

Consider $V$ as an $F[x]$-module, with $xu = \phi(u)$. Then:
$$
(a_nx^n + \cdots + a_1x + a_0)u = a_n\phi^n(u) + \cdots + a_1 \phi(u) + a_0u.
$$
And $F[x]$ is a PID. 

\bb\bb
Chapter 12 is about fundamental theorem of finitely generated modules over PIDs. 

Consider $\z,F[x]$. 

\begin{theorem}
Any finitely generated abelian group is isomorphic to a group of the form:
$$
\z^k \times \z_{n_1} \times \cdots \times \z_{n_l}.
$$
\end{theorem}

\begin{Def}
We introduce the rank of a module, not necessarily free. The \textbf{rank} of a module $M$ over an ID is the maximal number of linearly independent elements in $M$. 
\end{Def}

\begin{lem}
If $\Set{u_1,...,u_n}$ is a maximal linearly independent set, it don't have to generate $M$, but $M/R\Set{u_1,...,u_n}$ is a torsion module, because otherwise we could add one more element to this set and it would still be linearly independent. 
\end{lem}

\begin{proof}
Suppose $M/R\Set{u_1,...,u_n}$ is not torsion. Then $\exists u' \in M/R\Set{u_1,...,u_n}$ s.t. $ru' \neq 0 \in M/R\Set{u_1,...,u_n}$ (i.e. $ru' \notin R\Set{u_1,...,u_n}$) for all $r \in R$. But this is exactly the definition of linear independence, so then $\Set{u_1,...,u_n,u'}$ is independent, which is a contradiction since we said $\Set{u_1,...,u_n}$ was maximal. 
\end{proof}

\begin{rem}
Rank of a module is uniquely defined because we can multiply by the field of fractions. And it is equal to $\dim_FF \tens M$. Why is it equal? Because 
$$
0 \to R\Set{u_1,...,u_n} \to M \to M/R\Set{u_1,...,u_n} \to 0,
$$
where $R\Set{u_1,...,u_n}$ is free. So $F$ is flat, so:
$$
0 \to F^n \to F\tens M \to 0.
$$
\end{rem}

\bb\bb

\textbf{Wednesday, February 21st}

\begin{Def}
If $R$ is an integral domain, $M$ an $R$-module, the \textbf{rank} of $M$ is the cardinality of a maximal linearly independent subset of $M$. It is uniquely defined since we can extend it to the dimension of the vector space over field of fractions of $R$. 
\end{Def}

\begin{rem}
If $M = M_1 \oplus M_2$, then $\rank M = \rank M_1 + \rank M_2$. The following is also true and a homework problem:
$$
0 \to M_1 \to M \to M_2 \to 0.
$$
\end{rem}

\begin{rem}
If $N$ is a submodule of $M$, then $\rank N \leq \rank M$. 
\end{rem}

\begin{Ex}
$2 \z \subset \z$, but they both have rank $1$. So you can have a proper subset with the same rank as $M$, but not if they are vector spaces. 
\end{Ex}

\begin{theorem}\label{thm1213}
Let $R$ be a PID, and $M$ be a free $R$-module of rank $n$. Let $N$ be a submodule of $M$. Then $N$ is free, of rank $k \leq n$, and there is a basis $\Set{u_1,...,u_n}$ in $M$ and elements $a_1,...,a_n \in R$ s.t. $\Set{a_1u_1,...,a_ku_k}$ is a basis in $N$, and $a_1|a_2|\cdots|a_k$. 
\end{theorem}

\begin{proof}
By induction on $n$. If $n = 1$, $M \cong R$, and $N$ is an ideal in $R$. Since $R$ is a PID, we know $N = (a_1) = a_1R$. Then $\Set{1}$ is a basis in $M$, $a_1$ is a basis in $N$. 

Now let $M \cong R^n$. Let $N \neq 0$. So $M$ has rank $n$. $\forall f\in M^*$, $f(N)$ is an ideal in $R$, so $f(N) = (a_f)$ for some $a_f \in R$. Note $f(N)$ is a linear form. Note:
$$
f(x_1,...,x_n) = c_1x_1 + \cdots + c_nx_n.
$$
For at least one $f$, this ideal is nonzero. So there exists $f$ s.t. $f(N) \neq 0$, so $a_f \neq 0$ for this $f$. Now, $R$ is a PID, so it's a Noetherian ring. So any collection of ideals in $R$ has a maximal element. Choose $h \in M^*$ s.t. $(a_h)$ is a maximal element (not maximal ideal) of the set $\Set{(a_f):f \in M^*}$. Call it $a_1 = a_h$. So $a_1$ is the minimal element you can get this way. There exists $v_1 \in N$ s.t. $a_1 = h(v_1)$. So $a_1$ is the ``minimal" element which can be obtained this way. It is maximal in the sense that it is not contained in any larger ideal. In fact this ideal is absolutely maximal, and $a_1$ is absolutely minimal element, but we do not need this now. 

We now claim $\forall f \in M^*$, $a_1|f(v_1)$ (in fact, $a_1|f(v)$ $\forall v \in N$). If we apply linear forms to $v_1$, then $a_1$ divides all the results. 
\begin{proof}
Put $I = \Set{f(v_1):f \in M^*} = v_1(M^*)$, which is an ideal of $R$. So $I = (b)$ for some $b \in R$, since $R$ is a PID. Now $b|a_1 = h(v_1) \in I$. Also, there is $g \in M^*$ s.t. $g(v_1) = b$. So, $b \in g(N) = (a_g)$, and since $b|a_1$, $(a_1) = (a_h) \sub (a_g)$. But $h$ was chosen such that it was the maximal of all ideals of this sort, so $(a_h) = (a_g)$, so $(a_1) = (b) = I$. So $a_1|f(v_1)$ for all $f$. 
\end{proof}

In particular, if $M$ is identified with $R^n$, so if some basis in $M$ is chosen, then $a_1$ divides all coordinates of $v_1$. Remember that a \textbf{coordinate} is a linear form on $M = R^n$. It is a linear mapping from $M \to R$. So, there is $u_1 \in M$ s.t. $v_1 = a_1u_1$. And then, $h(u_1) = 1$, since $h(v_1) = a_1$. So what do we do, we consider all forms, linear forms on all elements of $N$, we find vector that gives us minimal result, then we claim that it is multiple of some vector with coefficient $a_1$. We find a minimal element and prove that all other elements are its multiples. 

Let $K = ker h$. We claim $M = Ru_1 \oplus K$, $n = Ra_1u_1 \oplus (K \cap N)$. Note $Ra_1 = Rv_1$. 

\begin{proof}
$\forall u \in M$, $u = h(u)u_1 + (u - h(u)u_1)$. Note $h(u)u_1 \in Ru_1$, and $u - h(u)u_1 \in K$, because when we apply $h$ to this on the right, we get zero, since $h(u_1) = 1$. Also, $K \cap Ru_1 = 0$. 

Also $\forall v \in N$, $v = h(v)u_1 + (v - h(v)u_1)$. Where the summand on the left is in $Ra_1u_1$, since $a_1|h(v)$, so $h(v)u_1 = \fracc{h(v)}{a_1}v_1$. And the summand on the right is in $K \cap N$. So $K \cap Ra_1u_1 = 0$. So this is direct sum. 
\end{proof}
Now, fix $M$, use induction on $\rank N$. Note $N = Rv_1 \oplus (K \cap N)$, and $\rank(K \cap N) = k - 1$, so by induction, $K \cap N$ is free. 

Now $M = Ru_1 + (M \cap K)$. And $\rank(M \cap K) = n - 1$. And $M \cap K$ is free as a submodule of $M$, and this follows from above. 

Now use induction on $n$, then $\exists$ a basis $\Set{u_2,...,u_n}$ in $M\cap K$ such that $\Set{a_2u_2,...,a_ku_k}$ is a basis in $N \cap K$, $a_2|\cdots|a_n$. Just prove $a_1|a_2$ and we are done. 

So define $f \in M^*$ by $f(\sum x_iu_i) = x_1 + x_2$. Note $u_1,...,u_n$ is a basis in $M$. Then $f(a_1u_1) = a_1$, where $a_1u_1 = v_1 \in N$. So $(a_1) \sub f(N) = (a_f)$, so $(a_1) = f(N)$ by maximality. Also, $f(a_2u_2) = a_2$, so $a_2 \in f(N) = (a_1)$, so $a_1|a_2$. 
\end{proof}

\textbf{Thursday, February 22nd}

We given an alternate, constructive proof of Theorem \ref{thm1213}, which works for Euclidean domains. 
\begin{proof}
Let $R$ be an ED, $M = R^n$, and let $N$ be a submodule of $M$. Then $N$ is finitely generated (\textbf{bonus problem to prove this}). A module is called \textbf{Noetherian } if any submodule is finitely generated, or equivalently, if any increasing sequence of submodules stabilizes. So $R^n$ is Noetherian if $R$ is Noetherian. Let $N$ be generated by $\Set{v_1,...,v_l}$. Then $N = \phi(R^l)$, where $\phi(e_i) = v_i$. Any finitely generated module is the image, factor module, of a free module. In coordinates in $R^n$, the matrix of $\phi$ in the natural basis of $R^n$ and the basis $e_i$ is $A = (v_1|v_2|...|v_l)$, where the $v$'s are the columns. Now we want to find a new bases $\Set{w_1,...,w_l} \in R^l$ and $\Set{u_1,...,u_n} \in R^n$ s.t. the matrix of $\phi$ has simplest form. We use elementary row operations. 

\begin{Def}
\textbf{Elementary operations:} if $\Set{u_1,...,u_n}$ is a basis in $R^n = M$, then switching the order $u_i \leftrightarrow u_j$ corresponds to switching the $i$-th and $j$-th rows. 

\begin{Ex}
Define:
$$
A = \lpar 
\begin{tabular}{c|c|c|c}
$a_{11}$ & $a_{12}$ & $\cdots$ & $a_{1l}$\\
$\vdots$ & $\vdots$ & & $\vdots$\\
$a_{n1}$ & $a_{n2}$ &$ \cdots$ &$ a_{nl}$
\end{tabular} \rpar.
$$
So:
$$
v = \lpar 
\begin{matrix}
a_1\\
\vdots\\
a_n
\end{matrix} \rpar.
$$
in $\Set{u_1,...,u_n}$. Where $v = a_1u_1 + \cdots + a_nu_n$. And the submodule is the block with $1$s on the diagonal:
$$
A =  \lpar 
\begin{tabular}{ccc|ccc}
1 & 0 & 0 & 0 & $\cdots$ & 0 \\
0 & $\ddots$ & 0 & \vdots & & \vdots  \\
0 & 0 & 1 & 0 & $\cdots$ & 0 \\
\hline
0 & $\cdots$ & 0 &0 & $\cdots$ & 0 \\
$\vdots$ & & $\vdots$ &$\vdots$ & & $\vdots$\\
0 & $\cdots$ & 0 &0 & $\cdots$ & 0 \\
\end{tabular}
\rpar. 
$$
And it is $k \times k$. 
\end{Ex}

Replacing $u_i$ by $u_i + cu_j$ corresponds to adding the $j$-th row multiplied by $c$ to the $i$-th row. 

Usually an elementary operation is multiplying a row by a constant, but we cannot do this here because not all constants are units. You can multiply rows by units, but not non-units. 
\end{Def}
\begin{Def}
Similar operations in $R^l$ correspond to \textbf{elementary column operations}. 
\end{Def}
\begin{enumerate}

\item
So assume there is an element in the first columm which is not divisible by $a_{11}$. If $a_{i1}$ is not divisible by $a_{11}$, find $c$ s.t. $a_{i1} = ca{11} + r$ with $N(r) < N(a_{i1})$ (Euclidean norm). Then the first entry of $row_i - crow_1$ is $r$. Switch row$_1$ and row$_i$ and get a smaller $(1,1)$ entry. Applying this to the first row, and column several times, we get all $a_{1k},a_{k1}$ to be divisible by $a_{11}$. This process cannot be infinite since $N(a) \in \n$. 
\item Subtract multiples of row$_1$ from other rows, and the multiples of col$_1$ from other columns, and get:
$$
\newcommand*{\temp}{\multicolumn{1}{c|}{b_{21}}}
\newcommand*{\tempq}{\multicolumn{1}{c|}{\vdots}}
\newcommand*{\tempw}{\multicolumn{1}{c|}{0}}
A = \lpar 
\begin{matrix}
b_{11} & 0 & \cdots & 0\\
\cline{2-4}
\temp & b_{22} & b_{23} & \cdots\\
\tempq & \ast & \ast & \ast\\
\tempw & \ast & \cdots & \ast
\end{matrix} \rpar 
$$
\item Assume there exists $i,j$ s.t. $b_{ij}$ not divisible by $b_{11}$. Add col$_j$ to col$_1$  to get $b_{ij}$ in the first column. Go to step 1. After repeating steps 1,2,3 many times, all $b_{ij}$ will be divisible by $b_{11}$. 
\item Pass to the submatrix:
$$
A = \lpar 
\begin{array}{c|cc}
b_{11} & 0 & \cdots\\
\hline
0 & B'
\end{array} \rpar 
$$
and continue.
\end{enumerate}
In the end we get:
$$
\lpar 
\begin{tabular}{ccc|ccc}
$c_1$ & 0 & 0 & 0 & $\cdots$ & 0 \\
0 & $\ddots$ & 0 & \vdots & & \vdots  \\
0 & 0 & $c_k$ & 0 & $\cdots$ & 0 \\
\hline
0 & $\cdots$ & 0 &0 & $\cdots$ & 0 \\
$\vdots$ & & $\vdots$ &$\vdots$ & & $\vdots$\\
0 & $\cdots$ & 0 &0 & $\cdots$ & 0 \\
\end{tabular}
\rpar,
$$
with $c_1|c_2|...|c_k$. We have a new basis $\Set{u_1,...,u_n}$ in $M = R^n$ and $\Set{c_1u_1,...,c_ku_k}$ is a basis in $N = \phi(R^l)$. 
\end{proof}

\begin{Ex}
Let $N$ be the submodule $\z^3$ generated by:
$$
\lpar 
\begin{array}{cc}
2 & 5\\
3 & 11\\
7 & 13
\end{array} \rpar.
$$
So we have:
$$
\newcommand*{\tempw}{\multicolumn{1}{c|}{0}}
\lpar 
\begin{array}{cc}
2 & 5\\
3 & 11\\
7 & 13
\end{array} \rpar 
\mapsto 
\lpar 
\begin{array}{cc}
1 & 6\\
2 & 5\\
7 & 13
\end{array} \rpar 
\mapsto
\lpar 
\begin{array}{cc}
1 & 6\\
0 & -7\\
0 & -29
\end{array} \rpar 
\mapsto
\lpar 
\begin{array}{cc}
1 & 0\\
\cline{2-2}
\tempw & -7\\
\tempw & -29
\end{array} \rpar 
$$
$$
\mapsto
\lpar 
\begin{array}{cc}
1 & 0\\
0 & -1\\
0 & -7
\end{array} \rpar 
\mapsto
\lpar 
\begin{array}{cc}
1 & 0\\
0 & -1\\
0 & 0
\end{array} \rpar 
\mapsto
\lpar 
\begin{array}{cc}
1 & 0\\
0 & 1\\
0 & 0
\end{array} \rpar.
$$
\end{Ex}

\begin{Ex}
Let $N = \z \cdot \Set{
\lpar 
\begin{matrix}
1\\
1
\end{matrix} \rpar,
\lpar 
\begin{matrix}
-1\\
1
\end{matrix} \rpar }.
$
And then:
$$
A = \lpar 
\begin{matrix}
1 & -1\\
1 & 1
\end{matrix} \rpar 
\mapsto
 \lpar 
\begin{matrix}
1 & -1\\
0 & 2
\end{matrix} \rpar 
\mapsto
 \lpar 
\begin{matrix}
1 & 0\\
0 & 2
\end{matrix} \rpar.
$$
\end{Ex}

\begin{rem}
Theorem \ref{thm1213} is not true if $R$ is not a PID:
\end{rem}

\begin{Ex}
Consider $M = R = F[x,y]$, and $N = (x,y)$. Note $\rank (M) = 1$, and $M/(x)$ is a torsion module. It is not free since $x,y$ do not have a common divisor. Note $N$ is not free because it cannot be generated by one element. Note $x,y$ cannot be linearly independent, since $yx - yx = 0$. Where we treat $x,y$ as elements of $N$, and coefficients from $R$. 
\end{Ex}

\begin{Ex}
Take $R = \z$, and $M = \z^n$. So $M$ is a lattice in $M$ in $n$-dimensional space. Assume we have a sublattice: we want to find a new basis such that the transformed sublattice aligns with the lattice formed by the points in $\z^n$. Let $u_2$ be the green vector, $u_1$ be the blue vector. Note $N = \z\Set{(1,1),(-1,1)}$. Where $(1,1) = u_1$, and $(-1,1) = 2u_2 - u_1$. Note $\Set{u_1,u_2}$ is a basis in $M$, and $\Set{u_1,2u_2}$ is a basis in $N$. 
\end{Ex}

\textbf{Thursday, February 22nd}

\begin{theorem}
If $R$ is a PID and $M$ is a finitely generated $R$-module, then $M$ is a product of cyclic modules, 
$$
M \cong R^r \oplus R/(a_1) \oplus \cdots \oplus R/(a_m).
$$
where $a_1,...,a_m \in R$ (non-units), and $a_1|a_2|...|a_m$,
 $r,a_1,...,a_m$ are uniquely defined, $r = rank M$, $a_1,...,a_m$ are called the invariant factors of $M$. 
\end{theorem}

\begin{proof}
$M$ is generated by $n$ elements. Then we have:
$$
0 \to N \to R^n \to M \to 0.
$$
$M = R^n/N$, where $N$ is a submodule of $R^n$. 

Now find a basis $\Set{u_1,...,u_n}$ in $R^n$ s.t. $\Set{a_1u_1,...,a_ku_k}$ is a basis on $N$, and $a_1|a_2|...|a_l$. Now $M = R^n/N$. So $u_1,...,u_k,...,u_n$. 
\end{proof}

\textbf{Friday, February 23rd}

We do some exercises. 

\begin{lem}
rank$(M)$ well-defined. 
\end{lem}
\begin{proof}

Let $N = R\Set{x_1,...,x_n}$. Then $N$ is a maximal free submodule of $M$. Then we have:
$$
0 \to N \to M \to M/N \to 0.
$$
Where $M/N$ is torsion. Let $F$ be the field of fractions of $R$. Then $F$ is a flat module. So:
$$
0 \to N \tens F \to M \tens F \to (M/N) \tens F \to 0
$$
is exact. Note $M/N$ is torsion, so then tensor product with $F$ is zero, kills torsion. And $N \tens F \cong F^n$ because of freedom. 
So we have:
$$
0 \to F^n \to M \tens F \to 0.
$$
So if $M$ has a maximal linearly independent set of cardinality $n$, then $M \tens F$ is an $n$-dimensional $F$-vector space. Since "$\dim$" is uniquely defined, $n$ is unique. So $F^n \cong M \tens F$ by exactness as well. 
\end{proof}


\begin{rem}
Let $M$ be a module over an ID. Then Tor$(M)$ is a submodule. But $M/Tor(M)$ doesn't have to be free. 
\end{rem}

\begin{rem}
If $N$ is a max free submodule, then $M/N$ is a torsion module, but it may be "larger" than Tor$(M)$. 
\end{rem}

\begin{Ex}
Let $R = F[x,y]$. Consider $M = (x,y) \sub F[x,y]$. It is torsion free, but not free (factorize). Also if we let $N = (x)$, then $M/N$ is torsion module, but $Tor(M) = 0$. 
\end{Ex}

\begin{rem}
If $R$ is a principal ideal, and $M$ is finitely generated, then:
 $$
 M \cong R^r \oplus [R/(a_1) \oplus \cdots \oplus R/(a_m)].
 $$
 Then:
 $$
 M = R^r \oplus R/(a_1) \oplus \cdots \oplus R/(a_m).
 $$
 Where $R^r$ is the free part, and the rest is the torsion part = Tor$(M)$. The free part is not uniquely defined. 
\end{rem}

\begin{rem}
Let $R$ be a principal ideal domain, and $M$ be an $R$-module. Then $M$ is free if and only if it is torsion-free. 
\end{rem} 

\begin{rem}
Let $R$ be a PID. Then $\forall a = p_1^{r_1}\cdots p_k^{r_k}$, distinct primes. Then $R/(a) \cong R/(p_1^{r_1}) \oplus \cdots \oplus R/(p_k^{r_k})$. By CRT.
\end{rem}

\begin{rem}
if $M$ is a finitely generated $R$-module, then:
\bee
M &\cong R^r \oplus R/(a_1) \oplus \cdots \oplus R/(a_m)\\
&\cong R^r \oplus  R/(p_1^{r_1}) \oplus \cdots \oplus R/(p_k^{r_k}),
\eee
for some primes $p_i$ not necessarily distinct. 
\end{rem}

\begin{Def}
These $p_1^{r_1},...,p_k^{r_k}$ above are called \textbf{elementary divisors} of $M$. (You will prove in homework that they are uniquely defined)
\end{Def}

\begin{Def}
Note:
\bee
M \cong R^r &\oplus \lpar R/(p_1^{r_1}) \oplus \cdots \oplus R/(p_1^{r_k})\rpar\\
&\oplus \lpar R/(p_2^{s_1}) \oplus \cdots \oplus R/(p_2^{s_l})\rpar\\
&\oplus \cdots \oplus \lpar • \rpar,
\eee
where $p_i$ are distinct now. So the first row in big parentheses is the $p_1$-primary component. And the stuff in second set of parentheses in second row is the $p_2$-primary component. 
\end{Def}

\begin{Def}
\textbf{($P$) - primary component of $M$} is:
$$
\bigcup_{r = 1}^\infty \text{Ann}(p^r) =\text{Ann}(p^r):r = max\Set{r_1,...,r_k}.
$$
\end{Def}

\begin{lem}
The $p_i$-primary components are uniquely defined. 
\end{lem}

\begin{proof}
To prove: given a finitely generated module annihilated by $p^r$ for some $r$, then it is isomorphic to $R/(p^{r_1}) \oplus \cdots \oplus R/(p^{r_k})$, where $r_1,...,r_k$ are uniquely defined. 
\end{proof}

\bb\bb

\textbf{Monday, February 26th}

\begin{theorem}

If $M$ is finitely generated over a PID, then:
$$
M \cong R^r \oplus R/(p_1^{r_1}) \oplus \cdots \oplus R/(p_l^{r_l}),
$$
where $p_i$ are primes. These $p_i^{r_i}$ are uniquely defined. Two modules written this way are isomorphic if and only if all the summands are the same up to permutation. 

\end{theorem}


From this you can deduce that the invariant factors of $M$ are also uniquely defined. You have another isomorphism:
$$
M \cong R/(a_1) \oplus \cdots \oplus R/(a_m) \oplus R^r.
$$
$a_1,...,a_m$ are called \textbf{invariant factors of $M$} and are uniquely defined up to units. Note we have invariant factors $\Rightarrow$ elementary divisors. We have:
\bee
a_1  &=p_1^{r_{1,1}}\cdots p_{e_1}^{r_{1,l_1}}\\
a_2  &=p_1^{r_{2,1}}\cdots p_{e_1}^{r_{2,l_2}}.\\
&\vdots
\eee
some of the exponents maybe equal to zero, but these $a_i$ are the elementary divisors. So we are solving for the $p_i^{r_i}$'s. 

And we can also go the other way around, find invariant factors from elementary divisors. We write:
\bee
&p_1^{r_{1,1}}\cdots p_{e_1}^{r_{1,s_1}},\\
&p_2^{r_{2,1}}\cdots p_{e_1}^{r_{2,s_2}},\\
&\vdots\\
&p_k^{r_{k,1}}\cdots p_{e_1}^{r_{k,s_k}}\\
\eee
where $r_{1,1} \geq r_{1,2} \geq ...$, and so on for all $i$. Then put:
\bee
a_m &= p_1^{r_{1,1}}p_2^{r_{2,1}}\cdots p_k^{r_{k,1}}\\
a_m &= p_1^{r_{1,2}}p_2^{r_{2,2}}\cdots p_k^{r_{k,2}}\\
&\vdots
\eee
Then we have $a_1|a_2|\cdots|a_m$. 

\begin{Ex}
Let our invariant factors be $2,2\cdot 3,2^2\cdot 3,2^3\cdot 3^2$. This gives us elementary divisors:
\bee
2\\
2,3\\
2^2,3\\
2^3,3^2.
\eee
So just take the biggest index of each distinct prime. 
\end{Ex}

Let $M$ be a finitely generated module over a PID $R$. Represent $M$ as a quotient module of $R^n$. Let $M \cong R^n/N$. So $N$ is free, so has a basis. Find such a basis $\Set{v_1,...,v_k}$ in $N$. And $A = (v_1|v_2|...|v_k)$. Note it's an $n \times k$ matrix. 
\begin{Def}
We have this matrix, it's called \textbf{the relation matrix of $M$}. 
Why is it called that? Let $\Set{u_1,...,u_n}$ be the generating set of $M$ used to construct the homomorphism $R^n \to M$ given by $e_i \to u_i$. The $u_i$ are not linearly independent in general. Let:
$$
A = \lpar 
\begin{matrix}
a_{11} & \cdots & a_{k1}\\
\vdots & & \vdots\\
a_{1n} & \cdots & a_{kn}
\end{matrix} \rpar. 
$$
Then it must be that 
\bee
a_{11}u_1 + \cdots + a_{1n}u_n &= 0\\
\vdots \\
a_{k1}u_1 + \cdots + a_{kn}u_n &= 0.
\eee
Indeed $a_{11}e_1 + \cdots + a_{1n}e_n  =v_1$, and $\phi(v_1)  =0$. This is the definition of a relation matrix. 
\end{Def}

So we want to find a basis $\Set{w_1,...,w_n}$ in $R^n$ s.t. $\Set{c_1w_1,...,c_mw_m}$ is a basis in $N$, with $c_1|c_2|...|c_m$. Then $M \cong R^n/N \cong R/(c_1) \oplus \cdots \oplus R/(c_m) \oplus R^{n - m}$, so $c_1,...,c_m$ are the invariant factors of $M$. In Euclidean domains, we use row-column operation to reduce $A$ to a form:
$$
\lpar 
\begin{tabular}{ccc}
$c_1$ & 0 & 0  \\
0 & $\ddots$ & 0\\
0 & 0 & $c_m$  \\
\hline
0 & $\cdots$ & 0  \\
$\vdots$ & & $\vdots$ \\
0 & $\cdots$ & 0  \\
\end{tabular}
\rpar,
$$
and ``protocol" all row operations you use. 

A basis in $M$ will be $\Set{\phi(c_1w_1),...,\phi(c_mw_m),\phi(w_{m + 1}),...,\phi(w_n)}$. Where the stuff up to index $m$ generates the torsion part, and the stuff from $m + 1$ up generates $R^{n  -m}$. Now shouldn't the rank of $A$ be $k$, since the $v_i$'s are linearly independent? So we must have $m = k$, Professor made a mistake. 










%========compile.fast==================================














\section*{12.1 Exercises}

\begin{enumerate}[label=\arabic*.]
\setcounter{enumi}{1}
\item \textit{$B = \Set{x_1,...,x_n}$ be a maximal linearly independent set in $M$ if and only if $RB$ is free and $M/RB$ is torsion module. }

\begin{proof}
\begin{enumerate}
\item 
\textit{$\Set{x_1,...,x_n}$ is linearly independent if and only if 
$$
R\Set{x_1,...,x_n}
$$ is a free module with basis $\Set{x_1,...,x_n}$. }
\begin{proof}
Professor Leibman completed this proof in class. 
\end{proof}
\item Let $\Set{x_1,...,x_n}$ be a maximal linearly independent set. Let $y  \in M$. Then $\exists a_1,...,a_n,b$ s.t. $a_1x_1 + \cdots + a_nx_n + by = 0$ and not all of $a_1,...,a_n,b$ are zero. If $b = 0$, then $a_1x_1 + \cdots + a_nx_n = 0$, this is impossible, since $x_1,...,x_n$ are linearly independent. So $b \neq 0$, and $by=  0 \mod R\Set{x_1,...,x_n}$. So $b\bar{y} = 0 \in M/R\Set{x_1,....,x_n}$. So $\forall \bar{y} \in M/R\Set{...}$, $\exists b \neq 0$ s.t. $b\bar{y} = 0$. 

Now we prove in the other direction. Assume that $M/R\Set{x_1,...,x_n}$ is a torsion module. Take $\forall y \in M$. Find $b \neq 0$ s.t. $b \bar{y} = 0$, that is, $by \in R\Set{x_1,...,x_n}$. So $by = a_1x_1 + \cdots + a_nx_n$ for some $a_i$, so $y,x_1,...,x_n$ are linearly dependent, so $\Set{x_1,...,x_n}$ is a maximal linearly independent set. We know this since we proved we could not add any other linearly independent element without making the whole set dependent. So it's maximal. 
\end{enumerate}
\end{proof}

\setcounter{enumi}{3}
\item \textit{Let $R$ be an integral domain, let $M$ be an $R$-module and let $N$ be a submodule of $M$. Suppose $M$ has rank $n$, $N$ has rank $r$ and the quotient $M/N$ has rank $s$. Prove that $n = r +s$. }
Use:
$$
0 \to N \to M \to M/N \to 0.
$$
Multiply tensor by field of fractions. Use 
$$rank(M) = rank(N) + rank(M/N).$$

\begin{proof}
Let $A = \Set{x_1,...,x_s}$, a set of elements in $M$ whose images are a maximal independent set in $M/N$. And let $B = \Set{x_{s + 1},...,x_{s + r}}$ be a maximal independent set in $N$. We prove $A$ is independent in $M$. Suppose it weren't. Then there is $l \neq 0$ in $R$ and $x_i \in A$ s.t. $lx_i = \sum_{j \neq i, \leq s} r_jx_j$. But then under the natural projection we would have a similar equality for $\bar{x_i}$ which would contradict the independence of $\bar{A}$. 

We wish to show that $A \cup B$ is a maximal linearly independent set. We first show it is independent. Let $x_i \in A$. Suppose there exists a nonzero $l \in R$ s.t. $lx_i = r_{s + 1}x_{s + 1} + \cdots + r_{s + r}x_{s + r}$ for $r_i \in R$. Then under the natural projection $\pi:M \to M/N$, we have $\pi(lx_i) = l\pi(x_i) = 0 \in M/N$. But note $\pi(x_i)$ is in $\bar{A	}$ which is an independent set in $M/N$ so we must have $\pi(x_i) \neq 0$ and that $\nexists l \in R$ s.t. $l\pi(x_i) = 0$. This is a contradiction, so we must have that there exists no such $l$, so every element in $A$ is independent of $B$. Now let $x_j \in B$ and suppose there exists a nonzero $l \in R$ s.t. $lx_j = r_1x_1 + \cdots + r_sx_s$. Then $\pi$ maps this to $0 \in M/N$ since $lx_j \in N$, but then since $\bar{A}$ is independent in $M/N$, we must have $r_1 = \cdots = r_s = 0$. Then we have $lx_j = 0$ which is a contradiction since $B$ cannot contain any torsion elements or it would not be independent. Then we have proved $A\cup B$ is independent. 



Now we show $A \cup B$ is maximal. Let $y \in M$. Then since $\bar{A}$ is a maximal linearly independent set in $M/N$, we know there exist $c,c_1,...,c_s$ not all zero such that:
$$
c\bar{y} + c_1\bar{x_1} + \cdots + c_s\bar{x_s} = 0,
$$
 which implies:
 $$
 cy + c_1x_1 + \cdots + c_sx_s = n \in N.
 $$
Now since $B$ is a maximal linearly independent set in $N$, we know that since $n \in N$, there exists $k,c_{s + 1},...,c_{s + r} \in R$ not all zero s.t. 
$$
kn = k(cy + c_1x_1 + \cdots + c_sx_s) = c_{s + 1}x_{s + 1} + \cdots + c_{s + r}x_{s + r}.
$$
But if $k = 0$, then we must have $c_{s + 1},..,c_{s + r} = 0$ since $B$ is independent. So we must have $k \neq 0$, thus we can write:
$$
kcy = \sum_{i = 1}^s-kc_ix_i + \sum_{i = s + 1}^{s + r}c_ix_i.
$$
And since we know $c,c_1,...,c_s$ are not all zero, we have found a nonzero $kc \in R$ (since we are in an ID) s.t. $kcy$ is a linear combination of $x_1,...,x_{s + r}$. So we have shown that $A \cup B$ is a maximal independent set in $M$, since for any $y \in M$ there is $kc$ s.t. $kcy$ is a combination of elements in $A \cup B$. 

Now we wish to show that $rank(M) = n = r + s$. So we use part (b) of Exercise 2 above. Note that $R^{r + s}$ is a submodule of $M$, since $x_1,...,x_{s + r}$ = $A \cup B$ is a maximal linearly independent set in $M$, and $R(A \cup B) = R^{r + s}$, and we have closure by ring action since $M$ is an $R$-module. 

\begin{lem}
If $\Set{u_1,...,u_n}$ is a maximal linearly independent set, it doesn't have to generate $M$, but $M/R\Set{u_1,...,u_n}$ is a torsion module, because otherwise we could add one more element to this set and it would still be linearly independent. 
\end{lem}

\begin{proof}
Suppose $M/R\Set{u_1,...,u_n}$ is not torsion. Then 
$$\exists u' \in M/R\Set{u_1,...,u_n}$$ s.t. $ru' \neq 0 \in M/R\Set{u_1,...,u_n}$ (i.e. $ru' \notin R\Set{u_1,...,u_n}$) for all $r \in R$. But this is exactly the definition of linear independence, so then $\Set{u_1,...,u_n,u'}$ is independent, which is a contradiction since we said $\Set{u_1,...,u_n}$ was maximal. 
\end{proof}

So by the above Lemma, we know $M/R^{r + s}$ is torsion. Then by Exercise 2 part (b), we know $rank(M) = n = r+ s$. 
\end{proof}

\item \textit{Consider $\z[x] \sim F[x,y]$. Note $(2,x)$ is not principal. }
Note $M$ has rank 1, is torsion free, but not free. It has rank 1 because if you take one of these elements, something linearly dependent maybe, idk. Consider $M/(2)$ then $x$ is a torsion element here since $2x = 0$. So it's a torsion module or something. And actually, it's true for any module over PID. 



\setcounter{enumi}{8}
\item \textit{Give an example of an integral domain $R$ and a nonzero torsion $R$-module $M$ such that $Ann(M) = 0$. Prove that if $N$ is a finitely generated torsion $R$-module, then $Ann(N) \neq 0$. }

Let $R = \z$, an integral domain. Define:
$$
M = \bigoplus_{i = 1}^\infty \z/2^i\z.
$$
Then $\forall a \in M$, $\exists k \in \z$ such that:
$$
a = (a_1 + \z/2\z,...,a_k + \z/2^k\z,0,...)
$$
for some $a_1,...,a_k \in \z$. Thus $2^ka = 0 \in M$, so $M$ is a torsion module. We claim that Ann$(M) = 0$. Suppose there exists a nonzero $r \in \z$ s.t. $r \in Ann(M)$. Then choose $k \in \z$ s.t. $r < 2^k$. Then define:
$$
a = (0,...,0,1 + \z/2^k\z,0,...)
$$
where the nonzero entry is in the $k$-th position. Then since $ra = 0$, we must have $r = 0$ since $r$ will not annihilate the nonzero entry of $a$ since $r < 2^k$. This is a contradiction since we said $r \neq 0$. So we must have Ann$(M) = 0$. 

\begin{proof}
Let $R$ be a integral domain. Let $N$ be finitely generated torsion $R$-module. Then $N \sub R\Set{x_1,...,x_n}$. And since it is torsion, there exist $\Set{r_1,...,r_n}$ s.t. $r_ix_i = 0$, where $r_i \neq 0$ $\forall i$. Then since we have no zero divisors, $lcm(r_1,...,r_n) \neq 0$, and this is in the annihilator by commutativity in $R$. 
\end{proof}

\setcounter{enumi}{10}

\item \textit{Let $R$ be a PID, let $a$ be a nonzero element of $R$ and let $M = R/(a)$. For any prime $p$ of $R$, prove that:
$$
p^{k - 1}M/p^kM \cong \begin{cases}
R/(p) & \text{ if } k \leq n\\
0 & \text{ if } k > n
\end{cases},
$$
where $n$ is the power of $p$ dividing $a$ in $R$. 
}

\begin{proof}
We first treat the case where $p \nmid a$. Then since $p$ is a prime in $R$, we know $\gcd(a,p) = 1$. So then we have $(p) \cap (a) = 0$. let $\pi:R \to R/(a) = M$. Then observe:
$$
\pi((p)) = (p)/(a) \cong [(p) + (a)]/(a) \cong (p)/((p) \cap (a)) \cong (p)/(0) \cong (p).
$$
But note that $(p) + (a) = (1) = R$, so we have shown $(p) = pM \cong R/(a) = M$, so $p^{k - 1}M = p^kM = M$ for all $k$, and thus since $M/M \cong 0$, we have the desired result. 

Now let $p\mid a$, and assume $k \leq n$. Then we have $a = p^np_1^{c_1}\cdots p_l^{c_l}$, for some distinct primes $p_i$. Using the result of Exercise 12.1.7 and the Chinese remainder theorem, we have:
\bee
\fracc{p^{k - 1}M}{p^kM} &= \fracc{p^{k - 1}R/(a)}{p^kR/(a)}\\
&\cong \fracc{p^{k -1} R/(p^n)(p_1^{c_1})\cdots(p_l^{c_l})  }{p^{k} R/(p^n)(p_1^{c_1})\cdots(p_l^{c_l})  }\\
&\cong \fracc{ R/(p^{n -k +1})(p_1^{c_1})\cdots(p_l^{c_l})  }{R/(p^{n - k})(p_1^{c_1})\cdots(p_l^{c_l})  }\\
&\cong \fracc{ R/(p^{n -k +1})\oplus R/(p_1^{c_1})\oplus \cdots\oplus R/(p_l^{c_l})  }{R/(p^{n - k})\oplus R/(p_1^{c_1})\oplus \cdots\oplus R/(p_l^{c_l})  }\\
&\cong (R/(p^{n -k +1}))/(R/(p^{n -k})) \oplus (R/(p_1^{c_1}))/(R/(p_1^{c_1}) )\\
&\oplus \cdots\oplus (R/(p_l^{c_l}))/(R/(p_l^{c_l}))\\
&\cong (R/(p^{n -k +1}))/(R/(p^{n -k}))\oplus 0 \oplus \cdots \oplus 0\\
&\cong (R/(p^{n -k +1}))/(R/(p^{n -k}))\\
&\cong R/(p).
\eee
Now suppose $k > n$. Then $a|p^{k - 1} \Rightarrow p^{k - 1}M \cong raR/(a) \cong 0$. 
\end{proof}

\item \textit{Let $R$ be a PID and let $p$ be a prime in $R$. }

\begin{enumerate}
\item \textit{Let $M$ be a finitely generated torsion $R$-module. Use the previous exercise to prove that $p^{k - 1}M/P^kM \cong F^{n_k}$ where $F$ is the field $R/(p)$ and $n_k$ is the number of elementary divisors of $M$ which are powers $p^{\alpha}$ with $\alpha \geq k$. }

\begin{proof}
Recall that a module over a PID is free if and only if it is torsion free, so since $M$ is not torsion free, it is not free, and by Theorem 6, we have:
$$
M \cong R^r \oplus R/(p_1^{\alpha_1}) \oplus \cdots \oplus R/(p_l^{\alpha_l}),
$$
where the primes are not necessarily distinct, and all the $\alpha$'s are positive. But then by Theorem 5, since $M$ is torsion, we know $r = 0$. So we have:
$$
M \cong  R/(p_1^{\alpha_1}) \oplus \cdots \oplus R/(p_l^{\alpha_l}).
$$
Define $a = p_1^{\alpha_1} \cdots p_l^{\alpha_l}$. Now we apply the result of the previous exercise to each of these summands. Let $s$ be the power of $p$ dividing $p_i^{\alpha_i}$. We set $M' = R/(p_i^{\alpha_i})$. So we know:
$$
p^{k - 1}M'/P^kM' \cong \begin{cases}
R/(p) & \text{ if } k \leq s\\
0 & \text{ if } k > s
\end{cases},
$$
So we have that $k \leq s$ for exactly $n_k$ of the elementary divisors $p_i^{\alpha_i}$, and so each of these summands is isomorphic to $F$, and the rest are zero. So we have:
$$
M \cong  F \oplus \cdots \oplus F \cong F^{n_k}. 
$$

\end{proof}

\item \textit{Suppose $M_1$ and $M_2$ are isomorphic finitely generated torsion $R$-modules. Use (a) to prove that, for every $k \geq 0$, $M_1$ and $M_2$ have the same number of elementary divisors $p^\alpha$ with $\alpha \geq k$. Prove that this implies $M_1$ and $M_2$ have the same set of elementary divisors. }

\begin{proof}
Applying part (a), we have:
$$
F^{n_{k_1}} \cong F^{n_{k_2}}.
$$
which tells us $n_{k_1} = n_{k_2}$ since they are isomorphic vector spaces of those dimensions. And we are done, since we iterate over the list of primes $p_i$ in the list of elementary divisors $\Set{p_i^{\alpha_i}}$, and also iterate over $k$ from zero to $\alpha_i$ for each $p_i$, and observe that we have exactly the same elementary divisors for $M_1$ and $M_2$ by induction. 
\end{proof}
\end{enumerate}


\end{enumerate}


\section{The rational canonical form}

\textbf{Tuesday, February 27th}

\begin{rem}
Let $F$ be a field, $V$ an $n$-dimensional $F$-vector space, and $T$ a linear transformation $T:V \to V$. Then $V$ gets the structure of an $F[x]$-module by $xu = T(u)$ for $u \in V$. 
\end{rem}

We state some facts about $V$. 

\begin{lem}
$V$ is a torsion module as an $F[x]$-module.  
\end{lem}

\begin{proof}
Note that any nonzero free $F[x]$-module is isomorphic to a direct sum of copies of $F[x]$, but each copy of $F[x]$ is an infinite-dimensional vector space over $F$, and so if $V$ has finite dimension over $F$, it must be a torsion $F[x]$ module, since its free rank must be zero. 
\end{proof}

\begin{lem}
Any $F[x]$-submodule of $V$ is a $T$-invariant subspace of $V$ over $F$. 
\end{lem}

\begin{proof}
Let $W$ be such a submodule, then it is a subgroup, so $FW\sub W$, so $xw = T(w) \in W$. 
\end{proof}

\begin{rem}
If $V = V_1 \oplus V_2$ as $F[x]$ modules then the summands are $T$-invariant. 
\end{rem}

\begin{rem}
$V$ is cyclic if there exists $u \in V$ s.t. $F[x]u = V$. 
\end{rem}

\begin{Def}
In the above case, $u$ is called a \textbf{cyclic vector} for $V$. 
\end{Def}

\begin{rem}
If $u$ is a cyclic vector for $V$, then $\forall v \in V$, $v = a_kT^k(u) + \cdots + a_1T(u) + a_0u$ for some $a_0,...,a_k \in F$. 
\end{rem}

Thus we have:

\begin{rem}
span$\Set{u,T(u),T^2(u),...} = V$ for any cyclic vector $u$ of $V$. 
\end{rem}

\begin{rem}
If $u$ is a cyclic vector of $V$, then $V \cong F[x]/(f)$ where $(f) = Ann(u)$ as an $F[x]$ module. 
\end{rem}


Let $f(x) = x^k + \cdots + a_1x + a_0$. Then $F[x]/(f) = \Set{b_0 + b_1x + \cdots + b_{k -1}x^{k - 1} \mod f}$, which is a $k$-dimensional vector space with basis $1,x,x^2,...x^{k - 1}$. 

Now since $F[x]/(f) \cong V$, $1 \cdot u$, $xu = T(u)$, $x^2u = T^2(u)$,...,$x^{k - 1}u = T^{k - 1}(u)$. We map:
\bee
1 &\mapsto x\\
x &\mapsto x^2\\
\vdots\\
x^{k - 2} &\mapsto x^{k - 1}\\
x^{k - 1} &\mapsto x^k = a_0 - \cdots a_{k - 1}x^{k - 1}.
\eee

\begin{Def}
And we have the \textbf{companion matrix} of $f$ where $f$ is monic, given by:
$$
A_i 
= \lpar 
\begin{array}{cccc|c}
0 & 0 & \cdots & 0 &-a_0\\
1 & 0 & & & -a_1\\
0 & 1 &\ddots & &\vdots\\
0 & 0 & \ddots & & \vdots\\
\vdots & \vdots & & 1 & -a_{k - 1}
\end{array} \rpar. 
$$
\end{Def}

\begin{rem}
Multiplication by $x$ in $F[x]/(f)$ corresponds to the action of $T$ on $V$. So in the basis $\Set{u,T(u),...,T^{n - 1}(u)}$, $T$ has this matrix also. 
\end{rem}

\begin{rem}
Let $T: V \to V$, and $S:W \to W$, then $V,W$ are $F[x]$-modules. What is a homomorphism $\phi:V \to W$? $\phi$ is a homomorphism of groups under addition, and we have $\phi(au = a\phi(u)$ $\forall a \in F$, so $\phi$ is a linear transformation $V \to W$. And additionally, $\phi(xu) = x\phi(u)$, where $\phi(xu) = \phi(T(u))$ and $x\phi(u) = S(\phi(u))$ typo? And the following diagram is commutative: 

\begin{center}
\begin{tikzcd}
V \arrow[r, "\phi"] \arrow[d, "T"] & W \arrow[d, "S"] \\
V \arrow[r, "\phi"] & W
\end{tikzcd}.
\end{center}
\end{rem}


\begin{rem}
$F[x]$ is a Euclidean domain (ED). 
\end{rem}

\begin{rem}
By the fundamental theorem, $V$ is a direct sum of cyclic submodules:
$$
V = V_1 \oplus \cdots + \oplus V_m,
$$
where $\forall i$, $V_i$ is cyclic, $V_i \cong F[x]/(f_i)$. Moreover, we may choose $V_i$ s.t. $f_1|f_2|...|f_m$ (invariant factors), or so that $\forall i$, $f_i = P_i^{r_i}$, where $P_i$ are irreducible polynomials (elementary divisors). 


In each $V_i$, choose a "cyclic" basis $\Set{u,Tu,...,T^{k - 1}u}$ and unite these basis elements. Then in the obtained basis, the matrix of $T$ is:
$$
\newcommand*{\boxone}{\multicolumn{1}{c|}{A_1}}
\newcommand*{\boxtwo}{\multicolumn{1}{|c|}{A_2}}
\newcommand*{\boxk}{\multicolumn{1}{|c}{A_k}}
\newcommand*{\boxdots}{\multicolumn{1}{|c|}{\ddots}}
\newcommand\bigzero{\makebox(0,0){\text{\huge0}}}
A = \lpar
\begin{matrix}
\boxone &  &  & \text{\huge0}\\
\cline{1-2}
 & \boxtwo &  & \\
 \cline{2-2}
 &  &  \ddots & \\
  \cline{4-4}
\text{\huge0} &  & & \boxk
\end{matrix} \rpar,
$$
where $\forall i$, $A_i$ is the companion matrix of $f_i$. If $f_1,...,f_m$ are invariant factors, $A$ is called the \textbf{rational canonical form} of the matrix of $T$. Both the \textbf{rational canonical form} and elementary divisors form are uniquely defined. 
\end{rem}

\begin{rem}
Any $n \times n$ matrix over $F$ is similar to such "RCF" and "elem divisors" form matrices. 
\end{rem}

\begin{Def}
Two matrices are similar ($B = PCP^{-1}$) if and only if they have the same invariant factors or the same elementary divisors. 
\end{Def}

\begin{rem}
The invariant factors $f_1,...,f_m$ don't depend on the field (what does this mean?)
\end{rem}



























\textbf{Wednesday, February 28th}

Let $\dim V = N$, $T:V \to V$. Then $\exists$ a basis in which the matrix of $T$ is:
$$
\newcommand*{\boxone}{\multicolumn{1}{c|}{A_1}}
\newcommand*{\boxtwo}{\multicolumn{1}{|c|}{A_2}}
\newcommand*{\boxk}{\multicolumn{1}{|c}{A_k}}
\newcommand*{\boxdots}{\multicolumn{1}{|c|}{\ddots}}
\newcommand\bigzero{\makebox(0,0){\text{\huge0}}}
A = \lpar
\begin{matrix}
\boxone &  &  & \text{\huge0}\\
\cline{1-2}
 & \boxtwo &  & \\
 \cline{2-3}
 &  &  \boxdots & \\
  \cline{3-4}
\text{\huge0} &  & & \boxk
\end{matrix} \rpar
$$

\textbf{Thursday, March 1st}

Consider $w_1,...,w_n \in F[x]^n$. Where:
$$
w_1 = \lpar \begin{matrix}
x\\
0\\
\vdots\\
0
\end{matrix} \rpar  - 
\lpar \begin{matrix}
a_{11}\\
\vdots\\
a_{n1}
\end{matrix} \rpar,...,
w_n = \lpar \begin{matrix}
0\\
\vdots\\
0\\
x
\end{matrix} \rpar  - 
\lpar \begin{matrix}
a_{1n}\\
\vdots\\
a_{nn}
\end{matrix} \rpar
$$
Define $\tilde{N} = R\Set{w_1,...,w_n}$. Then $\tilde{M} = F[x]^n/\tilde{N}$ is an $F$-vector space of dim $\leq n$. In $\tilde{M}$, we have:
$$
\lpar \begin{matrix}
x\\
0\\
\vdots\\
0
\end{matrix} \rpar  =
\lpar \begin{matrix}
a_{11}\\
\vdots\\
a_{n1}
\end{matrix} \rpar,...,
\lpar \begin{matrix}
0\\
\vdots\\
0\\
x
\end{matrix} \rpar  =
\lpar \begin{matrix}
a_{1n}\\
\vdots\\
a_{nn}
\end{matrix} \rpar
$$
And we have:
\bee
\lpar \begin{matrix}
x^2\\
0\\
\vdots\\
0
\end{matrix} \rpar 
&= 
x \cdot 
\lpar \begin{matrix}
a_{11}\\
\vdots\\
a_{n1}
\end{matrix} \rpar
=
\lpar \begin{matrix}
a_{11}x\\
\vdots\\
a_{n1}x
\end{matrix} \rpar
=
a_{11}
\lpar \begin{matrix}
x\\
0\\
\vdots\\
0
\end{matrix} \rpar 
+ \cdots + 
a_{n1}
\lpar \begin{matrix}
0\\
\vdots\\
0\\
x
\end{matrix} \rpar\\
&= 
a_{11}
\lpar \begin{matrix}
a_{11}\\
\vdots\\
a_{n1}
\end{matrix} \rpar
+ \cdots + 
a_{n1}
\lpar \begin{matrix}
a_{1n}\\
\vdots\\
a_{nn}
\end{matrix} \rpar\\
&= \lpar 
\begin{matrix}
c_1\\
\vdots\\
c_n
\end{matrix} \rpar.
\eee
So $\forall $ element of $\tilde{M}$ can be written as $\lpar \begin{matrix}
b_1\\
\vdots\\
b_n
\end{matrix} \rpar \in F$. So $\dim_F\tilde{M} \leq n$. So $V = F[x]^n/N$ where $N$ has all relations, so $\tilde{N} \sub N$ so if $\tilde{N} \neq N$, then $V$ is a nontrivial factor of $\tilde{M}$, so $\dim V < n$, not the case, so $N = \tilde{N}$. It's not the case since we assumed $\dim V = n$. 


Observe we have $\tilde{N} \sub N \sub M$. Then we have $M/\tilde{N} = \tilde{M}$, and $V = M/N$ by definition. And $V \cong M/N \cong \lpar (M/\tilde{N})/(N/\tilde{N}) \rpar  = \tilde{M}/(N/\tilde{N})$. This is by the third isomorphism theorem for modules. 

Then we have $xI - A$ - relations matrix $\Rightarrow$ Smith's form. We have:
$$
\lpar 
\begin{tabular}{cccccc}
$c_1$ & 0 & 0 & 0 & $\cdots$ & 0 \\
0 & $\ddots$ & 0 & \vdots & & \vdots  \\
0 & 0 & $c_k$ & 0 & $\cdots$ & 0 \\
0 & $\cdots$ & 0 &$f_1$ & $\cdots$ & 0 \\
$\vdots$ & & $\vdots$ &$\vdots$ & & $\vdots$\\
0 & $\cdots$ & 0 &0 & $\cdots$ & $f_k$\\
\end{tabular}
\rpar,
$$
where the $f_i$'s are the invariant factors. And $f_m = m_T$ where $m_T$ is the minimal polynomial. Now we have $\det(xI - A) = \det(Smith) = f_1\cdots f_m$ (up to a scalar from $F$). Where $\det(xI - A)$ is the characteristic polynomial of $T$, $c_T(x)$. So $m_T|c_T$. 

\begin{Cor}[\textbf{Cayley-Hamilton theorem}]
$c_T(T) = 0$, since $m_T(T)  =0$. 
\end{Cor}

Then note $f_m|c_T$, and $f_m = m_T$ and $c_T$ have the same irreducible factors. This is because $f_1|f_2|\cdots|f_m$. So $c_T|f_m^m$. If $m_T = c_T$, then $m = 1$ and we have a single rational cell in the canonical form of $T$. 

If you know the order of the group you can write it as products of primes and write it so first divides the second... And we can do the same here for these polynomials. 




We discuss the \textbf{Jordan normal form }of $T$. This is not universal, so its different from rational. Assume that $f_m = m_T$ splits to a product of powers of linear factors:
$$m_T(x) =(x - \lambda_1)^{r_1}\cdots(x - \lambda_k)^{r_k}.
$$
 This is always the case if $F$ is "algebraically closed", for example, if $F = \c$. But we digress, since we assumed that $f_m$ splits. Then also $c_T(x)$ splits, $f_1,...,f_{m - 1}$ split. The elementary divisors form of the matrix of $T$ is:
$$
\newcommand*{\boxone}{\multicolumn{1}{c|}{A_1}}
\newcommand*{\boxtwo}{\multicolumn{1}{|c|}{A_2}}
\newcommand*{\boxk}{\multicolumn{1}{|c}{A_k}}
\newcommand*{\boxdots}{\multicolumn{1}{|c|}{\ddots}}
\newcommand\bigzero{\makebox(0,0){\text{\huge0}}}
A = \lpar
\begin{matrix}
\boxone &  &  & \text{\huge0}\\
\cline{1-2}
 & \boxtwo &  & \\
 \cline{2-2}
 &  &  \ddots & \\
  \cline{4-4}
\text{\huge0} &  & & \boxk
\end{matrix} \rpar. 
$$
where $\forall i$ $A_i$ is the companion matrix of $(x - \lambda_i)^{r_i}$ (find it the way you would for any regular companion matrix). 

\begin{rem}
If two matrices have the same $c_T$ then they have the same rational form, so they must be conjugate (if they are $2 \times 2$ matrices).
\end{rem}

\textbf{Friday, March 2nd}

We do exercises from the sample exam sheet. 

\textbf{Monday, March 5th}

We do exercises from the sample exam sheet. 

\section*{12.2 Exercises}

\begin{enumerate}[label = \arabic*.]

\setcounter{enumi}{2}
\item \textit{Prove that $2 \times 2$ non-scalar matrices are similar if and only if they have the same characteristic polynomial. }


\begin{proof}
\begin{Def}
A \textbf{scalar} matrix is a matrix of the form:
$$
\lpar \begin{array}{cc}
c & 0\\
0 & c
\end{array} \rpar  = cI.
$$
\end{Def}

Note $\deg c_T(x) = 2$. Note we have $c_T = f_1 = m_T$ or $c_T = f_1f_2$ s.t. $f_1|f_2$. If we are in the second case, then $c_T = f_1\cdot f_2$ then $f_1 = f_2 = x -c$, since we always assume that they are monic polynomials. And because at most linear. So $T = \lpar 
\begin{array}{cc}
c & 0\\
0 & c
\end{array} \rpar$. If $c_T = m_T$, then there exists a single rational cell whose companion matrix is $c_T$. So two matrices are conjugate (same thing as similar) if and only if they have same $c_T$, so same rational form. 

\end{proof}

\item \textit{Prove that 3 by 3 matrices are similar if and only if they have the same $m_T$ and same $c_T$. Give explicit counterexample for 4 by 4.}

\begin{proof}
This is a case by case argument. We know the degree of $c_T$ is 3. If $deg(m_T) = 3$ then we are done, we only have 1 invariant factor and the RCF (similarity class) is in 1-1 correspondence with invariant factors. Now suppose $m_T$ has degree 2. Then the remaining invariant factor must have degree 1 and divide $m_T$, so it is just equal to $c_T/m_T$ which is unique, so we're done. If $m_T$ has degree 1 then all three invariant factors are equal to it and we are done. 
\end{proof}

\setcounter{enumi}{4}

\item $\dim(End(F^n)) = \dim(M_{n \times n}) = n^2$. Then if we take $I,T,T^2,...,T^{n^2}$ are linearly dependent in $M_{n \times n}$. So there exists $a_0,...,a_n \in F$ not all zero s.t. $a_0I + a_1T + \cdots + a_{n^2}T^{n^2} = 0$, so $f(T) = 0$ for $f(x) = a_{n^2}x^{n^2} + \cdots + a_1x + a_0$. So we just took a nondegenerate transformation and made a set with identity of $n^2 + 1$ elements, used linear independence to construct a polynomial that would annihilate $T$, and proved that $deg(m_T) \leq n^2$. 

\setcounter{enumi}{9}

\item Over $\Q$ take $6 \times 6$ matrices with min polynomial $m_T= (x + 2)^2(x - 1) = a^2b$. Find all similarity classes. Then either we have $f_1 = f_2 = m_T$, or we could have $f_1 = (x + 2),f_2 = (x + 2)^2, f_3 = m_T$. Or we could have $f_1 = f_2 = f_3 = (x - 1)$ and $f_4 = m_T$. 
So the possibilities are:
\bee
a,a^2, a^2b\\
a,a,a,a^2b\\
b,b,b,a^2b\\
a^2b,a^2b\\
a,ab, a^2b\\
b,ab,a^2b
\eee
And that's it, that's the answer, no need to write anything else. But if you're asked to write the RCF then do that, and also say when Jordan exists. We write Jordan normal form for $a^2b,a^2b$:
$$
\lpar 
\begin{matrix}
-2 & 1 & & &\\
0 & -2 & & & \\
& & 1\\
& & & -2 & 1\\
& & & 0 & -2\\
& & & & & 1
\end{matrix} \rpar.
$$


\setcounter{enumi}{17}
\item $T^{-1} = T^2 + T$. $T^3 + T^2 - 1 = 0$, so $f = x^3 + x^2 - 1$ is irreducible over $\mathbb{Q}$. So $f$ is the minimal polynomial and all other invariant factors divide $f$. So the minimal polynomial must have degree $\leq n^2$. 





\item \textit{$\forall A$, $A,A^T$ are conjugate. (Number of problem not known)}

How do we prove this, we use row operations on $xI - A$ and column operations on $xI - A^T$ and get the same Smith normal form. And the Smith normal form gives you invariant factors, and they will be the same. The result will be the same because they remain transposes of each other. 







\end{enumerate}

\part{FIELD THEORY AND GALOIS THEORY}


\chapter{Field theory}

\setcounter{section}{-1}

\section{Overview of Galois theory}

\textbf{Wednesday, March 7th}

We solve: $a_nx^n + \cdots + a_1x + a_0 = 0$, for $a_i \in \z$. Then we have $a_1x + a_0 \Rightarrow x = -\fracc{a_0}{a_1}$. And:
$$
a_2x^2 + a_1x + a_0 = 0 \Rightarrow x = \fracc{-a_1 \pm \sqrt{a_1^2 - 4a_0a_2}}{2a_2}.
$$
And $x^2 - 2 = 0$, solution $x = \sqrt{2}$. Radicals: solutions of $x^n = a$, $\sqrt[n]{a}$. In 1540, Ferrari, Cardano found a cubic formula. All attempts to solve equations of degree 5 have failed. In around 1825, Abel found a example of a degree 5 equation unsolvable in radicals. 



Then we have Evariste Galois, 1811-1832. We consider:
$$
f(x) = x^n + \cdots + a_1x + a_0, a_i \in \Q.
$$
We have roots $\alpha_1,...,\alpha_n \in \c$. Field of $f$, $\Q(\alpha_1,...,\alpha_n)$ is the field generated by $\alpha_1,...,\alpha_n$. This means that we start with $\Q$. Then let $K_1 = \Q(\sqrt[3]{2})$, $K_2 = K_1(\sqrt[5]{\sqrt[3]{2} - 3})$, $K_3 = K_2(\sqrt{7})$, $K_4 = K_3(\sqrt{\sqrt{7} + \sqrt[3]{\sqrt[3]{
\sqrt{\cdots}}}})$. 

\begin{Def}
We construct a series of \textbf{extensions}:
\begin{center}
\begin{tikzcd}
K_m \arrow[d, no head] \\
\vdots \arrow[d, no head] \\
K_1 \arrow[d, no head] \\
\Q
\end{tikzcd},
\end{center}
where the $K_i$'s are radical extensions. We need a tower of radical extensions such that $\alpha_1,...,\alpha_n \in K_m$. 
\end{Def}

\begin{Def}\index{Galois!group}
Consider the group of permutations of $\alpha_1,...,\alpha_n$ that preserves all relations between $\alpha_1,...,\alpha_n$. This is the group of automorphisms Aut$(R/\Q) = \text{Aut}(F)$ and is called the \textbf{Galois group} of $F$, Gal$(F) = \text{Gal}(F/\Q)$. 
\end{Def}

\begin{rem}
Any element $\sigma \in \gal(K)$ is uniquely defined by its action on $\Set{\alpha_i}$, the roots of the polynomial of which $K$ is the splitting field. 
\end{rem}


\begin{rem}
Then for any extension $F$ of a field $K$, $F/K$, $F/K$ we have a group Gal$(F/K) = $Aut$(F/K)$, where $\phi:F \to F$ s.t. $\phi|_K = Id_K$. 
\end{rem}


We have $Gal(F/E) \leq Gal(F/K)$. And $Gal(E/K) = Gal(F/K)/(Gal(F/E)$, where:
\begin{center}
\begin{tikzcd}
F \arrow[d, no head] \\
E \arrow[d, no head] \\
K
\end{tikzcd}.
\end{center}

\begin{named}{Galois theorem}{}
There exists a 1-1 correspondence between subextensions $E/K$ of $F/k$ and subgroups of $Gal(F/K)$. And $E \leftrightarrow Gal(F/E)$. 
\end{named}

Consider a tower of extensions:
\begin{center}
\begin{tikzcd}
K_m \arrow[d, no head] \\
\vdots \arrow[d, no head] \\
K_1 \arrow[d, no head] \\
\Q = K_0
\end{tikzcd},
\end{center}
then we have a series of subgroups:
$$
\Set{1} = H_0 \leq H_2 \leq H_3 \leq ... \leq H_m.
$$

Assume $K_i$ contains all roots of 1, for a radical extension $K_{i + 1}/K_i$, $Gal(K_{i + 1}/K_i)$ is cyclic. Then $H_{i + 1}/H_i$ are cyclic, and $H_m$ is solvable. 

\begin{rem}
So if $f$ is solvable in radicals, its field $F \sub K_m$, then $gal(f) = Gal(F/\Q)$ is a quotient group of a solvable group, so it solvable (and vice versa). 
\end{rem}


\begin{rem}
So if $Gal(f)$ is not solvable, $f$ is not solvable in radicals. 
\end{rem}

Note $S_2,S_3,S_4$ are solvable groups so $\forall$ polynomial of degree $\leq 4$ is solvable in radicals. There exists polynomials of degree 5 whose $Gal \cong S_5$ so it is not solvable in radicals. 

\section{Basic theory of field extensions}



\begin{Def}
A field is a commutative ring where all nonzero elements are invertible, so that $F^* = F\setminus \Set{0}$ is a group under multiplication. 
\end{Def}

\begin{Def}
The \textbf{prime subfield} of $F$ is the subfield generated by $1$. 


We have $0,1,2 = 1 + 1,...$. 
\end{Def}

Two cases:
\begin{enumerate}
\item there exists minimal $n \in \n$ s.t. $n = 0$ in $F$, then $F$ contains a copy of $\z_n$. Since $F$ has no zero divisors, $n$ is prime, $n = p$, so $\z_p \sub F$, for $\z_p$ a prime subfield, $p = char(p)$, characteristic (finite). 
\item $\nexists n > 0$ s.t. $n = 0$ in $F$. Then $F$ contains a copy of $\z$. Then it contains a copy of $\Q$, the field of fractions of $\z$. Note if you have a homomorphism:
\begin{center}
\begin{tikzcd}
\z \arrow[r, "\phi"] \arrow[d] & F \\
\Q \arrow[ru] & 
\end{tikzcd}
\end{center}
given by $\phi(\fracc{m}{n}) \mapsto \fracc{\phi(m)}{\phi(n)}$. Then $\Q$ is the prime subfield, char$F = 0$. 
\end{enumerate}


\textbf{Thursday, March 8th}

Let $F$ be a field. We defined the prime subfield in the last class. Recall that it is the subfield of $F$ generated by 1. Then the prime subfield is either $\z_p$ or $\Q$. 

We repeat this, but recall that we always have a homomorphism, there is a unique epimorphism $\phi:\z \to F$ which sends $1 \to 1$. This is because $\z$ is the universal free abelian group. We have two cases:

\textbf{Case 1: } $\ker \phi \neq 0$. Then $\ker \phi = (p)$. Then $\z_p = \z/(p) \sub F$. So $\z_p$ has no zero divisors, so $p$ is prime. We have $\z_p = \mathbb{F}_p$- prime subfield of $F$, char$F = p$. 

\textbf{Case 2: }$\ker\phi = 0$. Then $\z \cong \phi(\z) \sub F$. Then $\phi$ extends to an injective homomorphism from $\Q \to F$. So then $F$ contains a copy of $\Q$ (prime subfield of $F$), and char$F = 0$. 

\begin{Def}
If $F$ is a subfield of $K$, then $K$ is called an \textbf{extension} of $F$. Notation: $K/F$ or:
\begin{center}
\begin{tikzcd}
K \arrow[d, no head] \\
F
\end{tikzcd}.
\end{center}
Note this fraction bar notation is \textbf{not factorization!!!!} No factorization exists, since there are no nontrivial ideals. 
\end{Def}

\begin{Def}
Let $K/F$ be an extension. Let $S \sub K$. The minimal subfield of $K$ containing $F$ and $S$ is called the \textbf{subextension generated by $S$}, denoted $F(S)$. Then:
$$
F(S) = \Set{\fracc{p(s_1,...,s_k)}{q(s_1,...,s_k)}: p,q \in F[x], s_1,...,s_k \in S}. 
$$
\end{Def}

\begin{Def}
If $K_1,K_2$ are subfields of $K$, then $K_1(K_2) = K_2(K_1)$ is called the \textbf{composite of $K_1,K_2$}, and is denoted by $K_1K_2$. 
\end{Def}

\begin{Def}
Let $K/F$ be an extension, let $\alpha \in K$. The extension $F(\alpha)/F$ is called \textbf{simple}. (An extension is simple if it is generated by a simple element, also called a primitive element.)
\end{Def}

We have a homomorphism $\phi:F[x] \to K$ s.t. $\phi(p(x)) = p(\alpha)$ where $x \mapsto \alpha$. 

\textbf{Case 1: }$\ker\phi = 0$. Then $K$ contains a copy of $F[x]$, so a copy of $F(x)$, the field of fractions of $F[x]$. So $F(\alpha) \cong F(x)$. Recall:
$$
F(\alpha)  = \Set{\fracc{p(\alpha}{q(\alpha)}:p,q \in F[x]}.
$$

\begin{Def}
$\alpha$ is said to be \textbf{transcendental} over $F$ if and only if $p(\alpha) \neq 0$ $\forall$ nonzero $p \in F[x]$. 
\end{Def}

\begin{Ex}
$\pi,e$ are transcendental over $\Q$. 
\end{Ex}


\textbf{Case 2: }$\ker\phi \neq 0$. Then $\ker\phi$ is an ideal in $F[x]$, so $\ker\phi = (f(x))$. $K$ has no zero divisors, so $(f)$ is a prime ideal, so $f$ is prime = irreducible in $F[x]$. So $F[x]/(f)$ is a field. So $F(\alpha) = \phi(F[x])$ is isomorphic to $F[x]/(f)$. 


If $f$ has degree $n$, then:
$$
F[x]/(f) = \Set{a_0 + a_1x + \cdots + a_{n - 1}x^{n - 1} \mod f:a_i \in F}.
$$
So:
$$
F(\alpha) = \Set{a_0 + a_1\alpha + \cdots + a_{n - 1}\alpha^{n - 1}:a_i \in F} = F[\alpha].
$$
Note here that $F(\alpha) = F[\alpha]$. 

And $\Set{1,\alpha,...,\alpha^{n - 1}}$ is a basis in $F(\alpha)$, and $[F(\alpha):F] = n$. 


\section*{13.1 Exercises}

\begin{enumerate}[label=\arabic*.]

\item \textit{$p(x) = x^3 + 9x + 6$ - irreducible in $\Q[x]$ by Eisenstein's. $\theta$ is a root of $p$. Find $(1 + \theta)^{-1}$. }

This shit is irreducible by Eisenstein. Consider 
$$
\Q(\theta) = \Set{a + b\theta + c\theta^2: a,b,c \in \Q}.
$$ 
Write:
\bee
(a + b\theta + c\theta^2)(1 + \theta) &= 1\\
a + a\theta + b\theta + b\theta^2 + c\theta^2 + c\theta^3 &= 1\\
a + a\theta + b\theta + b\theta^2 + c\theta^2 - 9c\theta -6c &= 1\\
\begin{cases}
a - 6c = 1\\
a + b - 9c = 0\\
b + c = 0
\end{cases}.
\eee
Note $\theta^3 = -9\theta - 6$. 

Another way to do it: consider $T: \Q(\theta) \to \Q(\theta)$ where $Tu = \theta u$. The matrix of $\theta$ is:
$$
\lpar 
\begin{matrix}
0 & 0 & -6\\
1 & 0 & -9\\
0 & 1 & 0
\end{matrix} \rpar,
$$
and the matrix of $(1 + \theta)$ is:
$$
\lpar 
\begin{matrix}
1 & 0 & -6\\
1 & 1 & -9\\
0 & 1 & 1
\end{matrix} \rpar = A.
$$

And we have the basis $\Set{1,\theta, \theta^2}$. Why is this a basis? Note $F(\alpha) \cong F[x]/(m_{\alpha})$. Note $m_{\alpha} = x^3 + ax^2 + bx + c$. When you factorize by this ideal, you get:
\bee
F(\alpha) = \Set{ax^2 + bx + c: a,b,c \in F}.
\eee
So $F(\alpha) = \Set{a\alpha^2 + b\alpha + c}$. We just have this set of polynomials, these are equivalence classes module this polynomial. And $1,x,x^2$ is a basis in this ring. If the polynomial is irreducible, this ring is a field. If you take this ring and factorize it by this field, the cosets, equivalence classes, just correspond to poylnomials with degree $< 3$. Why are they independent? If you take two polynomials of this sort, their difference never belongs to the thing we are factorizing by. If $m_\alpha$ is reducible, it is just a ring. If it is irreducible, then the ideal is maximal, so it's a field. Now if you start with $\alpha$, then you find its minimal polynomial, and we get the thing above. For each $\alpha$ which is algebraic, so we have an ideal generated by some polynomial, and if it is reducible we get zero divisors, but we have no zero divisors since field, so done. 



And we find $A^{-1}$. 

\item \textit{You can look at the book to find the question. }

\begin{proof}
Use Eisenstein, it's obvious. If it's not obvious, think about it. 
\end{proof}

\setcounter{enumi}{2}

\item \textit{Show that $x^3 + x + 1$ is irreducible over $\mathbb{F}_2$ and let $\theta$ be a root. Compute the powers of $\theta$ in $\mathbb{F}_2(\theta)$. } 

\begin{proof}
Suppose $p(x) = x^3 + x + 1$ were reducible over $\F_2$. Then since it has degree $3$, we would have $(x - 1)|p(x)$ or $x|p(x)$. Since we have a nonzero constant term, we know the second of these two options does not hold. Also note $p(1) = 1^3 + 1 + 1 = 1 + 1 + 1 = 1 \neq 0$, so $(x - 1) \nmid p(x)$. Thus it must be irreducible over $\F_2$ since this field only has these two elements. 
\end{proof}

Now consider $\F_2(\theta) = \Set{a + b\theta + c\theta^2: a,b,c \in \F_2}$, since $\theta$ is a root of degree 3. Note $\theta^3 = -\theta - 1$. We have:
\bee
\theta^0 &= 1\\
\theta^1 &= \theta\\
\theta^2 &= \theta^2\\
\theta^3 &= -\theta - 1\\
\theta^4 &= \theta^2 - \theta\\
\theta^5 &= (-\theta - 1) - \theta^2\\
&= -\theta^2 - \theta - 1\\
\theta^6 &= \theta + 1 - \theta^2 - \theta\\
&= 1 - \theta^2\\
\theta^7 &= \theta - (-\theta - 1)\\
&= 1.
\eee


\bb


\item \textit{Prove that $a + b\sqrt{2} \mapsto a - b \sqrt{2}$ is an automorphism of $\Q(\sqrt{2})$. }

\begin{proof}
Note $\sqrt{2},-\sqrt{2}$ are conjugate, have same minimal polynomial $x^2 - 2$. We have $F(\alpha_1) \cong F(\alpha_2)$ by $\alpha_1 \leftrightarrow \alpha_2$. And we have an isomorphism $\Q(\sqrt{2}) \overset{\phi}{\to} \Q(-\sqrt{2})$ such that $\phi(a) = a$ $\forall a \in \Q$, and $\phi(\sqrt{2}) \mapsto -\sqrt{2}$.

\begin{Ex}
Consider $F(x) \to F(x)$ by $x \mapsto x^2$. Note homomorphisms of fields are always injective. 
\end{Ex}
\end{proof}


\item \textit{Read book for prompt. }

\begin{proof}
Use rational root theorem $\alpha \in \z$. 
\end{proof}
\end{enumerate}




\section{Algebraic extensions}



\begin{Def}\index{degree!of an extension}
If $K/F$ is an extension, then $K$ is an $F$-vector space. $\dim_FK$ is called the \textbf{degree of $K$ over $F$}, $\deg_FK = [K:F]$. It may be finite or infinite. 
\end{Def}

\begin{Def}\index{extension!finite}
If $\deg_FK < \infty$, then $K/F$ is a \textbf{finite extension}. 
\end{Def}

\begin{Def}\index{extension!infinite}
If $\deg_FK = \infty$, then $K/F$ is an \textbf{infinite extension}. 
\end{Def}

\begin{Ex}
Consider $[\c:\R] = 2$, and $[\R:\Q] =\infty$. 
\end{Ex}

\begin{theorem}
Let $K/F$ and $E/K$ be finite extensions. Then $E/F$ is an extension, and $E/F$ is finite if and only if both $K/F,E/F$ are finite. In this case, $[E:F] = [E:K][K:F]$. 
\end{theorem}


\begin{Cor}
If 
\begin{center}
\begin{tikzcd}
E \arrow[d, no head] \\
K \arrow[d, no head] \\
F
\end{tikzcd}
\end{center}
is a tower of extensions, then $[E:K],[K:F]|[E:F]$. 
\end{Cor}

\begin{proof}
$\forall \gamma \in E$, $\gamma = \delta_1 \beta_{j_1} + \cdots + \delta_k \beta_{j_k}$ for some $j_1,...,j_k \in J$ and $\delta_1,...,\delta_k \in K$. $\forall l$< $\delta_l = a_{l,1}\alpha_{i_1} + \cdots + a_{l,t}\alpha_{i_t}$ for some $i_1,...,i_t \in J$, $a_{l,1},...,a_{l,t} \in F$. So $\gamma = \sum_{i,j} c_{i,j}\alpha_i\beta_j$, $c_{i,j} \in F$. So $\Set{\alpha_i\beta_j}$ spans $E$ over $F$. If $\sum c_{ij} \alpha_i\beta_j = 0$, then $\sum_j\lpar \sum_ic_{ij}\alpha_i \rpar \beta_j = 0$, where the thing in the parentheses is in $K$. So $\forall j$, $\sum_i c_{ij}\alpha_i = 0$< so $\forall i$, $c_{ij} = 0$. 
\end{proof}






\begin{Def}\index{minimal polynomial of $\alpha$}
$f$ is the \textbf{minimal polynomial of $\alpha$}, notation $m_{\alpha,F}$. 
\end{Def}

\begin{rem}
$g(\alpha) = 0$ if and only if $f|g$. 
\end{rem}

\begin{Ex}
\begin{enumerate}
\item $F = \R$, $\alpha = i$. Them $m_{\alpha,\R} = x^2 + 1$. So $\c \cong \R[x]/(x^2 + 1)$. 
\item $F = \Q, \alpha = \sqrt{2}$, Then $m_{\alpha,\Q} = x^2 - 2$. And $\Q(\sqrt{2}) \cong \Q[x]/(x^2 - 2)$. 
\end{enumerate}
\end{Ex}








\bb
\textbf{Friday, March 9th}

\bb

We reiterate some of the stuff done yesterday. 

We have an extension $K/F$ where $\alpha \in K$, and we consider the simple extension $F(\alpha)/K$. 


\textbf{Case 1: }$\nexists f \in F[x]$ s.t. $f(\alpha) = 0$. Then $\alpha$ is called \textbf{transcendental} over $F$. Then we have $F(\alpha) \cong F(x)$ - field of rational functions. We have $f(x) \leftrightarrow f(\alpha)$. So different rational functions have different values at $\alpha$. And $[F(\alpha):F] = \infty$. But actually, there is the theory of transcendental extensions, but it is beyond the scope of this class. 

\textbf{Case 2: } $\exists f \in F[x]$ s.t. $f(\alpha) = 0$. Then we have $\phi:F[x] \to K$ given by $g(x) \mapsto g(\alpha)$ which has nonzero kernel. Then $ker\phi = (m_{\alpha,F})$, where $m_{\alpha,F}$ is an irreducible polynomial called the \textbf{minimal polynomial} of $\alpha$. And $f(\alpha) = 0$ if and only if $m_{\alpha,F}|f$. 

Now:
$$
F(\alpha) = F[\alpha] = \Set{a_0 + a_1\alpha + \cdots + a_{n - 1}\alpha^{n  -1}:a_i \in F}.
$$
And $n = deg(m_{\alpha,F})$. Now $\Set{1,\alpha,...,\alpha^{n - 1}}$ is a basis of $F(\alpha)$, and $[F(\alpha):F] = n = deg(m_{\alpha,F})$, is called \textbf{the degree of $\alpha$} over $F$, also denoted deg$_F\alpha$. 

\begin{Ex}
Let $\alpha = \sqrt{2 + \sqrt{2}}, F = \Q$. Then we have $\alpha^2 = 2 + \sqrt{2}$, $\alpha^2 - 2 = \sqrt{2}$, and $(\alpha^2 - 2)^2  - 2 = 0$. 

So $f(\alpha) = 0$ for $f(x) = x^4 - 4x^2 + 2$. This is irreducible by \textbf{Eisenstein's criterion.} So it's the minimal polynomial of $\alpha$. 

Or, if we know deg$f = 4 \overset{?}{=}[\Q(\alpha):\Q]$? 
\end{Ex}

\begin{Ex}
Consider $\alpha = s\sqrt{2} + \sqrt{3}$. Then we have:
\bee
\alpha^2 &= 5 + 2\sqrt{6}\\
(\alpha^2 - 5)^2 &= 24\\
\alpha^4 - 10 \alpha^2 + 1 &= 0\\
f(x) &= x^4 - 10x + 1.
\eee
Note this polynomial does not have any roots because any root must divide the last coefficient. So it has no rational roots, and thus it's irreducible over $\Q$. If it has a root over $\Q$, then it must be integer root  by Gauss lemma, and thus it must divide the last coefficient, but none of the factors of $-1$ are roots. 

Okay, so why is this the minimal polynomial? If we know that $[\Q(\alpha):\Q] = 4$, then $f$ is the minimal polynomial $m_{\alpha,\Q}$. Is this degree easy to compute? No, because... We consider multiplication by $\alpha$ as a linear transformation of some vector space. But we will not do this just yet, so don't worry about it. 
\end{Ex}

\begin{rem}
We have a homomorphism $\phi:F[x] \to K$ given by $f(x) \mapsto f(\alpha)$. We assume it is not injective. And then we have $ker\phi = (m)$. Then we have $F[x]/(m) \sub K$. But this ring is a principal ideal domain, any prime ideal is a maximal ideal, so $F[x]/(m)$ is a field, and it's just the field generated by $\alpha$, and the elements are just polynomials in $\alpha$ as seen above. 
\end{rem}

\begin{Ex}
Take $\alpha = \sqrt{2},F = \Q$. Then we have $\Set{a = b\sqrt{2}: a,b \in \Q}$. Note this ring is a field since we can divide elements. Note:
\bee
\fracc{1}{a + b\sqrt{2}} = \fracc{a}{a^2 - 2b^2} - \fracc{b}{a^2 - 2b^2}\sqrt{2}.
\eee
\end{Ex}

\begin{rem}
It's too early for this, but consider the following:
\begin{center}
\begin{tikzcd}
 & \Q(\sqrt{2},\sqrt{3}) \arrow[ld, "2", no head] \arrow[rd, "2", no head] &  \\
Q(\sqrt{2}) \arrow[rd, "2", no head] &  & \Q(\sqrt{3}) \arrow[ld, "2", no head] \\
 & \Q & 
\end{tikzcd}.
\end{center}
The labels are the degrees. 
\end{rem}

\begin{rem}
If $K/F$ is finite, then, since $F(\alpha)/F$ is a subextension of $K/F$, we have deg$_F\alpha = [F(\alpha):F]|[K:F]$. 
\end{rem}

\begin{Ex}
If $[K:F] = 6$, $\alpha \in K$, then deg$_F\alpha = 1,2,3,$ or $6$. And it is 1 only in the case when $\alpha \in F$, $m_\alpha = x-  a$, where $a = \alpha$ (according to Eric). 
\end{Ex}


\begin{lem}
Let $K/E/F$, where $\alpha \in K$ is algebraic over $F$. Then $\alpha$ is algebraic over $E$. And we have $f(\alpha) = 0$ for $f \in F[x] \sub E[x]$. Recall that algebraic means ``not transcendental", i.e. it satisfies some polynomial, maybe. Now we have $m_{\alpha,F} \overset{?}{\leftrightarrow} m_{\alpha,E}$. Since $m_{\alpha,F}(\alpha) = 0$ (over $E$, though, it may be reducible), and $m_{\alpha,F} \in E[x]$, we have $m_{\alpha,E}|m_{\alpha,F}$, and $\deg_E\alpha \leq \deg_F\alpha$. 
\end{lem}

\begin{Ex}
Let $\alpha = \sqrt[6]{2},F = \Q,E = \Q(\sqrt{2})$. Note:
\bee
m_{\alpha,F} &= x^6 - 2\\
m_{\alpha,E} &= x^3 - \sqrt{2}.
\eee
\end{Ex}

\begin{rem}
Take any finite extension. We have $F(\alpha)/F$ and let $K/F$ be finite. Take $\alpha \in K$. If $F(\alpha_1) \neq K$, take $\alpha_2 \in K\setminus F(\alpha_1)$. If $F(\alpha_1,\alpha_2) \neq K$, take $\alpha_3 \in K/F(\alpha_1,\alpha_2)$, and repeat. We get a tower of simple extensions:
\begin{center}
\begin{tikzcd}
F(\alpha_1,...,\alpha_n) = K \arrow[d, no head] \\
\vdots \arrow[d, no head] \\
F(\alpha_1,\alpha_2,\alpha_3) = F(\alpha_1,\alpha_2)(\alpha_3) \arrow[d, no head] \\
F(\alpha_1,\alpha_2) = F(\alpha_1)(\alpha_2) \\
F(\alpha_1) \arrow[u, no head] \\
F \arrow[u, no head]
\end{tikzcd}.
\end{center}

Then we have:
\bee
[K:F] &= [F(\alpha_1):F]\cdot [F(\alpha_1,\alpha_2):F(\alpha_1)] \cdots [F(\alpha_1,...,\alpha_n):F(\alpha_1,...,\alpha_{n - 1})]\\
&\leq \deg_F\alpha_1\cdot \deg_F\alpha_2\cdots\deg_F\alpha_n.
\eee
\end{rem}


$K/F$-extension, $K_1,K_2$ are subfields of $K$ containing $F$, or: $K_1/F,K_2/F$ are subextensions of $K/F$. Composite: $K_1K_2/F$. Is this finite? Who knows...
In fact when we deal with algebraic extensions, something is not needed because rational functions are not equal to polynomials. What this something is, I don't know, I missed it. 

\begin{Def}
Note $K_1K_2$ is the smallest field containing both. We take all linear combinations of both products and quotients. 

\end{Def}

\begin{Prop}
If $K_1/F,K_2/F$ are finite extensions, then $K_1K_2/F$ is also finite, and $[K_1K_2:F] \leq [K_1:F]\cdot [K_2:F]$. We have:
\begin{center}
\begin{tikzcd}
 & K_1K_2 \arrow[ld, "\leq m"', no head] \arrow[rd, "\leq n", no head] &  \\
K_1 \arrow[rd, "n"', no head] &  & K_2 \arrow[ld, "m", no head] \\
 & F & 
\end{tikzcd}.
\end{center}
And note $n,m|[K_1K_2:F] \leq nm$. 
\end{Prop}

\begin{proof}
Let $\Set{\alpha_1,...,\alpha_n}$ be a basis of $K_1$ over $F$, and $\Set{\beta_1,...,\beta_m}$ be a basis of $K_2$ over $F$. Then $K_1K_2 = F(\alpha_1,...,\alpha_n,\beta_1,...,\beta_m) = F[\alpha_1,...,\alpha_n,\beta_1,...,\beta_m]$, since these elements are algebraic. Note this notation does not denote span, it denotes the field generated by these elements. We have:
\bee
F(\alpha_1,...,\alpha_n,\beta_1,...,\beta_m) &= F(\alpha_1)(\alpha_2) \cdots(\alpha_n)\beta_1)\cdots(\beta_m)\\
&= F[\alpha_1][\alpha_2]\cdots [\beta_m].
\eee
Note they are algebraic because they belong to finite extensions, if you have a transcendental element, it must generate an infinite extension. 

Now any product of $\alpha_i$'s is their linear combination, and the same for $\beta_j$'s. So if you take a polynomial in both sets, it is just a linear combination. Formally, any element $\gamma \in K_1K_2$ is just a linear combination of $\alpha_i\beta_j$ where $i = 1,...,n,j = 1,...,m$. Thus $[K_1K_2:F] \leq nm$. 

Why is it less equal: Trivial example is $K_1 = K_2$. It's degree is not $n^2$, it's just $n$. 

So why is it not $n + m$. It is because elements in this space are \textit{products} of linear combinations, not sums. 
\end{proof}

\begin{Cor}
If $(n,m) = 1$, then $[K_1K_2:F] = nm$, where $n = [K_1:F],m = [K_2:F]$. 
\end{Cor}

\begin{Ex}
It may be that $K_1 \cap K_2 \neq \emptyset$. We have:
\begin{center}
\begin{tikzcd}
 & K_1K_2 \arrow[rd, "m_2", no head] \arrow[ld, "n_2"', no head] &  \\
K_1 \arrow[rd, "n_1", no head] \arrow[rdd, "n"', no head] &  & K_2 \arrow[ld, "m_1"', no head] \arrow[ldd, "m", no head] \\
 & K_1 \cap K_2 \arrow[d, "d", no head] &  \\
 & F & 
\end{tikzcd}.
\end{center}
Now let $d = [K_1\cap K_2:F]$. Then we have $n_1 = n/d,m_1 = m/d$, and we have $n_2 \leq m_1$, and $m_2 \leq n_1$. Then we have $K_1K_2  = n_2n \leq m_1n = \fracc{mn}{d}$. If they have nontrival intersection, then the degree of this extension will be strictly less than $mn$. 
\end{Ex}


%##873283#


\textbf{Monday, March 19th}

\begin{rem}
So what is the difference between $F(\alpha)$ and $F[\alpha]$? Note:
\bee
F(\alpha) = \Set{\fracc{p(\alpha)}{q(\alpha)}:p,q \in F[x]}.\\
F[\alpha] = \Set{p(\alpha):p \in F[x]} \cong F[x]/(m_{\alpha,F}). 
\eee
If $\alpha$ is algebraic over $F$, then $F(\alpha) \cong F[\alpha]$. 
\end{rem}

If $E/K$, $K/F$ are finite, then $E/F$ is finite, and we have $[E:F] = [E:K][K:F]$. Recall $[K:F] = \dim_FK$. We have:
\begin{center}
\begin{tikzcd}
E \arrow[d, "m", no head] \arrow[dd, "mn"', no head, bend right] \\
K \arrow[d, "n", no head] \\
F
\end{tikzcd}.
\end{center}

Note if $F\sub K_1,K_2 \sub K$, $K_1/F,K_2/F$ are finite, then the composite $K_1K_2/F$ is finite. Observe:
\begin{center}
\begin{tikzcd}
 & K_1K_2 \arrow[ld, "\leq m"', no head] \arrow[rd, "\leq n", no head] &  \\
K_1 \arrow[rd, "n"', no head] &  & K_2 \arrow[ld, "m", no head] \\
 & F & 
\end{tikzcd}.
\end{center}

\begin{rem}
We have $K_1K_2 = K_1(K_2) = K_1[K_2]$ where this last equality holds if they are finite. 
\end{rem}

\begin{Ex}
\begin{enumerate}
\item Observe:
\begin{center}
\begin{tikzcd}
 & \Q(\sqrt{2},\sqrt{3}) \arrow[ld, "2"', no head] \arrow[rd, "2", no head] &  \\
Q(\sqrt{2}) \arrow[rd, "2"', no head] &  & \Q(\sqrt{3}) \arrow[ld, "2", no head] \\
 & \Q & 
\end{tikzcd}.
\end{center}
Note $\Q(\sqrt{2},\sqrt{3}) = Q(\sqrt{2},\sqrt{3})$. And $\sqrt{3} \notin \Q(\sqrt{2})$, and $\sqrt{3}$ is a root of $x^2 - 3$. And $x^2 - 3 = m_{\sqrt{3},\Q(\sqrt{2}}$. 

Note $\Q(\sqrt{2}) = \Set{a + b\sqrt{2}: a,b\in \Q}$. 

\item Consider $\Q(\sqrt[4]{2}, \sqrt[6]{2})$. 
\begin{center}
\begin{tikzcd}
 & \Q(\sqrt[4]{2},\sqrt[6]{2}) \arrow[rd] \arrow[rd, "2", no head] \arrow[ld, "3"', no head] &  \\
\Q(\sqrt[4]{2}) \arrow[rd, "2"', no head] &  & \Q(\sqrt[6]{2}) \arrow[ld, "3", no head] \\
 & \Q(\sqrt{2}) \arrow[d, "2", no head] &  \\
 & \Q & 
\end{tikzcd}.
\end{center}
Note $x^6 - 2 = m_{\sqrt[6]{2},\Q}$. 
The minimal polynomial of $\sqrt[4]{2}$ over $\Q(\sqrt{2})$ is $m_{\sqrt[4]{2},\Q(\sqrt{2})} = x^2 - \sqrt{2}$. Now we have: 
$$
m_{\sqrt[6]{2},\Q(\sqrt[4]{2})}(x) = x^3 - (\sqrt[4]{2})^2 = x^3 - \sqrt{2}.
$$

Note:
$$
\fracc{2^{\fracc{1}{4}}}{2^{\fracc{1}{6}}} = 2^{\fracc{1}{4} - \fracc{1}{6}} = 2^{\fracc{1}{12}}.
$$

\begin{rem}
If the degree of the element is the same as the degree of the extension then the element generates the extension. 
\end{rem}

\begin{rem}
Note $\Q(\sqrt[6]{2}) = \Q(\sqrt[6]{2},\sqrt{2})$. 
\end{rem}

So we have $x^6 - 2  = (x^3 - \sqrt{2})(x^3 + \sqrt{2})$. And:
$$
x^3 - \sqrt{2} = m_{\sqrt[6]{2},\Q(\sqrt{2})} =  m_{\sqrt[6]{2},\Q(\sqrt[4]{2})}.
$$
The difference between this example and the last one is that they have a common subfield or something. 

\item Consider $\omega = e^{2\pi i/3} = -\fracc{1}{2} + \fracc{\sqrt{3}}{2}i$. So we have $\omega^3 = 1$, a primitive 3rd root of unity. 


We have $\Q(\sqrt[3]{2},\omega \sqrt[3]{2}) = \Q(\sqrt[3]{2},\omega)  = \Q(\sqrt[3]{2}, \sqrt{-3})$. And we have $\sqrt{-3} = \sqrt{3}i$ and:
\begin{center}
\begin{tikzcd}
 & \Q(\sqrt[3]{2},\omega) \arrow[rd] \arrow[rd, "2", no head] \arrow[ld, "2"', no head] &  \\
\Q(\sqrt[3]{2}) \arrow[rd, "3"', no head] &  & \Q(\omega\sqrt[3]{2}) \arrow[ld, "3", no head] \\
 & \Q & 
\end{tikzcd}.
\end{center}
And so the min. polynomial is $x^3 - 2$. Now their intersection is trivial, and they have no subextension...? When we adjoin one of these elements, the minimal polynomial of the other extension simplifies? When in the original field $\Q(\sqrt[3]{2},\omega \sqrt[3]{2})$ when we adjoin $\sqrt[3]{2}$ then the minimal polynomial of $\omega \sqrt[3]{2}$ is no longer $x^3 - 2$. Note it is $x^3 - 2$ over $\Q$. But over $\Q(\sqrt[3]{2})$, we know:
$$
x^3 - 2 = (x - \sqrt[3]{2})(x^2 + \sqrt[3]{2} + \sqrt[3]{4}).
$$ 
And this factor on the right is the min polynomial of $\omega \sqrt[3]{2}$ over $\Q(\sqrt[3]{2})$. Note the total degree in this case is 6, but it is not true that the degree of this divides $mn$, not true in general. Do we have some systematic method to determine the minimal polynomial? How do we know we can split as in the above example? There is a systematic way, there is an algorithm, it is a linear algebra problem. But it's too computational or something. Why do we use radicals? We use them because we have nice notation for them, we can write $\sqrt[3]{2}$ is the root of $x^3 - 2$. There is nothing special about these polynomials. They are just used for convenience. 

\item We give an example of a tower which is not a composite. Consider:
\begin{center}
\begin{tikzcd}
\Q(\sqrt[5]{\sqrt{2} + \sqrt[3]{3}}) \arrow[d, no head] \\
\Q(\sqrt{2} + \sqrt[3]{3}) \arrow[d, no head] \\
\Q
\end{tikzcd}.
\end{center}
Why is the middle element algebraic? This is because $\Q(\sqrt{2} + \sqrt[3]{3}) \sub \Q(\sqrt{2},\sqrt[3]{3})$, and we have:
\begin{center}
\begin{tikzcd}
 & \Q(\sqrt{2},\sqrt[3]{3}) \arrow[rd, no head] \arrow[ld, no head] &  \\
\Q(\sqrt{2}) \arrow[rd, no head] &  & \Q(\sqrt[3]{3}) \arrow[ld, no head] \\
 & \Q & 
\end{tikzcd}.
\end{center}

We have $x^5 - (\sqrt{2} + \sqrt[3]{3})$. 




\end{enumerate}
\end{Ex}

\begin{Def}\index{algebraic!extension}
An extension $K/F$ is \textbf{algebraic} if $\forall \alpha \in K$, $\alpha$ is algebraic over $F$. 
\end{Def}

\begin{rem}
If $K/F$ is finite, then it is algebraic (since $\forall \alpha \in K$, $F(\alpha)/F$ is finite. 
\end{rem}

\begin{rem}
The converse is not true, we may have algebraic but not finite field extensions. 
\end{rem}

\begin{lem}
If $K/F$ is algebraic and finitely generated, then it is finite. 
\end{lem}

\begin{proof}
Let $K = F(\alpha_1,...,\alpha_n)$ where each $\alpha_i$ is algebraic over $F$. Then $K$ is a tower of finite extensions:
$$
K = F(\alpha_1,...,\alpha_n)/F(\alpha_1,...,\alpha_{n  -1})/.../F(\alpha_1)/F.
$$
So it is finite. 

\begin{Prop}
A tower, or a composite, of algebraic extensions is algebraic. 
\end{Prop}
\begin{proof}
We know that this is true for finite extensions (why?) but algebraic are not the same as finite. But almost the same. Let $E/K,K/F$ be algebraic. Let $\alpha \in E$. We want to show that is algebraic over $F$ as well. Then $f(\alpha) = 0$ for some $f \in K[x]$. Then $\alpha$ satisfies some polynomial over $K$. Let $L = F(\beta_1,...,\beta_n)$ where $\beta_i$ are the coefficients of $f$. Note $f$ is nonzero. Then $L/F$ is finite, $\alpha$ is algebraic over $L$, $L(\alpha)/F$ is finite, so $L(\alpha)/F$ is finite, so $\alpha$ is algebraic over $F$. Note $L\sub K$. So $E$ is algebraic over $F$. 
\end{proof}
Let $K_1/F,K_2/F$ be algebraic. Want to prove $K_1K_2/F$ is algebraic. Let $\alpha \in K_1K_2$. Then:
$$
\alpha = \fracc{f(\beta_1,...,\beta_n,\gamma_1,...,\gamma_m)}{g(\beta_1,..,\beta_n,\gamma_1,...,\gamma_m)}, f,g \in F[x_1,...,x_{n + m}],\beta_i \in K_1,\gamma_i \in K_2.
$$
Thus $\alpha \in F(\beta_1,...,\beta_n,\gamma_1,...,\gamma_m)$ - finite extension of $F$. so $\alpha$ is algebraic over $F$, so $K_1K_2$ is algebraic over $F$. 
\end{proof}



\textbf{Tuesday, March 20th}

We review the big proof we did in class yesterday. We proved that if $E/K, K/F$ are algebraic, then $E/F$ is algebraic. And we also proved that if $K_1,K_2\sub K$, and $K_1/F,K_2/F$ are algebraic, then $K_1K_2/F$ is algebraic. 

\begin{rem}
If $\alpha,\beta \in K$ are algebraic over $F$, then $\alpha + \beta, \alpha\beta, \alpha/\beta$ are also algebraic over $F$. ($\alpha + \beta, \alpha\beta, \alpha/\beta \in F(\alpha,\beta) = F(\alpha)F(\beta)$)
\end{rem}

\begin{rem}
Let $K/F$ be an extension. Let:
$$
E = \Set{\alpha \in K: \alpha \text{ is algebraic over }F}.
$$
Then $E$ is a field, $E/F$ is algebraic (the maximal algebraic subextension of $K/F$), and $|forall \alpha \in K\setminus E$, $\alpha$ is transcendental over $E$. Note that it is transcendental over $F$ by definition. Every element that is not in $E$ is transcendental over $E$ since if $\alpha$ is transcendental over $E$, then $E(\alpha)/E/F$ is a tower of algebraic extensions, so $\alpha$ is algebraic over $F$, so $\alpha \in E$. Note of course every element of $F$ is of course algebraic over $F$, since we can write $x - \alpha$. Elements of $F$ are exactly algebraic elements of degree $1$ since we can use a linear polynomial. 
\end{rem}

Note we cannot factorize fields. But we can say that $K/E$ is transcendental in the sense that all elements are transcendental. It has no algebraic torsions.

So, $\sqrt{2} + \sqrt[3]{3}, \sqrt[5]{\sqrt{2} + \sqrt[3]{3}} + 2\sqrt[4]{5}$ is algebraic over $\Q$ since anything we can construct with algebraic elements is algebraic. 


Note if $\alpha$ is a root of $x^3 + \beta x^2 + \gamma x + 2$, where $\beta$ is a root of $x^5 - 3x^2 + 1$, and $\gamma$ is a root of $x^2 + x + 1$, then $\alpha$ is algebraic over $\Q$. If you use algebraic elements of the field as coefficients of your polynomial, then the roots of the polynomial are also algebraic. 

\begin{rem}
Algebraic elements generate extensions of finite degree. 
\end{rem}

How to find the minimal polynomial? First we need some finite extension where this element lives, say $\sqrt{2} + \sqrt[3]{3} \in \Q(\sqrt{2},\sqrt[3]{3})$, which is a 6-dimensional vector space over $\Q$, with basis $\Set{1,\sqrt{2},\sqrt[3]{3}, \sqrt{2}\sqrt[3]{3}, \sqrt[3]{9}, \sqrt{2}\sqrt[3]{9}}$. 
We have:
\begin{center}
\begin{tikzcd}
 & \Q(\sqrt{2},\sqrt[3]{3}) \arrow[ld, "3"', no head] \arrow[rd, "2", no head] &  \\
Q(\sqrt{2}) \arrow[rd, "2"', no head] &  & \Q(\sqrt[3]{3}) \arrow[ld, "3", no head] \\
 & \Q & 
\end{tikzcd}.
\end{center}

Let $\alpha$ be algebraic, $\alpha \in K$, $K/F$ finite. Then $\alpha$ acts on $K$ by multiplication, $\beta \mapsto \alpha\beta$, and this is a linear transformation of $K$ as an $F$-vector space. 

The whole space is a direct sum of minimal $\alpha$-invariant cyclic subspaces. 

So note $F(\alpha)$ is $\alpha$-invariant. And $\forall \beta \in K$, $\beta F(\alpha)$ is $\alpha$-invariant. If $\Set{1,\beta_1,...,\beta_k}$ is a basis of $K$ over $F(\alpha)$, then:
$$
K = F(\alpha) \oplus \beta_1F(\alpha) \oplus ... \oplus \beta_k F(\alpha).
$$
This is the decomposition of $K$ into $\alpha$-invariant subspaces. And each of these subspaces is cyclic. 
Note the basis of $K/F$ is $\Set{1,\alpha,...,\alpha^{n - 1},\beta_1,\beta_1\alpha,...,\beta_1\alpha^{n - 1},...}$. 

\begin{Def}
And $F(\alpha)$ is $\alpha$-cyclic: the basis in $F(\alpha)$ is $1,\alpha,...,\alpha^{n - 1}$ where $N = \deg_F\alpha$. 
\end{Def}

\begin{Def}
Recall that a vector space $V$ is $T$-cyclic if $\exists u \in V$ s.t. $\Set{u,Tu,...,T^{n - 1}u}$ is a basis in $V$. 
\end{Def}

Also, $\beta_iF(\alpha)$ is $\alpha$-cyclic for any $i$. And so this is the decomposition of $K$ into cyclic, $\alpha$-invariant subspaces. If we consider $\alpha$ as a linear transformation, then the minimal polynomial is simply the minimal polynomial of $\alpha$. 

\begin{lem}
The minimal polynomial is the minimal polynomial of the linear transformation $T(\beta) = \alpha\beta$. 
\end{lem}

\begin{proof}
Note $m_{\alpha,F}$ is the minimal polynomial of $\alpha$ over $F$. Then $m_{\alpha,F}(\alpha)\beta = 0$ $\forall \beta$, so $T$ satisfies $m_{\alpha,F}(T) = 0$. And $\forall f \in F[x]$ with degree $< n$, we know $f(T) \neq 0$, since $f(\alpha) \neq 0$. So $f(\alpha) = f(T)1 \neq 0$. So $m_{\alpha,F}$ is the minimal polynomial of $T$. 

Simpler proof:
\bee
f(T)\cdot 1 &= f(\alpha)\\
f(T)\beta &= f(\alpha)\beta, \forall \beta\\
\Rightarrow f(T) = 0 &\Leftrightarrow f(\alpha) = 0.
\eee
\end{proof}

\begin{rem}
The rational canonical form of $T$ is:
$$
\newcommand*{\boxone}{\multicolumn{1}{c|}{A_1}}
\newcommand*{\boxtwo}{\multicolumn{1}{|c|}{A_2}}
\newcommand*{\boxk}{\multicolumn{1}{|c}{A_k}}
\newcommand*{\boxdots}{\multicolumn{1}{|c|}{\ddots}}
\newcommand\bigzero{\makebox(0,0){\text{\huge0}}}
A = \lpar
\begin{matrix}
\boxone &  &  & \text{\huge0}\\
\cline{1-2}
 & \boxtwo &  & \\
 \cline{2-2}
 &  &  \ddots & \\
  \cline{4-4}
\text{\huge0} &  & & \boxk
\end{matrix} \rpar,
$$
where all blocks are identical, they are equal to the companion matrix of $m_{\alpha,F}$. 
\end{rem}

\begin{rem}
The characteristic polynomial of $T$ is $m_{\alpha,F}^{k + 1}$ where $k + 1 = [K:F(\alpha)]$, so $K = F(\alpha)$ if and only if the characteristic polynomial is equal to the minimal polynomial.
\end{rem}


\begin{Ex}
$\alpha = \sqrt{2} + \sqrt{3}$. We have $\alpha^2 = 5 + 2\sqrt{6}$ and $(\alpha^2 - 5)^2 = 24$. Then we have $\alpha^4 - 10\alpha^2 + 1 = 0$. So is $x^4 - 10x^2 + 1$ the min poly?


Consider $V = \Q(\sqrt{2},\sqrt{3})$ as a $\Q$-vector space. Basis is $\Set{1 \sqrt{2},\sqrt{3},\sqrt{6}}$. Define $T(\beta) = \alpha\beta$, $T:V \to V$. We have:
\begin{center}
\begin{tikzcd}
 & \Q(\sqrt{2},\sqrt{3}) \arrow[ld, "2"', no head] \arrow[rd, "2", no head] &  \\
Q(\sqrt{2}) \arrow[rd, "2"', no head] &  & \Q(\sqrt{3}) \arrow[ld, "2", no head] \\
 & \Q & 
\end{tikzcd}.
\end{center}
The basis of $\Q(\sqrt{2})$ is $\Set{1,\sqrt{2}}$ and basis of $\Q(\sqrt{3})$ is $\Set{1,\sqrt{3}}$. Note all elements of $\Q(\sqrt{2})$ are of the form $a + b\sqrt{2} \overset{?}{=} \sqrt{3}$. But then we would have $(a + b\sqrt{2})^2 = 3$ but this has no rational solution. Note the product of the basis elements are always a spanning set. And now we map:
\bee
1 &\mapsto \sqrt{2} + \sqrt{3}\\
\sqrt{2} &\mapsto 2 + \sqrt{6}\\
\sqrt{3} &\mapsto \sqrt{6} + 3\\
\sqrt{6} &\mapsto 2\sqrt{3} + 3 \sqrt{2}.
\eee

Thus the matrix of $T$ is:
$$
\lpar 
\begin{matrix}
0 & 2 & 3 & 0\\
1 & 0 & 0 & 3\\
1 & 0 & 0 & 2\\
0 & 1 & 1 & 0
\end{matrix} \rpar .
$$

We're doing something, I don't know what it is. We have $1,\alpha, \alpha^2, \alpha^3$. We have: \bee
\alpha &= \sqrt{2} + \sqrt{3}\\
\alpha^2 &= 5+ 2\sqrt{6}\\
\alpha^3 &= 5\sqrt{2} + 5 \sqrt{3} + 6\sqrt{2} + 4\sqrt{3}\\
&= 11\sqrt{2} + 9\sqrt{3}.
\eee
So we have:
$$
\lpar 
\begin{matrix}
1 & 0 & 5 & 0\\
0 & 1 & 0 & 11\\
0 & 1 & 0 & 9\\
0 & 0 & 2 & 0
\end{matrix} \rpar .
$$
And the rows are linearly independent. So $V$ is $\alpha$-cyclic, so $\deg_{\Q}\alpha = 4$. 
\end{Ex}

Recall we had this direct summand form and we proved that minimal polynomial equivalence using that. 

Pronunciation guide:
\texttt{vee-ECK-tor}.

Consider $\beta = \sqrt{6}$ and then $\Q(\beta) \neq V$ because something is $2$-dimensional, dunno why or what. The matrix has two blocks or something. The RCF is:
$$
\lpar 
\begin{tabular}{cc|cc}
0 & 6 & 0 & 0\\
1 & 0 & 0 & 0\\
\hline
0 & 0 & 0 & 6\\
0 & 0 & 1 & 0
\end{tabular}
\rpar.
$$

\begin{Def}{conjugate!elements}
Let $K/F$ be an algebraic extension, and let $\alpha_1,\alpha_2 \in K$, and let $m_{\alpha_1,F} = m_{\alpha_2,F}$. $\alpha_1,\alpha_2$ are called \textbf{conjugate}. 
\end{Def}

\begin{rem}
Then $F(\alpha_1) \cong F[x]/(f) \cong F(\alpha_2)$. 
\end{rem}

\begin{Ex}

\begin{enumerate}
\item $i,-i$ are conjugate over $\R$ (and $\Q$). Minimal polynomial is $x^2 + 1$. 
$\pm \sqrt{a}$ are conjugate. 
\item $\sqrt[3]{3},\sqrt[3]{3}\omega,\sqrt[3]{3}\omega^2$ are conjugate over $\Q$ where $\omega = e^{2\pi i/3}$. 
\end{enumerate}
\end{Ex}



\textbf{Wednesday, March 21st}

\bb
\textbf{Methods of finding $m_{\alpha,F}$}\index{Methods of finding $m_{\alpha,F}$}
\bb
We discuss several methods to find the minimal polynomial. We have at least five. Let $\alpha = \sqrt{2} + \sqrt{3}$. 
\begin{enumerate}
\item Find $f$ such that $f(\alpha) = 0$ and prove that $f$ is irreducible. 
\item Find $f$ s.t. $f(\alpha) = 0$ and prove that $\deg f = \deg_F\alpha$. How do we find this? Either guess, or use $\deg_F\alpha = \dim span\Set{1,\alpha,\alpha^2,...}$. 
\item Find the matrix of $Tu = \alpha u$ in some basis, find the Smith form, and find the minimal polynomial of $T$. 
\item Find the matrix of $T$, find the characteristic polynomial $c_T$, and there is a good chance that this is the minimal polynomial, and if not, we proved yesterday that it is a power of the minimal polynomial. Represent $c_T = f^*$ where $f$ is irreducible, then $f = m_{\alpha,F}$. If $c_T$ is irreducible, then it is also $m_T$. 
\item This is the easiest way: find powers of $\alpha$ in some basis, and find the minimal linear relations between them. 
\end{enumerate}

\begin{Ex}
We apply method 5 to $\alpha$. $K = \Q(\sqrt{2},\sqrt{3})$, and basis is $\Set{1, \sqrt{2}, \sqrt{3}, \sqrt{6}}$. We have:
\bee
1 = \lpar 
\begin{matrix}
1\\0\\0\\0
\end{matrix} \rpar , \alpha = \lpar 
\begin{matrix}
0\\
1\\
1\\
0
\end{matrix} \rpar ,
\alpha^2 = \lpar 
\begin{matrix}
5\\
0\\
0\\
2
\end{matrix} \rpar ,
\alpha^3 = \lpar 
\begin{matrix}
0\\
11\\
9\\
0
\end{matrix} \rpar ,
\alpha^4 = \lpar 
\begin{matrix}
49\\
0\\
0\\
20
\end{matrix} \rpar .
\eee

We have:
$$
\alpha^2 = (\sqrt{2} + \sqrt{3})(\sqrt{2} + \sqrt{3}) = 5 = 2\sqrt{6}.
$$

Solve the equation $\alpha^4 = x_1 + x_2\alpha + x_3\alpha^2 + x_4\alpha^3$. We have:
$$
\begin{cases}
x_1 + 5x_3 = 49\\
x_2 + 11x_4 = 0\\
x_2 + 9x_4 = 0\\
2x_3 = 20
\end{cases}.
$$
So we have $x_2 = x_4 = 0$, $x_3 = 10,x_1 = -1$. Then we have $m_{\alpha,F} = x^4 - 10x^2 - 1$. 

\end{Ex}

\begin{rem}
Something something look at this:
$$
K = F(\alpha) \oplus F(\alpha)\beta_1 \oplus \cdots \oplus F(\alpha)\beta_k.
$$
And all the companion matrices are identical. 
\end{rem}


\begin{Ex}
let $K = \Q(\sqrt{2},\sqrt{3})$, and let $\beta = \sqrt{6}$. Matrix of $\beta$ is:
$$
\lpar 
\begin{matrix}
0 & 0 & 0 & 6\\
0 & 0 & 3 & 0\\
0 & 2 & 0 & 0\\
1 & 0 & 0 & 0
\end{matrix} \rpar.
$$
We have:
\bee
1 &\mapsto \sqrt{6}\\
\sqrt{2} & \mapsto 2\sqrt{3}\\
\sqrt{3} & \mapsto 3 \sqrt{2}\\
\sqrt{6} &\mapsto 6.
\eee
The characteristic polynomial is $(x^2 - 6)^2$. And $m_{\beta,\Q} = x^2 - 6$. Smith form is:
$$
\lpar 
\begin{matrix}
1 & & &\\
& 1\\
& & x^2 - 6\\
&&&x^2 - 6
\end{matrix} \rpar.
$$
\end{Ex}

Now we continue with theory. 

\begin{Def}
$\alpha_1,\alpha_2 \in K$ are \textbf{conjugate} over $F \sub K$ if $m_{\alpha_1,F} = m_{\alpha_2,F}$. In this case, $F(\alpha_1) \cong F(\alpha_2)$ under the isomorphism $\phi$ s.t. $\phi|_F = Id_F$, $\phi(\alpha_1) = \phi(\alpha_2)$. Note:
\bee
F(\alpha_1)& \cong F[x]/(f) \cong F(\alpha_2)\\
\alpha_1 &\leftrightarrow x \mod f \leftrightarrow \alpha_2.
\eee
\end{Def}

\begin{rem}
Any $\alpha$ has at most $\deg_f\alpha = \deg m_{\alpha,F}$ conjugates, counting itself. 
\end{rem}

\begin{rem}
Let $f \in F[x]$ be irreducible. If $K/F$ is an extension, $\alpha \in K$, $f(\alpha) = 0$, then $f = m_{\alpha,F}$, since it is irreducible, and $m_{\alpha,F}|f$. 
\end{rem}

If $K_1/F,K_2/F$, $\alpha_1 \in K_1,\alpha_2 \in K_2$, and $f(\alpha_1) = f(\alpha_2) = 0$, then still $F(\alpha_1) \cong F(\alpha_2)$ under isomorphism that is identical on $F$ and that maps $\alpha_1 \to \alpha_2$ for the same reason as above. We have:
\begin{center}
\begin{tikzcd}
\alpha_1 \arrow[rr] &  & \alpha_2 \arrow[ll] \\
F(\alpha_1) \arrow[rr, "\cong"] &  & F(\alpha_2) \\
 & F \arrow[lu] \arrow[ru] & 
\end{tikzcd}.
\end{center}

\begin{Def}
If $f \in F[x]$ is irreducible, $\alpha$ is a root of $f$ in some extension $K$ of $F$, then $F(\alpha)$ is \textbf{obtained by adjoining a root of $f$. }
\end{Def}

\begin{rem}
Such an extension always exists, it is $K = F[x]/(f)$. (Then $K = F(\alpha)$. 
\end{rem}

If we start with an abstract field, how do we know there exists a larger field which contains a root of an irreducible polynomial?

In $K$, $f(\bar{x}) = \bar{f(x)} = 0$ where $\bar{x} = x \mod f$. 


\begin{Ex}
\begin{enumerate}

\item 
$F = \R, f = x^2 + 1$. Define $K = \R[x]/(x^2 + 1)$. Put $\alpha = \bar{x} \in K$. Then:
$$\alpha^2 + 1 = \bar{x}^2 + 1  = \bar{x^2 + 1} = 0.
$$
Then $K = F(\alpha)$. 
\item $F = \Q$, $f = x^3 - 2$, put $K = \R[x]/(x^3 - 2)$. Then $\alpha = \bar{x} \in K$. 

\end{enumerate}
\end{Ex}

If $f$ is reducible, decompose it into irreducible components: $f = f_1\cdots f_k$. Then adjoin a root $\alpha$ of $f_1$, then $f(\alpha) = 0$. But now the result depends on our choice of irreducible. They will be different fields. 

\begin{rem}
So the operation of ``adjoining a root of $f$" is not a well-defined operation. 
\end{rem}

\begin{lem}
Let $\phi:F_1 \to F_2$ be an isomorphism, let $f_1 \in F_1[x]$, $f_2 \in F_2[x]$ be such that $\phi(f_1) = f_2$. $\phi$ applies to coefficients of $f_1$ and transforms into $f_2$. Also assume that they are irreducible. Let $\alpha_1$ be a root of $f_1$ and $\alpha_2$ be a root of $f_2$, in some larger fields, not in this field, since they are irreducibe $\Rightarrow$ they don't have roots in \textit{this} field. Then $\phi$ extends to an isomorphism $F_1(\alpha_1) \overset{\cong}{\to} F_2(\alpha_2)$ such that $\phi(\alpha_1) = \alpha_2$. And:
\begin{center}
\begin{tikzcd}
F_1(\alpha_1) \arrow[r, "\phi"] \arrow[d] & F_2(\alpha_2) \arrow[d] \\
F_1 \arrow[r, "\phi"] \arrow[u] & F_2 \arrow[u]
\end{tikzcd}
\end{center}
is commutative. 
\end{lem}

\begin{proof}
Note $F_1[x] \overset{\phi}{\to} F_2[x]$ is given by $\phi((f_1)) = (f_2)$. So:

$$
F_1(\alpha_1) \cong F_1[x]/(f_1) \overset{\cong}{\to} F_2[x]/(f_2) \cong F_2(\alpha_2),
$$ 

and this is given by $\alpha_1 \leftrightarrow \bar{x} \leftrightarrow \alpha_2$. 
\end{proof}

\section*{13.2 Exercises}






\begin{enumerate}[label=\arabic*.]

\setcounter{enumi}{2}

\item \textit{Find the minimal polynomial of $\alpha = 1 + i$ over $\Q$. }

Note $\alpha^2 = 1 - 1 + 2i = 2i$. And $\alpha^4 = -4$, so $\alpha^4 + 4 = 0$. So $m_\alpha = x^4 + 4$? I don't think so, because $\alpha$ belongs to $\Q(i)$, which has degree 2, so $\alpha$ has degree at most 2. So that is not the minimal polynomial, it must be reducible. Try again. Write $(\alpha - 1)^2 = i^2 = -1$, so $\alpha^2 - 2\alpha + 1 = -1$, so $\alpha^2 - 2\alpha + 2 = 0$. So $m_\alpha = x^2 - 2x + 2$. We can see it's irreducible since it has no roots in $\Q$. 


\textbf{Desmond: }$\Q$ is in general, kind of a comfortable field. If we're in a less familiar field, is there a systematic way to find the minimal polynomial?

\textbf{Leibman: }There are algorithms, you must be able to make computations. There are some algorithms, I am not sure. 
 
\item \textit{Find $\deg_\Q \alpha$ for $\alpha_1 = 2 + \sqrt{3}$ and $\alpha_2 = 1 + \sqrt[3]{2} + \sqrt[3]{4}$.  }

Note $\alpha_1 \in \Q(\sqrt{3})$ - deg 2, so $\deg_\Q \alpha_1 = 2$. Again the degree of the root must divide the degree of the extension. $\alpha_2 \in \Q(\sqrt[3]{2})$ - degree 3, so $\deg_\Q\alpha_2 = 3$. 

\begin{rem}
The degree of the root divides the degree of the extension. 
\end{rem}

\item \textit{$F = \Q(i)$. Prove that $x^3 - 2$ is irreducible over $F$. }

\begin{proof}
Find the roots and see that none of them are in $F$. It is easy. There is a simpler argument based on the degree. The root of the polynomial must be of degree 3, but the extension has degree 2, so the element cannot belong to $F = \Q(i)$. 
\end{proof}






\setcounter{enumi}{5}
\item \textit{Prove that $F(\alpha_1,...,\alpha_n) = F(\alpha_1)...F(\alpha_n)$. }

\begin{proof}
This should be easy, and obvious, if a field contains blank and all blank then it must contain blank$'$.
\end{proof}


\setcounter{enumi}{7}

\item \textit{Let $F$ be a field of characteristic $\neq 2$. Let $D_1$ and $D_2$ be elements of $F$, neither of which is a square in $F$. Prove that $F(\sqrt{D_1},\sqrt{D_2})$ is of degree $4$ over $F$ if $D_1D_2$ is not a square in $F$ and is of degree $2$ over $F$ otherwise. When $F(\sqrt{D_1},\sqrt{D_2})$ is of degree $4$ over $F$, the field is called a \textbf{biquadratic extension of $F$}. }

\begin{proof}
Assume $D_1D_2$ is not a square. Suppose $\sqrt{D_1},\sqrt{D_2}$ are linearly dependent. Then we have $\sqrt{D_1} = \alpha\sqrt{ D_2} + \beta$ for some $\alpha,\beta \in F$. Suppose for contradiction that $\beta = 0$. Then we have:
\bee
\sqrt{D_1} &= \alpha \sqrt{D_2}\\
D_1 &= \alpha^2D_2\\
D_1D_2 &= \alpha^2D_2^2.
\eee
But we said $D_1D_2$ is not a square, so we have a contradiction, so we must have that $\beta \neq 0$. And $\alpha \neq 0$ since otherwise $\sqrt{D_1} \in F \Rightarrow D_1$ is a square in $F$. 

Then: 
$$
D_1 = (\alpha\sqrt{ D_2} + \beta)^2 = \alpha^2D_2 + 2\alpha\beta\sqrt{D_2} + \beta^2,
$$
and since we are over a field of characteristic $\neq 2$, we know that $2\neq 0 \Rightarrow 2\alpha\beta \neq 0$, so we must have that $\sqrt{D_2} \in F$ which means that $D_2$ is a square in $F$, contradiction, so $\sqrt{D_1},\sqrt{D_2}$ must be linearly independent over $F$. Thus $m_{\sqrt{D_1},F(\sqrt{D_2}} = x^2 - D_1$, and so the degree of $F(\sqrt{D_1},\sqrt{D_2})$ over $F(\sqrt{D_2})$ is $2$. Since $D_2$ is not a square in $F$, we know $m_{\sqrt{D_2},F} = x^2 - D_2$, which as degree $2$, so $F(\sqrt{D_2})$ has degree 2 over $F$, and note these are both finite extensions. Recall that if $E/K$, $K/F$ are finite, then $E/F$ is finite, and we have $[E:F] = [E:K][K:F]$. So $[F(\sqrt{D_1},\sqrt{D_2}):F] = 4$. 


If $D_1D_2$ is a square, we would have $\sqrt{D_1}\sqrt{D_2} = a$ for some integer $a$. Thus $\sqrt{D_1} = \fracc{a}{\sqrt{D_2}}$, and so $F(\sqrt{D_1},\sqrt{D_2}) = F(\sqrt{D_2})$. Then we showed $F(\sqrt{D_2})$ has degree 2 over $F$, so $[F(\sqrt{D_1},\sqrt{D_2}):F] = 2$. 
\end{proof}

\item \textit{Let $F$ be a field of characteristic $\neq 2$. Let $a,b$ be elements of the field $F$ with $b$ not a square in $F$. Prove that a necessary and sufficient condition for $\sqrt{a + \sqrt{b}} = \sqrt{m} + \sqrt{n}$ for some $m$ and $n$ in $F$ is that $a^2 - b$ is a square in $F$. Use this to determine when the field $\Q(\sqrt{a + \sqrt{b}})(a,b \in \Q)$ is biquadratic over $\Q$. }

\begin{proof}
Let $a^2 - b$ be a square in $F$. Then:
\bee
\lpar \sqrt{a + \sqrt{b}} \rpar^2 \lpar \sqrt{a - \sqrt{b}} \rpar^2 = (a + \sqrt{b})(a - \sqrt{b}) = a^2 - b = c^2,
\eee
for some $c \in F$. Then we have $\sqrt{a^2 - b} \in F$. Define $m = \fracc{a + \sqrt{a^2 - b}}{2}$ and $n = \fracc{a - \sqrt{a^2 - b}}{2}$, which are well defined since we said the characteristic of our field is not 2. Then we have:
\bee
m &= \fracc{2a + 2\sqrt{a^2 - b}}{4}\\
&= \fracc{(a + \sqrt{b}) + 2\sqrt{a^2 - b} + (a - \sqrt{b})}{4}\\
&= \lpar \fracc{\sqrt{a + \sqrt{b}} + \sqrt{a - \sqrt{b}}}{2} \rpar^2\\
\Rightarrow \sqrt{m} &= \fracc{\sqrt{a + \sqrt{b}} + \sqrt{a - \sqrt{b}}}{2}.
\eee
Similarly, we have:
\bee
\sqrt{n} &=  \fracc{\sqrt{a + \sqrt{b}} - \sqrt{a - \sqrt{b}}}{2}
\eee
Thus:
\bee
\sqrt{m} + \sqrt{n} &= \fracc{\sqrt{a + \sqrt{b}} + \sqrt{a - \sqrt{b}}}{2} + \fracc{\sqrt{a + \sqrt{b}} - \sqrt{a - \sqrt{b}}}{2} = \sqrt{a + \sqrt{b}}.
\eee
So we have shown the claim holds in the first direction.



Assume we have the following:
\bee
\sqrt{a + \sqrt{b}} &= \sqrt{m} + \sqrt{n}\\
a + \sqrt{b} &= m + n + 2\sqrt{mn}.
\eee
Now we claim we must have $a = m + n$ and $b = 4mn$. Suppose $\sqrt{b} = c + 2\sqrt{mn}$ for some $c \in F$. Then $b = c^2 + 4c\sqrt{mn} + 4mn$. Since char$F \neq 2$, and $b \in F$, we know we must have either $\sqrt{mn} \in F$, or $c = 0$. If $\sqrt{mn} \in F$, then $\sqrt{b} \in F$, which means $b$ is a square, contradiction. So we must have $c = 0$, thus the claim holds. Then we have:
\bee
a^2 - b &= (a + \sqrt{b})(a - \sqrt{b})\\
&= (m + n + 2\sqrt{mn})(m + n - 2\sqrt{mn})\\
&= m^2 + mn - 2m\sqrt{mn} + mn + n^2 - 2n\sqrt{mn} + 2m\sqrt{mn} + 2n\sqrt{mn} - 4mn\\
&= m^2 + n^2 - 2mn\\
&= (m - n)^2.
\eee
Thus if $a^2 - b$ is a square, then we have:
\bee
\Q(\sqrt{a + \sqrt{b}}) &= \Q(\sqrt{m} + \sqrt{n}) = \Q(\sqrt{m},\sqrt{n}).
\eee
Clearly the degree is either 2 or 4, but if it is 2, then we would have $m = n$ which would give us $b$ is a square, contradiction, so the degree is 4. So $\Q(\sqrt{a + \sqrt{b}})$ is biquadratic. 
\end{proof}

\setcounter{enumi}{9}
\item \textit{Find the degree of the extension. $K = \Q(\sqrt{3 + 2 \sqrt{2}})$. What is $[K:\Q]$?}

We have:
\begin{center}
\begin{tikzcd}
\Q(\sqrt{3 + 2 \sqrt{2}}) \arrow[d, "2", no head] \\
\Q(\sqrt{2}) \arrow[d, "2", no head] \\
\Q
\end{tikzcd}.
\end{center}
Note it is tempting to say that the degree is 4, but this is false since $\alpha = (1 + \sqrt{2})^2 = 3 + 2\sqrt{2}$. So $\sqrt{\alpha} = 1  + \sqrt{2} \in \Q(\sqrt{2})$. So when we adjoin the root of $\alpha$ we do nothing, it is already in $\Q(\sqrt{2}) = \Q(\sqrt{3 + 2 \sqrt{2}})$. 

\setcounter{enumi}{11}
\item \textit{$K/F$, $[K:F] = p$ - prime. Then there exist no proper nontrivial subextensions: 
$K/E/F \Rightarrow E = K$ or $E = F$. }

\begin{proof}
$[E:F]|[K:F]$, so $= 1$ or $p$. 
\end{proof}

\item \textit{Suppose $F = \Q(\alpha_1,\alpha_2,...,\alpha_n)$ where $\alpha_i^2 \in \Q$ for $i = 1,2,...,n$. Prove that $\sqrt[3]{2} \notin F$. }

\begin{proof}
Since these roots are all quadratic, we know that the degree of $\Q(\alpha_i)$ over $\Q(\alpha_1,...,\alpha_{i - 1})$ is at most 2, and if $\alpha_i$ is generated by $\alpha_1,...,\alpha_{i - 1}$ then the degree is 1. Thus these are all finite extensions, and then by induction, we know that $F/\Q$ has finite degree, and it's degree is the product of all the extensions $\Q(\alpha_i)/\Q(\alpha_1,...,\alpha_{i - 1})$. Since these are all $1$ or $2$, we know $[F:\Q] = 2^k$ for some integer positive integer $k$ (positive since the first extension has degree $2$ over $\Q$). But $[\Q(\sqrt[3]{2}):\Q] = 3$, and if $\sqrt[3]{2} \in F$, then we would have $[\Q(\sqrt[3]{2}):\Q]|2^k$, which is not the case. Thus $\sqrt[3]{2} \notin F$. 
\end{proof}

\setcounter{enumi}{13}

\item \textit{Prove that $[F(\alpha):F]$ is odd $\Rightarrow F(\alpha) = F(\alpha^2)$. }

\begin{proof}
We have:
\begin{center}
\begin{tikzcd}
F(\alpha) \arrow[d, "1\text{ or } 2", no head] \arrow[dd, "odd"', no head, bend right] \\
F(\alpha^2) \arrow[d, no head] \\
F
\end{tikzcd}.
\end{center}
In the middle thing, $\alpha$ is a root of $x^2 = \alpha^2$. $2 \nmid [F(\alpha):F]$, so $[F(\alpha):F(\alpha^2)] = 1$. 
\end{proof}

\setcounter{enumi}{15}

\item \textit{Let $K/F$ be an algebraic extension and let $R$ be a ring contained in $K$ and containing $F$. Show that $R$ is a subfield of $K$ containing $F$. }

\begin{proof}
Since $K/F$ is algebraic, we know that $\forall \alpha \in K$, $\alpha$ is algebraic over $F$. So $\alpha$ is the root of some polynomial $p(x) \in F[x]$. So let $r \in R$, nonzero, we wish to construct an inverse $r^{-1}$ for $r$. Then we have:
\bee
p(r) &= a_nr^n + \cdots + a_1r + a_0 = 0\\
a_0 &= -a_nr^n - \cdots - a_2r^2 - a_1r\\
1 &= -\fracc{a_n}{a_0} r^n - \cdots - \fracc{a_2}{a_0}r^2 - \fracc{a_1}{a_0}r\\
\fracc{1}{r} &= -\fracc{a_n}{a_0} r^{n- 1} - \cdots - \fracc{a_2}{a_0}r - \fracc{a_1}{a_0}. 
\eee
This is well defined since $r$ is nonzero. Thus we have found $r^{-1}$, and it is an element of $r$ since $a_i \in F \sub R$, and since we have additive and multiplicative closure in $R$. Thus we have inverses in $R$ and it is a field. 
\end{proof}

\setcounter{enumi}{19}

\item \textit{Show that if the matrix of the linear transformation ``multiplication by $\alpha$" considered in the previous exercise is $A$ then $\alpha$ is a root of the characteristic polynomial of $A$. This gives an effective procedure for determining an equation of degree $n$ satisfied by an element $\alpha$ in an extension of $F$ of degree $n$. Use this procedure to obtain the monic polynomial of degree 3 satisfied by $\sqrt[3]{2}$ and by $1 + \sqrt[3]{2} + \sqrt[3]{4}$. }

\begin{proof}
Let $c_A = a_nx^n + \cdots + a_1x + a_0$ be the characteristic polynomial of the matrix $A$ of multiplication by $\alpha$. Then we know:
\bee
c_A(A) &= a_nA^n + \cdots + a_1A + a_0 = 0,
\eee
where $0$ represents the 0 matrix. Then replacing $A$ with $\alpha$, we have:
\bee
c_A(\alpha) &= a_n\alpha^n + \cdots + a_1\alpha + a_0 = 0,
\eee
which makes sense since $\alpha$ must be an eigenvalue of $A$ since $Ar = \alpha r$. So it must be a root by definition.
\end{proof}



Now we find a monic polynomial of degree 3 satisfied by $\sqrt[3]{2}$. We have a basis $\Set{1,\sqrt[3]{2},\sqrt[3]{4}}$. We set $k = \lpar 
\begin{matrix}
a\\b\\c
\end{matrix} \rpar$. We solve for $A$ knowing:
\bee
A\lpar 
\begin{matrix}
a\\b\\c
\end{matrix} \rpar &= \sqrt[3]{2}\lpar 
\begin{matrix}
a\\b\\c
\end{matrix} \rpar\\
&= (\sqrt[3]{2}a + \sqrt[3]{4}b + 2c)\\
\Rightarrow A &= \lpar 
\begin{matrix}
0 & 0 & 2\\
1 & 0 & 0\\
0 & 1 & 0
\end{matrix} \rpar.
\eee
And the characteristic polynomial of $A$ is $x^3 - 2$. 


Using the exact same basis, we find that for $\alpha = 1 + \sqrt[3]{2} + \sqrt[3]{4}$, 
\bee
A = \lpar 
\begin{matrix}
1 & 2 & 2\\
1 & 1 & 2\\
1 & 1 & 1
\end{matrix} \rpar
\eee
Thus the characteristic polynomial is given by $x^3 - 3^2 - 3x - 1$. 





\end{enumerate}

\section{Classical straightedge and compass constructions}

This section is constructions with a ruler and compass; we will skip it until after we hit Galois theory. 

\textbf{Thursday, April 12th}

And we're back. Note this is a straightedge, not a ruler, since we can't measure arbitrary distances with a straightedge. Given a set $S \sub \R^2$. We are allowed to:
\begin{enumerate}
\item $\forall s_1,s_2 \in S$, we can connect the line through $s_1,s_2$. 
\item $\forall s_1,s_2,s_3 \in S$, we can construct a circle centered at $s_1$ of radius dist$(s_2,s_3)$. We can find the intersection of any two lines, or a line and a circle, or two circles to produce more points. 
\end{enumerate}


Let's try to translate this problem to the analytic level. We introduce coordinates in the plane. If the plane is $\R^2$, then we already have coordinates. Then each point is given by its coordinate. Let $F$ be the field generated by the coordinates of ``given" points. What new numbers can we get? We can add numbers, multiply them, subtract them, and divide them, using the legal operations. How do we add two elements of $F$?

Also, for $x \in F$, $x > 0$, we can construct $\sqrt{x}$. Finally, we can construct all elements of the quadratic closure of $F$, the minimal field containing and $F$ and closed under $\sqrt{•}$. 

\begin{Def}
ELements of this field are called \textbf{constructible} (from $S$) numbers. 
\end{Def}


\begin{Claim}
Only constructible numbers can be constructed using our operations. 
\end{Claim}

\begin{proof}
Setting $y = ax + b$, $a,b \in F$, $y = cx + d$, $c,d \in F$, new point satisfies $ax + b = cx + d$, so $x \in F$. For points on a circle or intersection points of two circles, these solutions are of quadratic equations, so they are contained in a quadratic extensions of $F$. 
\end{proof}

\begin{rem}
A number is constructible (from $F$) if it is contained in a finite tower of quadratic extensions of $F$. 
\end{rem}

\begin{Def}
Call such extensions: $K = K_n/K_{n - 1}/.../K_0 = F$ with $[K_{i + 1}:K_i] = 2$ $\forall i$ \textbf{polyquadratic. }
\end{Def}

In particular, if $\alpha$ is constructible, $\deg_F \alpha = 2^k$ for some $k$. 

\begin{Ex}

\begin{enumerate}
\item Bisection of an angle: is possible. 

\item But a trisection is not possible, generally speaking. Our $F$ is $\Q(\cos \theta, \sin \theta)$, we want $\alpha = \cos\lpar \theta/3 \rpar$. 

\bee
\cos \theta = 4 \cos^3\lpar \theta/3 \rpar  - 3 \cos\lpar \theta/3 \rpar  = 4\alpha^3 - 3 \alpha. 
\eee
So $\alpha$ is a root of $4x^3 - 3x - \cos\theta = f$. If $F$ is irreducible, $\deg_F\alpha = 3$, and $\alpha$ is not constructible. For instance, for $\theta - \pi/3$, $\cos \theta = \fracc{1}{2}$, so $f = 4x^3 - 3x - \fracc{1}{2}$, so $F = \Q(\sqrt{3})$. So $2f = 8x^3 - 6x - 1$. $y  = 2x$, so we have $y^3 - 3y - 1$ is irreducible over $\Q$, over $\Q(\qwe{3})$? $f$ has no roots in $\Q(\qwe{3})$ either, so is irreducible over $\Q(\qwe{3})$. 

\item ``Doubling a cube". Construct $\alpha \in \R$ such that $\alpha^3 = 2$. Impossible! $\alpha$ has degree 3 over $\Q$. 

\item ``Squaring a circle": construct $\alpha$ such that $\alpha^2 = \pi$. Not solvable over $\Q$, $\qwe{\pi}$ is not constructible over $\Q$. 
\end{enumerate}
\end{Ex}

\begin{rem}
Any subextension of a polyquadratic extension is polyquadratic, and any composite of such extensions is also polyquadratic. 
\end{rem}

\begin{proof}
If you have:

\begin{center}
\begin{tikzcd}
K_n = K \supseteq L \arrow[d, no head] &  & L \arrow[d, no head] \\
K_{n - 1} \arrow[d, no head] &  & L \cap K_{n - 1} \arrow[d, no head] \\
\vdots \arrow[d, no head] & \Rightarrow & L \cap K_{n - 2} \arrow[d, no head] \\
K_1 \arrow[d, no head] &  & \vdots \arrow[d, no head] \\
F &  & F
\end{tikzcd},
\end{center}
each of the extensions in the right tower is either quadratic or trivial. (No idea why). 
\end{proof}


\begin{theorem}
Given a field $F \sub \R$, $\alpha \in R$, is constructible from $F$ if and only if $\alpha$ is contained in a real polyquadratic extension of $F$ (a tower of quadratic extensions). 
\end{theorem}

\begin{center}
\begin{tikzcd}
K_n \arrow[d, "2", no head] \\
\vdots \arrow[d, "2", no head] \\
K_1 \arrow[d, "2", no head] \\
F
\end{tikzcd}.
\end{center}

\begin{rem}
The composite of two polyquadratic extensions is polyquadratic. 
\end{rem}

\begin{proof}


\begin{center}
\begin{tikzcd}
K_n \arrow[d, "2", no head] &  &  & L_m \\
\vdots \arrow[d, "2", no head] &  & \udots \arrow[ru, "2", no head] &  \\
K_1 \arrow[d, "2", no head] & L_1 \arrow[ru, "2", no head] &  &  \\
F \arrow[ru, "2", no head] &  &  & 
\end{tikzcd}.
\end{center}
And we get:
\begin{center}
\begin{tikzcd}
K_nL_m \arrow[d, "\leq 2", no head] \\
\vdots \arrow[d, "\leq 2", no head] \\
K_nL_1 \arrow[d, "\leq 2"] \\
K_n \arrow[d, "2", no head] \\
\vdots \arrow[d, "2", no head] \\
K_1 \arrow[d, "2", no head] \\
F
\end{tikzcd}.
\end{center}
So $\forall i$, $[K_nL_{i + 1}:K_nL_i] \leq [L_{i + 1}:L_i] = 2$. So, $K_nL_{i + 1}/K_nL_i$ is either trivial or quadratic. 
\end{proof}

We are in characteristic 0. So the Galois closure of a polyquadratic extension is polyquadratic. 

\begin{rem}
The Galois group of a Galois polyquadratic extensions is a $2$-group, since it has order $2^n$ for some $n$. 
\end{rem}

\begin{rem}
Conversely, if $K/F$ is Galois and $\gal(K/F)$ is a $2$-group, then $\gal(G/F)$ has a normal series. 
\end{rem}

\bee
1 = H_0 \propnorm H_1 \propnorm \cdots H_n = \gal(K/F),
\eee
with $|H_{i + 1}:H_i| = 2$ for all $i$. So $K$ is a tower of polyquadratic extensions. 


\begin{Cor}
Any subextension of a polyquadratic extension is polyquadratic. 
\end{Cor}

\begin{proof}
We may assume that $K$ is Galois. Let $G = \gal(K/F)$. Let $L \sub K$, let $H = \gal(K/L)$. Then by Sylow's theorem, there is a sequence:

\bee
H \propnorm H_1 \propnorm H_2 \propnorm ... \propnorm G,
\eee
with $H_{i + 1}:H| = 2$. So, there exists a tower of subfields $L/K_{n - 1}/.../F$ such that $[K_i:K_{i - 1}] = 2, \forall i$. 
\end{proof}

\begin{Def}
$z = x + iy \in \c$ is \textbf{constructible} if $x,y$ are constructible. 
\end{Def}

\begin{Claim}
$z = x + iy \in \c$ is constructible over $F \sub \R$ if and only if $z$ is an element of a polyquadratic extension. 
\end{Claim}
\begin{proof}
If $z$ is constructible, then $x,y$ are constructible, so $x \in K_1, y \in K_2$, where $K_1,K_2$ are polyquadratic extensions of $F$, so $z \in K_1K_2(i)$, which is also polyquadratic. 

If $z \in K$ polyquadratic, then $\bar{z} \in \bar{K}$. Then $x = \fracc{z + \bar{z}}{2}, y = \fracc{z - \bar{z}}{2}\in K\bar{K}(i)\cap \R$, which is a real polyquadratic extension of $F$. So $x,y$ are constructible, so $z$ is constructible. 
\end{proof}

\begin{theorem}
$\alpha \in \c$ is constructible from $F \sub \R$ if and only if $\gal(m_{\alpha,F})$ is a 2-group. 
\end{theorem}

\begin{Ex}
Constructions of regular $n$-gons. $F = \Q$. Equivalently, for which $n$ is $\omega = e^{2\pi i/n}$ constructible? Minimal polynomial is $\Phi_i, \gal(\Phi_n/\Q) = \z_n^* has order \phi(n)$. 
\end{Ex}

\begin{rem}
$\omega$ is contructible if and only if $\phi(n)= 2^k$ for some $k$. 
\end{rem}

If $N = 2^rp_{1}^{s_1}\cdots p_{l}^{s_l}$, then $\phi(n) = 2^{r - 1}p_1^{s_1 - 1}(p_1 - 1)\cdots p_l^{s_l - 1}(p_l - 1)$, so it must be that $s_1 = \cdots = s_l = 1$, and $\forall i$, $p_i - 1$ is a power of 2. 

\begin{Def}
\textbf{Fermat's primes:} $3,5,17,257$,... (they are of the form $2^{2^r} + 1$). 
\end{Def}

So $3,6,12,10,20,30$-gons are constructible, and $7,9,22$-gons are not. So we need $n = 2^rp_1\cdots p_l$, for distinct Fermat primes. 











\section{Splitting fields and algebraic closures}


\begin{Def}\index{splitting field}
Let $f \in F[x]$. An extension $K/F$ is a \textbf{splitting field of $f$ }if in $K$, $f$ splits ``completely": $f = f_1\cdots f_k$, where $f_i$ are linear, so $f = c(x - \alpha_1)\cdots(x - \alpha_k)$, and $K$ is the minimal field with this property. 
\end{Def}

\begin{rem}
For any subfield $E\sub K$, $f$ doesn't split in $K$. Equivalently, $K = F(\alpha_1,...,\alpha_k)$. 
\end{rem}

\begin{theorem}
$\forall f \in F[x]$, the splitting field of $f$ exists, and is unique up to isomorphism. 
\end{theorem}

\begin{proof}
You take $f$, if it doesn't split completely, it has nonlinear irreducible component, adjoin roots, you will get a splitting field. Okay, now for the formal proof. Let $f = g_1\cdots g_m$ where $g_i$ are irreducible. If one of $g_i$ is not linear, adjoin a root $\alpha$ of $g_i$, then in $F(\alpha)$, $g_i(x) = h_i(x)(x - \alpha)$, $\deg h_i = \deg g_i - 1$, and continue... until all irreducible polynomials are linear. There is more to this proof, it follows from preceeding lemma. 
\end{proof}

Note $[F(\alpha):F] = \deg_F(\alpha) = \deg g_i$. 


Behold, a tower:
\begin{center}
\begin{tikzcd}
F(\alpha_1,...,\alpha_n) \\
\vdots \arrow[u, no head] \\
F(\alpha_1,\alpha_2) \arrow[u, no head] \\
F(\alpha_1) \arrow[u, no head] \\
F \arrow[u, no head]
\end{tikzcd}.
\end{center}

\begin{Prop}
Let $K = F(\alpha_1,...,\alpha_n)$, the splitting field of $F$,$\alpha_i$ the roots of $f$, then $[K:F] \leq n!$. Note $\deg f = n$. 
\end{Prop}
\begin{proof}
$[F(\alpha_1):F] \leq n$. Over $F(\alpha_1)$, $f = (x - \alpha_1)\tilde{f}$, $\deg \tilde{f} = n  -1$, $K$ is the splitting field of $\tilde{f}$ over $F(\alpha_1)$. We have:
$$
[K:F] = \deg_F\alpha_1 \cdot \deg_{F(\alpha_1)} \alpha_2 \cdots \deg_{F(\alpha_1,...,\alpha_{n - 1})}\alpha_n.
$$
We know $\deg_F\alpha_1 \leq n$, and $\deg_{F(\alpha_1)} \alpha_2 \leq n - 1$, and so on.
By induction, $[K:F(\alpha_1)] \leq (n - 1)!$, so $[K:F] \leq n!$. 
\end{proof}

\begin{Ex}
$\deg f = 2$, $f = (x - \alpha_1)(x - \alpha_2)$. Then $F(\alpha_1)$ is the splitting field. 
\end{Ex}



\textbf{Thursday, March 22nd}

\bb
Let $f \in F[x] \Rightarrow$ splitting field $K$ of $f$. This is the minimal field where $f$ splits ocmpletely, $f(x) = c(x - \alpha_1)\cdots(x - \alpha_n)$. We have $K = F(\alpha_1,...,\alpha_n)$, and $[K:F] \leq n!$. 

\textbf{Get the rest from Paul for this day. }

\begin{Ex}
We give some examples: 
\end{Ex}







\bb

\textbf{Friday, March 23rd}

We do exercises from several sections.  


\bb

\textbf{Monday, March 26th}

\begin{Def} \index{normal!extension}
$K/F$ is said to be \textbf{normal} if:
\begin{enumerate}
\item it is algebraic and $\forall \alpha \in K$, $m_{\alpha,F}$ splits completely over $K$.
\item Or: if an irreducible polynomial over $F$ has a root in $K$, then it splits over $K$. 
\item Or: $\forall \alpha \in K$, \textbf{all conjugates of $\alpha$ are in $K$}. This is to say, if you take an element $\alpha \in K$, all its conjugates in any \textit{larger} field are in $K$. 
\end{enumerate}  
\end{Def} 

Why are they equivalent definitions? 
\begin{proof}
$m_{\alpha}$ splits completely over $K$. $m_{\alpha}(x) = (x - \alpha_1)(x - \alpha_2)\cdots(x - \alpha_n), \alpha_i \in K$. That is, all conjugates of $\alpha$ are in $K$. 
\end{proof}

We always assume $K \sub \bar{F}$ - algebraic closure of $F$. $K$ is normal if for any element in $K$, all conjugates of $\alpha$ in $\bar{F}$ are in $K$. 

\begin{Def}\index{normal!closure}
The \textbf{normal closure} of an extension $K/F$ is the minimal extension which contains all conjugates of all elements of $K/F$. 
\end{Def}

\begin{Def}\index{conjugate}
A \textbf{conjugate} is a root of the same minimal polynomial: $\sqrt{2}\mapsto -\sqrt{2}$; $\sqrt[3]{2} \mapsto \omega\sqrt[3]{2},\omega^2\sqrt[3]{2}$. 
\end{Def}

What does it mean to split completely in $K$? I don't know. Paul doesn't either. 

\begin{Ex}
Any extension of degree 2 is normal. $\forall \alpha \in K\setminus F$, $m_{alpha,F}(x) = x^2 + ax + b$, and over $K$, $m_{\alpha,F}(x) = (x - \alpha)(x - \beta)$. 
\end{Ex}

\begin{rem}
In the book, this notion of normal extensions is not introduced, but it's very standard. 
\end{rem}


\begin{theorem}
A finite extension $K/F$ is normal if and only if $K$ is a splitting field of some $f \in F[x]$. 
\end{theorem}

$K$ is a splitting field means it is normal for just one polynomial. $K$ is a splitting field of $f$ if $\forall$ root $\alpha$ of $f$, all conjugates of $\alpha$ are in $K$. That is, ``$K$ is normal for generators of $K$ over $F$."

\begin{Ex}
$\Q(\sqrt[3]{2}/\Q)$ - not normal. 
\end{Ex} 

\begin{proof}
\textbf{$(\Rightarrow)$} Let $K = F(\alpha_1,...,\alpha_n)$, let$f = m_{\alpha_1,F}\cdots m_{\alpha_n,F}$. Then $f$ splits over $K$, since all irreducible components of $f$ split, and $K$ is generated by the roots of $f$. So, $K$ is a splitting field of $f$. 

\textbf{$(\Leftarrow)$} Let $K$ be a splitting field of $f \in F[x]$, where $f$ is arbitrary. And let $\alpha \in K$. Let $\beta$ be a conjugate of $\alpha$ in some larger field, say the algebraic closure of $F$. We have to show that $\beta \in K$. Consider the diagram:
\begin{center}
\begin{tikzcd}
K \arrow[d, no head] &  & K' \arrow[d, no head] \\
F(\alpha) \arrow[rd, no head] \arrow[rr, "\overset{\phi}{\cong}", no head] &  & F(\beta) \arrow[ld, no head] \\
 & F & 
\end{tikzcd}.
\end{center}
We also have:
\begin{center}
\begin{tikzcd}
 & K' \arrow[ld, "\sub"', no head] \arrow[rd, "f", no head] &  \\
K \arrow[rd, "f", no head] &  & F(\beta) \arrow[ld, no head] \\
 & F & 
\end{tikzcd}.
\end{center}
Let $K'$ be the splitting field of $f$ over $F(\beta)$. $K$ is a splitting field of $f$ over $F(\alpha)$ as well. Note $\phi(\alpha) = \beta$. So $\phi$ extends to an isomorphism $K \overset{\cong}{\to} K'$. But actually, $K \sub K$, since $K'$ is the splitting field of $f$ over a larger field: if $E/F$, $f \in F[x]$, $K$ is the splitting field of $f$ over $F$, then the splitting field $\tilde{K}$ of $f$ over $E$ contains $K$: $K = F(\alpha_1,...,\alpha_n)$, $\tilde{K} = E(\alpha_1,...,\alpha_n)$. 

So $K \sub K'$. But $\dim_F K' = \dim_F K$, so $K' = K$, and so $\beta \in K' = K$. 
\end{proof}


\textbf{Properties: }
\begin{enumerate}
\item If $K/E/F$ are extensions and $K/F$ is normal, then $K/E$ is normal. 
\item If $K_1/F,K_2/F$ are normal, then $(K_1 \cap K_2)/F$ is normal. (trivial) Note $K_1,K_2 \sub K$. 
\item If $K_1,K_2 \sub K$, $K_1/F,K_2/F$ are normal and finite (finiteness not needed), then $(K_1K_2)/F$ is normal. 

\begin{proof}
$K_1$ is a splitting field of $f_1$, since finite, $K_2$ is a splitting field of $f_2$, so $K_1K_2$ is a splitting field of $f_1f_2$ (using previous result about splitting fields).  So $K_1$ is generated by some roots of some polynomial, and all other roots of this polynomial are here. So all conjugates of generators are here. 
\end{proof}
\end{enumerate}

\textbf{Separable extensions}

$\alpha \in K$, $\deg_F \alpha = n$. Number of conjugates of $\alpha$? $\leq n$. Every polynomial has exactly $n$ roots counting multiplicities. It is $n$ if and only if $m_{\alpha,F}$ has no multiple roots. If $f$ is irreducible, may $f$ have multiple roots? 

 \begin{Def}\index{separable!extension}
An extension $K/F$ is \textbf{separable} if and only if:
\begin{enumerate}
\item the minimal polynomial over $F$ of every element is separable. 
\item every element of $K$ is the root of a separable polynomial over $F$.
\end{enumerate} 
\end{Def}

\begin{rem}
Note $f$ has a multiple root if and only if it is a common root with it's derivative $f'$, i.e. if and only if $f(\alpha) = f'(\alpha) = 0$. 
\end{rem}


\bb

\textbf{Tuesday, March 27th}


\begin{Def}\index{separable!polynomial}
A polynomial $f$ is \textbf{separable} if it has no multiple roots (in any extension of $F$). This is so if and only if $f'(\alpha) \neq 0$ $\forall$ root $\alpha$ of $f$. 
\end{Def}

\begin{Def}\index{separable!element}
An algebraic over $F$ element $\alpha$ is \textbf{separable} if $m_{\alpha,F}$ is a separable polynomial. In this case, $\alpha$ has exactly $n$ conjugates, counting $\alpha$ itself, where $n = \deg_F \alpha$. 
\end{Def}

\begin{Def}\index{extension!perfect}
A field is called \textbf{perfect} if any irreducible $f \in F[x]$ is separable. 
\end{Def}

\begin{rem}
Let $f \in F[x]$ be irreducible. Then $f$ is not separable if $f$ and $f'$ have a common root $\alpha$. But $f$ is the minimal polynomial of $\alpha$, so this is only possible if $f' = 0$. 
\end{rem}

\begin{Cor}
So any field of characteristic 0 is perfect.  
\end{Cor}
\begin{proof}
When the characteristic is zero, then $\deg f' = \deg f - 1 \neq 0$ (unless $f = $ constant), so $f$ is separable. 
\end{proof}


\begin{rem}
In char $p$: for $f = x^p - a$, $f' = px^{p - 1} = 0$, since $p = 0$ in a characteristic $p$ field. 
\end{rem}

Let:
$$
f(x) = a_nx^n + a_{n - 1}x^{n - 1} + \cdots + a_1x + a_0.
$$
 Then:
 $$
 f'(x) = na_nx^{n - 1} + (n - 1)a_{n - 1}x^{n - 2} + \cdots + 2a_2x  + a_1.
 $$
  So $f'(x) = 0$ if and only if $ka_k = 0$ for all $k$, so either $a_k = 0$ or $k = 0$, that is $a_k = 0$, or $p|k$. Setting $n = mp$, $f'(x) = 0$ if and only if:
  $$
  f(x) = a_{mp}x^{mp} + a_{(m - 1)p}x^{(m - 1)p} + \cdots + a_px^p + a_0,
  $$ 
  that is $f(x) = g(x^p)$ for some $g \in F[x]$ ($g(x) = a_{mp}x^{m} + a_{(m - 1)p}x^{(m - 1)} + \cdots + a_px + a_0$). So if char$f = p$, and $f$ is irreducible and nonseparable, then $f(x) = g(x^p)$. 

\begin{Def}\index{Frobenius endomorphism}
The mapping $\phi:F \to F$, $\phi(a) = a^p$, is called the \textbf{Frobenius endomorphism} of $F$. It is a homomorphism: $\forall a,b, (ab)^p = a^pb^p$. And:
\bee
(a = b)^p &= a^p + pa^{p - 1}b + \binom{p}{2}a^{p - 2}b^2 + \cdots + pab^{p - 1} + b^p\\
&= a^p + b^p,
\eee
since all the middle terms go to zero. 
\end{Def}

\begin{Def}\index{Frobenius automorphism}
If it is surjective, it is called the \textbf{Frobenius automorphism} of $F$. 
\end{Def}

So Frobenius endomorphism is an automorphism if and only if $\forall a \in F$, $\exists \sqrt[p]{a} \in F$, that is $b$ s.t. $b^p = a$. This $\sqrt[p]{a}$ is unique, since ker$\phi = 0$. 

\begin{Prop}
$F$ is perfect if and only if its Frobenius is surjective. 
\end{Prop}

\begin{proof}
Assume that Frobenius is surjective. Let $f(x) = g(x^p) = a_nx^{np} + \cdots + a_1x^p + a_0$. $\forall k$, let $b_k = \sqrt[b]{a_k}$, so that $f(x) = b_n^px^{np} + \cdots + b_1^px^p + b_0^p$. Then we know
$$
f(x) = (b_nx^n + \cdots + b_1x + b_0)^p,
$$ by the Frobenius homomorphism, and it is reducible. So there are no irreducible polynomials of this form $g(x^p)$, so $F$ is perfect. 

Now if Frobenius is not surjective: if there exists $a$ s.t. $\alpha = \sqrt[p]{a} \notin F$, then $f = x^p - a$ is irreducible, but $f' = 0$, and $f$ has multiple root: $f(x) = (x - \alpha)^p$. $\alpha$ is not in $F$, and the minimal polynomial of $\alpha$ divides $f(x)$, and is the form $(x - \alpha)^k$ for some $k > 1$ and it has multiple roots, so $F$ is not perfect. So $\alpha$ is a multiple root of its minimal polynomial, so its derivative is zero, so we must have that it is equal to $f$. So $m_{\alpha,F} = (x - \alpha)^k$, $k \geq 2$ (actually $k = p$) so $\alpha$ is inseparable, and $F$ is not perfect. 
\end{proof}

\begin{Ex}
Examples of perfect and non-perfect things. 
\begin{enumerate}
\item Any finite field is perfect. 
\begin{proof}
Frobenius $\phi:F \to F$ is injective, so it is surjective, since we are mapping to and from a finite space. 
\end{proof}
\item The field $\F_p(t)$ (rational functions over $\F_p$) is not perfect: $\sqrt[p]{t} \notin \F_p(t)$, $t \in \F_p(t)$. $x^p - t$ is irreducible but inseparable. 
\end{enumerate}
\end{Ex}

\begin{rem}
Galois theory deals only with separable extensions. The non-separable results are not impressive. 
\end{rem}




















\section*{13.4 Exercises}

\begin{enumerate}[label=\arabic*.]
\item \textit{Find splitting field of $x^4 - 2$. }

We have four roots in the plane. $\pm \sqrt[4]{2}, \pm i \sqrt[4]{2}$. We just adjoin them and this will be the splitting field. It is $\Q(\sqrt[4]{2},i)$. It is of degree 8. We know this since $\sqrt[4]{2}$ gives you 4 and adding $i$ gives you 2 more, multiply to 8. 

\item \textit{Find the splitting field and its degree over $\Q$ for $x^4 + 2$. }

We have four roots in the plane. Observe:
\bee
x^4 &= -2\\
x^2 &= \pm i\sqrt{2}\\
x &= \pm \sqrt{i \sqrt{2}}, \pm i\sqrt{i\sqrt{2}}\\
&= \pm \sqrt{i}\sqrt[4]{2},\pm i\sqrt{i}\sqrt[4]{2}.
\eee
We adjoin them and this will be the splitting field. It is $\Q(\sqrt{i}\sqrt[4]{2}, i)$. The first root $\sqrt{i}\sqrt[4]{2}$ is of degree 4 since $x^4 + 2$ has degree 4 and it is a root of this irreducible polynomial. And $i$ has degree 2 and is linearly independent, so we know the splitting field has degree 8 over $\Q$. 




\setcounter{enumi}{2}

\item \textit{Find splitting field of $x^4 + x^2 + 1$ over $\Q$. }

$$
x^4 + x^2 + 1 = \fracc{x^6 - 1}{x^2 - 1}.
$$
There is no general rule, you should just know this. The roots are $\pm \fracc{1}{2} \pm \fracc{\sqrt{-3}}{2}$. Note $\sqrt{-3} = i\sqrt{3}$. These are roots of unity of degree 6. We suspect it is $\Q(\sqrt{-3})$. So then these roots would be $\alpha,\alpha^2,\alpha^4,\alpha^5$. but note we have:
$$
\fracc{x^6 - 1}{x^2 - 1} = \fracc{(x^3 - 1)(x^3 + 1)}{(x - 1)(x + 1)} = (x^2 + x + 1)(x^2 - x + 1).
$$



\end{enumerate}

\section{13.5 Exercises}

\begin{enumerate}[label=\arabic*.]
\setcounter{enumi}{4}

\item \textit{For any prime $p$ and any nonzero $a \in \F_p$, prove that $x^p - x + a$ is irreducible and separable over $\F_p$. [For the irreducibility: One approach -- prove first that if $\alpha$ is a root then $\alpha + 1$ is also a root. Another approach -- suppose it's reducible and compute derivatives.]}

\begin{proof}
Suppose $\alpha$ is a root. Then we have $\alpha^p - \alpha + a = 0$. Behold: 
\bee
(\alpha + 1)^p - (\alpha + 1) + a &= \lpar\sum_{k = 0}^p \binom{p}{k}\alpha^k\rpar - \alpha - 1 + a\\
&= \lpar \sum_{k = 1}^{p - 1} \binom{p}{k}\alpha^k \rpar + \alpha^p - \alpha + a\\
&= \sum_{k = 1}^{p - 1} \binom{p}{k}\alpha^k \\
&= \sum_{k = 1}^{p - 1} \fracc{p!}{k!(p - k)!}\alpha^k.
\eee
We claim that $\fracc{p!}{k!(p - k)!}$ is divisible by $p$ for all integer values of $k$ in the range $[1,p - 1]$. Note for these values of $k$ that $p\nmid (k!(p - k)!)$ but that $p|p!$, and the binomial coefficient is an integer, so we must have that $p|\lpar \fracc{p!}{k!(p - k)!} \rpar $. Thus: 
$$
\sum_{k = 1}^{p - 1} \fracc{p!}{k!(p - k)!}\alpha^k \mod p \equiv 0.
$$
And since we are over $\F_p$, we know that $\alpha + 1$ must then be a root. Now note that by induction, we have that if any $\alpha \in \F_p$ is a root, then all elements of $\F_p$ are roots, hence $0$ is a root. So we have:
$$
0^p - 0 + a = 0 \Rightarrow a = 0,
$$
which is a contradiction, since we said $a\neq 0$. So we must have that $\nexists \alpha \in \F_p$ such that $\alpha$ is a root of the given polynomial. So let $\alpha$ be a root, then $\alpha \notin \F_p$. Then consider the extension $\F_p(\alpha)$. It must contain $\alpha + k$, for all $k \in \F_p$. Then $f(x)$ must be the product of all minimal polynomials. Also since $\F_p(\alpha) \cong \F_p(\alpha + k)$ we know that they all have the same degree, say $m$. Then $p = km$, which tells us $k = 1$ since $p$ prime. Then we must have that the minimal polynomial is $f$ and it is irreducible. 
Now we show that it is separable. Simply recall from Proposition 37 in the book that every irreducible polynomial over a finite field is separable. 
\end{proof}



\end{enumerate}

\setcounter{section}{5}

\section{Cyclotomic polynomials and extensions}

\begin{Def} \index{root of unity}
$\forall$ field $F$, roots of $x^n = 1$ are called the \textbf{roots of unity}. 
\end{Def}

In any extension of $F$, roots of unity form a group under multiplication, of order $\leq n$. (If $\alpha^n = 1, \beta^n = 1$, then $(\alpha\beta)^n = 1$)

It can have order $< n$ when you are in a finite field and you have the case of multiple roots. 

\begin{Def}\index{$n$-th cyclotomic extension of $F$}
The splitting field of $x^n - 1$ is called the $n$-th \textbf{cyclotomic extension of $F$}. 
\end{Def}

\begin{Def}\index{primitive roots of unity}
Generators of this group are called \textbf{primitive roots of unity}. 
\end{Def}

\begin{rem}
Over $\Q$: roots of unity of degree $n$ are $1,\omega,...,\omega^{n - 1}$, where $\omega = e^{2\pi i/n}$. Think of them on a circle. Primitive roots of unity are $\omega^k$ with $(k,n) = 1$. If $(k,n) = d > 1$, then $(\omega^{k})^{n / d} = 1$, so $\omega^k$ is a root of unity degree $n/d$. 
\end{rem}

\bb

\textbf{Thursday, March 29th}

\begin{Def}
The \textbf{$n$-th cyclotomic field} is the splitting field of $x^n - 1 \in \Q[x]$. 
\end{Def}

Let it be $K$, $K = \Q(\omega)$, $\omega = e^{2\pi i/n}$. All roots of $x^n - 1$ are $1,\omega,...,\omega^{n - 1}$. If $(k,n) \neq 1$< then the root $\omega^k$ is not primitive, and satisfies $x^m - 1$ for $m  = \fracc{n}{(n,k)}$. 


So what is $[K:\Q] = $? It is equal to $\deg_\Q\omega = \deg_\Q \alpha$ for any primitive root $\alpha$ (since $K = \Q(\alpha)$). 

\begin{Def}
\bee
\Phi_n(x) = \prod_{\alpha}(x - \alpha) = \prod_{(k,n) = 1}(x - \omega^k),
\eee
where $\alpha$ is a primitive $n$-th root of unity. 
\end{Def}

\begin{lem}
$x^n - 1 = \prod_{d|n}\Phi_d(x)$. 
\end{lem}

\begin{proof}
Each root of 1 of degree $n$ is a primitive root $\alpha$ of 1 of degree $d$ for some $d|n$. This $d = |alpha|$ in $\Set{1,\omega,...,\omega^{n - 1}}$ minimal $d$ s.t. $\alpha^d = 1$. So then 
$$
x^n - 1 = \prod_{\alpha^n  = 1}(x - \alpha) = \prod_{d|n}\prod_{\alpha \text{ primitive dth root}}(x - \alpha) = \prod_{d|n}\Phi_d(x).$$
\end{proof}

\begin{Prop}
$\forall n$, $\Phi_n \in \z[x]$, $\deg\Phi_n = \phi(n)$, the totient function. 
\end{Prop}

\begin{proof}
By induction. 
We have:
$$
\Phi_n(x) = \fracc{(x^n - 1)}{\prod_{\substack{d|n \\ d< n}}\Phi_d(x)}.
$$
And we have by induction that this is a monic polynomial from $\z[x]$. So $\Phi_n \in \Q[x]$. And then by Gauss's Lemma, $\Phi_n \in \z[x]$. Recall that Gauss's Lemma states that if $f|g$ over $\Q$ and the content of $f$ $c(f)$ is 1, then $f|g$ over $\z$. 
\end{proof}

\begin{Ex}
\bee
\Phi_1(x) &= x - 1\\
\Phi_2(x) &= x + 1 = \fracc{x^2 - 1}{x - 1}\\
\Phi_3(x) &= \fracc{x^3 - 1}{x - 1} = x^2 + x + 1\\
\Phi_4(x) &= \fracc{x^4 - 1}{\Phi_1(x)\Phi_2(x)} = \fracc{x^4 - 1}{(x - 1)(x + 1)} = x^2 + 1\\
\Phi_5(x) &= \fracc{x^5 - 1}{x - 1} = x^4 + x^3 + x^2 + x + 1\\
\Phi_p(x) &= x^{p - 1} + x^{p - 2} + \cdots + x + 1 = \prod_{k - 1}^{p - 1}(x - \omega^k), \omega = e^{2\pi i/p}, \forall p \in \mathbb{P}\\
\Phi_6(x) &= \fracc{x^6 - 1}{\Phi_1\Phi_2\Phi_3} \overset{?}{=} \Phi_3(-x) = x^2 -x + 1.
\eee
Note the coefficients are \textbf{NOT} always $\pm 1$!
\end{Ex}

\begin{theorem}
$\forall n$, $\Phi_n$ is irreducible over $\Q$. So $[\Q(\omega):\Q] = \phi(n)$. All primitive roots of unity of degree $n$ are conjugate. 
\end{theorem}


\begin{proof}


Assume that $\Phi_n$ is reducible, $f$ is its irreducible factor, $\Phi_n(x) = f(x)g(x)$. All roots of $\Phi_n$ are distinct, so it's separable. Let $\alpha$ be a root of $f$, $\alpha$ is a primitive root of 1 of degree $n$. 


We claim that $\forall p \in \mathbb{P}$ s.t. $p\nmid n$, $\alpha^p$ is also a root of $f$. If so, then all primitive roots of unity of degree $n$ are roots of $f$, so $f = \Phi_n$. (any such root is $\omega^{p_1p_2\cdots p_k}$, $p_i\nmid n$)

\begin{proof}
$\alpha^p$ is also a primitive root, so is a root of $\Phi_n$. Assume it is a root of $g$, so $g(\alpha^p) = 0$, so $\alpha$ is a root of $g(x^p)$ so $f|g(x^p)$. . And now pass to the field $\F_p$ by $\z \mapsto \z/(p)$. Over $\F_p$< $g(x^p) = g(x)^p$. (In $\F_p$, $a^p = a$ $\forall p$, so $(x^n + a_{n - 1}x^{n - 1} + \cdots + a_1x + a_0)^p = x^{pn} + a_{n - 1}(x^p)^{n - 1} + \cdots + a_0$) So $f|g^p$ over $\F_p$. So $\Phi_n$ is not separable over $\F_p$. If so, $x^n - 1$ is not separable over $\F_p$. But $(x^n - 1)' = nx^{n - 1}$ has no common roots with $x^n - 1$. 
\end{proof}
Then $f = \Phi_n$ which is a contradiction since we assumed $\Phi_n$ is reducible, so it must be irreducible. 
\end{proof}



\section*{13.6 Exercises}

\begin{enumerate}[label=\arabic*.]
\setcounter{enumi}{5}
\item \textit{Prove that for $n$ odd, $n> 1$, $\Phi_{2n}(x) = \Phi_n(-x)$. }

\begin{proof}
Let $n$ be odd, and let $\phi(x)$ be Euler's totient function. Then $\phi(n) = \phi(2n)$ since the only factor of $2n$ which is not already a factor of $n$ is 2, and $2\nmid n$ since $n$ is odd. So then $\Phi_{2n}(x)$ has the same degree as $\Phi_n(-x)$. So they both have the same number of roots. But note we know that if $\omega$ is an $n$-th root of unity, then we know that $-\omega$ is also an $n$-th root of unity and also a $2n$-th root of unity. Then the roots of $\Phi_{n}(-x)$ are also roots of $\Phi_{2n}(x)$, and since we already proved that they have the same number of roots, we know they are the same polynomial. (Note I got the idea for this proof from Jack Peltier)
\end{proof}

\end{enumerate}














\chapter{Galois theory}

\section{Basic definitions}

\textbf{Friday, March 30th}

Let $E/F$ be an extension, $K/F$ be a finite subextension ($F\sub K \sub E$). 

\begin{Def}\index{embeddings}
\textbf{Embeddings} of $K$ to $E$ over $F$:
homomorphisms $\phi:K \to E$ such that $\phi|_F = \text{Id}_F$. It may be that $\phi(K) = K$, but $\phi$ is non-trivial: $\phi \neq \text{Id}_K$. 
\end{Def}

\begin{Def}\index{conjugate!of an extension}
If $\phi$ is such an embedding of $K$, then $\phi(K)$ is called a \textbf{conjugate} of $K$. 
\end{Def}

\begin{lem}
If $\alpha \in K$, $\phi$ is an embedding, then $\phi(\alpha)$ is conjugate of $\alpha$. 
\end{lem}

\begin{proof}
If $f = m_{\alpha,F}$, then $\phi(f) = f \in F[x]$, where $\phi(f)$ is $\phi$ applied to the coefficients of $f$. So $\phi$ maps roots of $f$ to roots of $f$. We have:
\bee
f(\alpha) = 0 \Rightarrow \phi(f(\alpha)) = 0.
\eee
And $f(\phi(\alpha)) = \phi(f(\alpha))$. So $\phi(K)$ consists of conjugates $\phi(\alpha)$ of elements $\alpha$ of $K$. So, if $K/F$ is normal, then $\phi(K) = K$, $\forall \phi$. ($K/F$ is normal means that $\forall \alpha \in K$, all conjugates of $\alpha$ are in $K$)
\end{proof}

If $K/F$ is simple, $K = F(\alpha)$. 
Under any embedding $\phi$, $\phi(\alpha)$ is a conjugate of $\alpha$, and $\phi(\alpha)$ defines $\phi$. So the number of embeddings of $K$ over $F$ is just the number of conjugates of $\alpha$ over $F$ in $E$. So, the number of embeddings is always $\leq\deg_F\alpha$ and is equal to $\deg_F\alpha$ if and only if $\alpha$ is separable and $m_{\alpha,F}$ splits in $E$. 


\begin{Ex}
\begin{enumerate}
\item Embeddings of $\Q(i)$ over $\Q$ into $\c$. We may have $i \mapsto i$ or $i \mapsto -i$. The embeddings are:
\bee
\begin{cases}
z \mapsto z\\
z \mapsto \bar{z}
\end{cases}.
\eee
We define $\bar{z}$ to be the complex conjugate. 
Note both images are $\Q(i)$. 
\item Embeddings of $\c$ into $\c$ over $\R$. We have the same:
\bee
\begin{cases}
z \mapsto z\\
z \mapsto \bar{z}
\end{cases}.
\eee
\item Embeddings of $\Q(\sqrt[3]{2})$ over $\Q$ into $\c$. We have three:
\begin{center}
\begin{tikzcd}
 & \sqrt[3]{2} \\
\sqrt[3]{2} \arrow[ru, maps to] \arrow[r, maps to] \arrow[rd, maps to] & \omega\sqrt[3]{2} \\
 & \omega^2\sqrt[3]{2}
\end{tikzcd}.
\end{center}
And $\omega = e^{2\pi i/3}$. Note $\Q(\sqrt[3]{2}),\Q(\omega\sqrt[3]{2}),\Q(\omega^2\sqrt[3]{2})$ are conjugates of $\Q(\sqrt[3]{2})$ in $\c$. 

\item We give an example of a time when the number of embeddings is less than the degree of $\alpha$. Consider $F = \Q$, $K = \Q(\sqrt[8]{3})$, and $E = \Q(\sqrt[8]{3},i)$. Then we have $\alpha\omega^k$, $\omega = \fracc{1+ i}{\sqrt{2}}$. In $E$, $\alpha$ only has 4 conjugates, so there exist only 4 embeddings. 
\end{enumerate}
\end{Ex}

Let $\phi$ be an embedding of $K_1$ with $K_2 = \phi(K_1)$. Consider $K_1(\alpha)$, $\alpha \in E$. We have:
\begin{center}
\begin{tikzcd}
K_1(\alpha) \arrow[rr, "\psi"] \arrow[d, no head] &  & L \arrow[d, no head] \\
K_1 \arrow[rr, "\phi"] \arrow[rd, no head] &  & K_2 \arrow[ld, no head] \\
 & F & 
\end{tikzcd}.
\end{center}
What are embeddings of $K_1(\alpha)$ extending $\phi$: $\psi:K_1(\alpha) \to E$ such that $\psi|_{K_1} = \phi$. Any such $\psi$ is uniquely defined by $\psi(\alpha)$. Let $f = m_{\alpha,K_1}$. Then $\psi(\alpha)$ must be a root of $\phi(f)$. Indeed, $f(\alpha) = 0$, so $\psi(f(\alpha)) = 0$. But $\psi(f(\alpha)) = \psi(f)(\psi(\alpha)) = \phi(f)(\psi(\alpha))$. 

We have $\phi(f) \in K_2[x]$ and is irreducible, so $\phi(f)$ is the minimal polynomial of $\psi(\alpha)$. 

Conversely, if $\beta \in E$ is a root of $\phi(f)$, then we can extend up to $K_1(\alpha)$ by defining $\psi(\alpha) = \beta$, since $K_2(\beta) \cong K_1(\alpha)$. So, we have $\leq n$ embeddings $\psi$ extending $\phi$, where $n = \deg\phi(\alpha) = \deg f = \deg_{K_1}\alpha$. And: we have exactly $n$ extensions if and only $\phi(f)$ is separable and splits in $E$. 


Let $g =m_{\alpha,F}$. Then both $f|g$, so $\phi(f)|g$. So if $g$ is separable and splits and $E$, we have exactly $n$ extensions of $\phi$. 


Now let $K = F(\alpha_1,...,\alpha_k)$. Then we have a tower of extensions:
\begin{center}
\begin{tikzcd}
K = F(\alpha_1)\cdots(\alpha_k) \arrow[d, no head] \arrow[r] & K_2 \arrow[d, no head] \\
\vdots \arrow[d, no head] & \vdots \arrow[d, no head] \\
F(\alpha_1)(\alpha_2) \arrow[d, no head] \arrow[r] & F(\beta_1,\beta_2) \arrow[d, no head] \\
F(\alpha_1) \arrow[d, no head] \arrow[r, "\cong"] & F(\beta_1) \\
F \arrow[ru, no head] & 
\end{tikzcd}.
\end{center}
$\forall i$ we have $m_i$ choices how to extend the embedding of $F(\alpha_1,...,\alpha_{i - 1})$, where $m_i \leq n_i = \deg_{F(\alpha_1,...,\alpha_{i  -1})}\alpha_i$. Totally, we have $\leq  n_1n_2\cdots n_k = [K:F]$ embeddings. If $E/F$ is normal and separable, then we have exactly $[K:F]$ embeddings of $K$ over $F$ into $E$. 

\begin{theorem}
Let $E/K/F$. Then there are $\leq [K:F]$ embeddings of $K$ over $F$ to $E$. If $E/F$ is normal and separable, then there are exactly $[K:F]$ embeddings. 
\end{theorem}

If $\exists \alpha \in K$ s.t. $\alpha$ has $< \deg_F\alpha$ conjugates in $E$ (that is, $\alpha$ is not separable, or $m_{\alpha,F}$ doesn't split in $E$) then we have $< n$ embeddings of $K$. 


This is true because $K = F(\alpha,\alpha_1,...,\alpha_k)$ and for this $\alpha$ we find less than the correct number of conjugates, so when we adjoin the rest, we cannot get all the ones needed. 

\begin{Cor}
If $K$ is a splitting field of a separable polynomial, then $K$ is normal and separable. 
\end{Cor}



\bb


\textbf{Thursday, April 5th}

\begin{Def}\index{Galois!extension}
Let $K/F$ be finite, $[K:F] = n$. $K/F$ is \textbf{Galois} if:
\begin{enumerate}
\item it is normal and separable. 
\item $|\text{Aut}(K/F)| = n$.
\item $K = F(\alpha_1,...,\alpha_k)$ s.t. $\alpha_i$ are separable and all their conjugates are in $K$.
\item $K$ is a splitting field of a separable polynomial.
\end{enumerate}
So we have 4 equivalent definitions. 
\end{Def}


\begin{Def}\index{Galois!closure}
If $L/F$ is finite and separable, let $L = F(\alpha_1,...\alpha_k)$. Adjoin all conjugates of $\alpha_i$, they are still separable. Then we get an extension $K$, generated by separable elements whose conjugates are in $K$, so $K$ is Galois (the minimal Galois extension of $F$ containing $K$). $K/F$ is called the \textbf{Galois closure} of $L/F$. (It is the normal closure of $L/F$). 
\end{Def}

\begin{Def}\index{separable!element}
Recall that an element $\alpha$ is \textbf{separable} if and only if it is a root of a separable polynomial. 
\end{Def}

\begin{theorem}
\begin{enumerate}
\item 
If $K = F(\alpha_1,...,\alpha_k)$ s.t. all conjugates of $\alpha_i$ are in $K$, then $K/F$ is normal. 
\item If $K = F(\alpha_1,...,\alpha_k)$ where $\alpha_i$ are separable, then $K/F$ is separable. 
\end{enumerate}
\end{theorem}
\begin{proof}
(1) Any embedding of $K$ over $F$ into a larger field is an automorphism of $K$. This implies that $K/F$ is normal. (If $\alpha$ in $K$ has conjugate $\beta \notin K$, consider $\phi:F(\alpha) \overset{\cong}{\to} F(\beta), \alpha \mapsto \beta$, and extend it to $K \to E$. We proved that if we have an isomorphism of fields and $K$ is a splitting field of some polynomial, then take the splitting field of the image of this polynomial, and we'll have an isomorphism. Observe:
\begin{center}
\begin{tikzcd}
K \arrow[r, "\overset{\cong}{\phi}"] \arrow[d, no head] & K' \arrow[d, no head] \\
F(\alpha) \arrow[r, "\cong"] & F(\beta)
\end{tikzcd}.
\end{center}
Where $K'$ is the splitting field of some polynomial. 
\end{proof}

\begin{proof}
(2) Adjoin all conjugates of $\alpha_i$. Then we get an extension of $K$ generated by ``normal", separable elements. So this extension is Galois, so it is separable, so $K/F$ is separable. 
\end{proof}

\begin{Def}\index{Galois!group of $f$ over $F$}
Let $f \in F[x]$. The \textbf{Galois group of $f$ (over $F$)}, Gal$(F/F)$ or just Gal$(f)$, is Gal$(K/F)$, where $K$ is a splitting field of $f$. 
\end{Def}

\begin{Ex}
Gal$(x^3 - 2/\Q) \cong S_3$. 

Gal$((x^3 - 2)(x^3 - 3)/\Q) \cong V_4$. 
\end{Ex}

\begin{Def}\index{abelian!extension}
Galois extension $K/F$ is \textbf{abelian} if Gal$(K/F)$ is abelian. $K/F$ is <insert nice property here> if Gal$(K/F)$ is <insert nice property here>. (cyclic, nilpotent, solvable,...)
\end{Def}

\begin{Ex}
$\Q(\sqrt{2},\sqrt{3})/\Q$ is abelian, $\Q(\sqrt[3]{2},e^{2\pi i/3})/\Q$ is not. $\F_{p^n}/\F_p$ is cyclic (Gal$(\F_{p^n}/\F_p) = \langle \phi \rangle \cong \z_n)$, where $\phi$ is Frobenius. 
\end{Ex}

\begin{Ex}
$G = $ Gal$(x^n - 1/\Q) = $Gal$(K/\Q)$, $k = \Q(\omega)$, $\omega = e^{2\pi i/n}$. $|G| = [K:\Q] = \phi(n)$. Conjugates of $\omega$ are $\omega^k$, $(k,n) = 1$, where $\omega^k$ are roots of $\Phi_n$. $\forall \phi \in G$ is defined by $\phi(\omega)$ which is one of $\omega^k$, $(k,n) = 1$. So $G = \Set{\phi_k:(k,n) = 1}$, $\phi_k(\omega) = \omega^k$. $\forall k,l$, $\phi_k\phi_l(\omega) = \phi_k(\omega^l) = \omega^{kl}$. So $\phi_k\phi_l = \phi_{kl}$. So $G \cong \z_n^*$. $\forall n$. then $n$-th cyclotomic polynomial extension is abelian. 
\end{Ex}

Let $K/F$ be Galois, $\gal (K,F) = G$. Let $L/F$ be a subextension, $F\sub K \sub K$. We have:
\begin{center}
\begin{tikzcd}
K \arrow[dd, "G"', no head, bend right=49] \arrow[d, no head, bend left=49] \\
L \\
F
\end{tikzcd}.
\end{center}

Then $K/F$ is Galois. Let $H = \gal (K/L)$. We have:
\bee
H &= \Set{\phi:K \to K : \phi \text{ fixes }K:\phi(\alpha) = \alpha, \forall \alpha \in L}\\
\leq G &= \Set{\phi:K \to K: \phi \text{ fixes }F:\phi(\alpha) = \alpha \forall \alpha \in F}.
\eee
Note $H \leq G$. We also have:
\bee
|H| = [K:L] = \fracc{[K:F]}{[L:F]} = \fracc{|G|}{[L:F]}.
\eee
So $[L:F] = |G:H|$. 

And $L \Rightarrow H = \gal (K/L) \leq G$. 


Let $H\leq G$. 
\begin{Def}
$\fix(H)  = \Set{\alpha \in K:\phi(\alpha) = \alpha, \forall \phi \in H}$. $\fix(H)$ is a subfield of $K$ and contains $F$. So $H \Rightarrow L = \fix(H) \sub K$. 
\end{Def}

\begin{theorem}[\textbf{Fundamental Galois theorem (short version)}]
These two operations: 
\bee
K \mapsto H = \gal(K/L)
H \mapsto K = \fix(H)
\eee
are inverses of each other. So, subextensions $L/F$ of $K/F$ are in bijection with subgroups $H \leq G$. Note $L \sub K$ and $H \sub G$. 
\end{theorem}

\begin{proof}
Just the idea of the proof. Consider:
\bee
L \mapsto H = \gal(K/L) \mapsto \fix(H) \overset{?}{=}L.
\eee
Note $L \sub \fix(H)$. And $[K:L] = n \Rightarrow|H| = n$. So it is enough to show that $[K:\fix(H)] = n$. We must also sow that 
\bee
H \mapsto L = \fix(H) \mapsto \gal(K/L) \overset{?}{=} H.
\eee
We know $H \sub \gal(K/L)$. We know $|H| = n$. $[L:K] \Rightarrow |\gal(K/L)| = [K:L]$. If we prove that $[K:L] = n$, we are done, where $L = \fix(H), n = |H|$. 
\end{proof}

\begin{Prop}
Let $K$ be a field, let $G \leq \aut(K)$, finite, let $F = \fix(G)$, then $[K:F] = |G|$. 
\end{Prop}


\bb

\textbf{Friday, March 6th}

\begin{theorem}[\textbf{Galois theorem - full version}]\index{Galois!theorem}
Let $K/F$ be a Galois extension, let $g = \gal(K/F)$. Then \begin{enumerate}
\item The correspondence: subextension $L/F \leftrightarrow$ subgroup $H \leq G$ defined by $H = \gal(K/L)$, $L = \fix(H)$ is injective with  $|H| = [K:L],|G:H| = [L:F]$. We postpone the proof until Monday. The idea is to prove that if we define subextension this way, then the degree of $K/L$ will be exactly the order of $H$, and this is the key point of the proof. 
\item If $L_1 \leftrightarrow H_1, L_2 \leftrightarrow H_2$, then $L_1 \sub L_2$ if and only if $H_1 \geq H_2$ and $[L_2:L_1] = |H_1:H_2|$. So there exists only finitely many subextensions of $K/F$, and the diagram of subextensions is the same as the diagram of subgroups of $G$ drawn \textbf{upside down}:
\begin{center}
\begin{tikzcd}
 & 1 \arrow[ld, "n_1"', no head] \arrow[d, "n_2"', no head] \arrow[rd, "n_4", no head] &  &  &  & K \arrow[ld, "n_1"', no head] \arrow[d, "n_2", no head] \arrow[rd, "n_4", no head] &  \\
H_1 \arrow[rd, "m"', no head] & H_2 \arrow[d, no head] & H_4 \arrow[ldd, no head] & \leftrightarrow & L_2 \arrow[rd, "m"', no head] & L_2 \arrow[d, no head] & L_4 \arrow[ldd, no head] \\
 & H_3 \arrow[d, no head] &  &  &  & L_3 \arrow[d, no head] &  \\
 & G &  &  &  & F & 
\end{tikzcd}.
\end{center}
\item If $L_1 \leftrightarrow H_1, L_2 \leftrightarrow H_2$, then $L_1 \cap L_2 \leftrightarrow \langle H_1,H_2 \rangle$ and $L_1L_2\leftrightarrow H_1 \cap H_2$. And the following diagram is the proof:
\begin{center}
\begin{tikzcd}
 & L_1L_2 \arrow[rrr] \arrow[ld, no head] \arrow[rd, no head] &  &  & H_1 \cap H_2 \arrow[lll] \arrow[ld, no head] \arrow[rd, no head] &  \\
L_1 \arrow[rd, no head] &  & L_2 \arrow[ld, no head] & H_1 \arrow[rd, no head] &  & H_2 \arrow[ld, no head] \\
 & L_1\cap L_2 &  &  & \langle H_1,H_2 \rangle & 
\end{tikzcd}.
\end{center}
Since $L_1 \cap L_2$ is the max subfield contained in $L_1$ and $L_2$, it corresponds to the minimum subgroup countaining $H_1$ and $H_2$, which is $\langle H_1, H_2 \rangle$. And...

\item If $L \leftrightarrow H$, then any embedding $L \leftrightarrow L$ is given by some $\phi \in G$. $\phi_1,\phi_2 \in G$ define the same embedding $\phi|_{L} = \phi_2|_L$ if and only if $\phi_1 = \phi_2 \mod H$. So embeddings $\leftrightarrow$ cosets $G/H$. 

 \item Any conjugate of $L$ (result of an embedding) is of the form $\phi(L)$, $\phi \in G$. The subgroup, corresponding to $\phi(L)$ is $\phi H \phi^{-1}$. So conjugate subextensions $\leftrightarrow$ conjugate subgroups:
$$
\phi(L) \leftrightarrow \phi H\phi^{-1}.
$$

\item If $L \leftrightarrow H$, then $L$ is normal if and only if $H \norm G$. In this case, $L/F$ is Galois, and $\gal(L/F) = G/H$. If $L$ is normal, then the extension from $F$ to $L$ is normal. 
\end{enumerate}
\end{theorem}
\begin{proof}
(2) If $L_1 \sub L_2$, then $H_1 = \gal(K/L_1) = \Set{\phi \text{ fixes }L_1}$. And $H_2 = \gal(K/L_2) = \Set{\phi \text{ fixes } L_2}$. Then we have:
\bee
|H_1:H_2| = \fracc{|H_1|}{|H_2|} = \fracc{[K:L_1]}{[K:L_2]} = [L_2:L_1].
\eee
\end{proof}

\begin{proof}
(4) If you have some $\phi:L \to K$, then observe:
\begin{center}
\begin{tikzcd}
K \arrow[r, "\overset{\cong}{\phi \in G}"] \arrow[d, no head] & K \arrow[d, no head] \\
L \arrow[r, "\overset{\cong}{\phi}"] & L'
\end{tikzcd}.
\end{center}
And the number of embeddings $L \to K$ given by elements of $G$ is $|G:H|$. But the number of embeddings from $L \to K$ is just the degree of $L$ which is $[L:F] = |G:H|$. 
\end{proof}

\begin{proof}
(5) $\forall \alpha \in L$, $\forall \psi \in H$:
\bee
\phi\psi\phi^{-1}(\phi(\alpha)) = \phi(\psi(\alpha)) = \phi(\alpha).
\eee
So , $\phi H \phi^{-1}$ fixes $\phi(L)$. 
\end{proof}


\begin{proof}
(6) $L$ is normal if and only if $\phi(L) = L$ $\forall \phi \in G$ if and only if $\phi H \phi^{-1} = H$ $\forall \phi \in G$ if and only if $H \norm G$. Let's use the first isomorphism theorem. Let $D = \gal(L/F)$. $\forall \phi \in G$, $\phi(L) = L$, so $\phi|_L \sub D$, so we have a homomorphism $G \to D$ given by $\phi \mapsto \phi|_L$. It is surjective since $\forall$ element if $D$ extends to some $\phi \in G$. Its kernel is $H = \gal(K/F)$. So $D \cong G/H$. 
\end{proof}


\begin{Ex}
\begin{enumerate}
\item 
$K = \Q(\sqrt{2},\sqrt{3})$. $\gal(K/\Q) \cong V_4 = \Set{1,\phi_1,\phi_2,\phi_3}$. $1 = Id_K$. 
\bee
\phi_1: \begin{cases}
\sqrt{2} \mapsto -\sqrt{2}\\
\sqrt{3} \mapsto \sqrt{3}
\end{cases}
\phi_2: \begin{cases}
\sqrt{2} \mapsto \sqrt{2}\\
\sqrt{3} \mapsto -\sqrt{3}
\end{cases}
\phi_3 = \phi_1\phi_2: \begin{cases}
\sqrt{2} \mapsto -\sqrt{2}\\
\sqrt{3} \mapsto -\sqrt{3}
\end{cases}
\eee

We have:
\begin{center}
\begin{tikzcd}
 & 1 \arrow[rd, "2", no head] \arrow[d, "2", no head] \arrow[ld, "2"', no head] &  &  & K \arrow[ld, "2"', no head] \arrow[d, "2", no head] \arrow[rd, "2", no head] &  \\
\langle \phi_1 \rangle \arrow[rd, "2"', no head] & \langle \phi_2 \rangle \arrow[d, "2", no head] & \langle \phi_3 \rangle \arrow[ld, "2", no head] & L_1 \arrow[rd, "2"', no head] & L_2 \arrow[d, "2", no head] & L_3 \arrow[ld, "2", no head] \\
 & G &  &  & \Q & 
\end{tikzcd}.
\end{center}

$L_1 = \fix(\langle \phi_1 \rangle) = \fix( \phi_1) = \Q(\sqrt{3})$. And $L_2 = \fix(\phi_2) = \Q(\sqrt{2})$. And $L_3 = \fix(\phi_3) = \Q(\sqrt{6})$. 
\item $K = \Q(\sqrt[3]{2},\omega)$, $\omega = e^{2\pi i/3}$. $\gal(K/\Q) \cong S_3$. 
\end{enumerate}
\end{Ex}

\begin{rem}
Roots of coprime elements are rationally independent over $\Q$. 
\end{rem}


\bb

\textbf{Monday, April 9th}

\begin{rem}
$K/F$ is Galois if and only if $K$ is a splitting field of a separable polynomial $f \in F[x]$. 
\end{rem}

\begin{proof}
Just the idea. So $K = F(\alpha_1,...,\alpha_k)$  such that $\alpha_i$ are separable and $K$ contains all of their conjugates. So $K/F$ is Galois if all elements are separable and all their conjugates are in $K$. We claim that it is sufficient to just show this on the generators. This is a nontrivial fact. This is a sort of counting. We prove that if we have $K = (\alpha_1,..,\alpha_k)$, then $|\aut(K/F)| = [K:F]$, and from this it follows that all elements of $K$ are ``good" (separable and all conjugates are in $K$), and $K/F$ is thus Galois. So if there is a bad element, then we can construct a tower which doesn't have enough automorphisms. 
\end{proof}



\textbf{Tuesday, April 10th}

We have $Tu_i = \lambda_i u_i$. 
\bee
\lambda_1(a_1u_1 + \cdots + a_nu_n = 0\\
T:a_1\lambda_1u_1 + \cdots a_n\lambda_nu_n = 0.
\eee

Composite of $K_1,K_2$. Let $K/F$ be Galois, let $K_1/F,K_2/F$ be subextensions. We have:
\begin{center}
\begin{tikzcd}
 & K_1K_2 \arrow[ld, "m_2"', no head] \arrow[rd, "m_1", no head] &  &  & K_1K_2 \arrow[ld, "\leq l_2"', no head] \arrow[rd, "\leq l_1", no head] &  \\
K_1 \arrow[rd, "n_1"', no head] &  & K_2 \arrow[ld, "n_2", no head] & K_1 \arrow[rd, "l_1", no head] \arrow[rdd, "n_1"', no head] &  & K_2 \arrow[ld, "l_2"', no head] \arrow[ldd, "n_2", no head] \\
 & F &  &  & L \arrow[d, no head] &  \\
 &  &  &  & F & 
\end{tikzcd}.
\end{center}
Where $m_2 \leq n_2,m_1\leq n_1$. Why can it be that $m_1 < n_1,m_2< n_2$? One reason: $K_1 \cap K_2 \neq F$. Then $m_2 \leq l_2 < n_2$, and $m_1 \leq l_1 < n_1$. Example:
\begin{center}
\begin{tikzcd}
 & \Q(\omega,\sqrt[3]{2}) \arrow[ld, "2"', no head] \arrow[rd, "2", no head] &  \\
\Q(\sqrt[3]{2}) \arrow[rd, "3"', no head] &  & \Q(\omega\sqrt[3]{2}) \arrow[ld, "3", no head] \\
 & \Q & 
\end{tikzcd}.
\end{center}
Neither of these extensions are normal. 
But what if $K_1 \cap K_2 = F$?

Note $G = \langle H_1,H_2 \rangle \geq H_1H_2$. And $H_1 \cap H_2 = 1$. We have $|H_1| = m_2,|H_2| = m_2$. If $G > H_1,H_2$, which is not a group, then $n_1 > m_1,n_2> m_2$. But if $H_1 \norm G$, then $G = \langle H_1,H_2 \rangle = H_1H_2$, $|G| = m_1m_2$. 

We have for the Galois group:
\begin{center}
\begin{tikzcd}
 & 1 \arrow[ld, "m_2"', no head] \arrow[rd, "m_1", no head] &  \\
H_1 \arrow[rd, "n_1"', no head] &  & H_2 \arrow[ld, "n_2", no head] \\
 & G & 
\end{tikzcd}.
\end{center}

\begin{theorem}
IF $K_1/F,K_2/F$ are subextensions of a Galois extension $K_1 \cap K_2 = F$, and $K_1/F$ is normal, then $[K_1K_2:F] = [K_1:F][K_2:F]$. $(m_1 = n_1,m_2 = n_2)$. 
\end{theorem}


\bb

\textbf{Monday, April 16th}

Proof that $\qwe{3} \notin \Q(\sqrt[8]{2})$?
\begin{Prop}
Let $a > 0$, and let $x^n - a$ be irreducible. Let $K  = \Q(\sqrt[n]{a})$. Then the only subfields of $K$ are of the form $\Q(\sqrt[d]{a}), d|n$. 
\end{Prop}

\begin{proof}
Let $L \sub K$, $[L:\Q] = d$ (to prove $L = \Q(\sqrt[d]{a})$). Let $f$ be the minimal polynomial of $\sqrt[n]{a}$ over $L$. Then $\deg f = \fracc{n}{d}$, and:
\bee
f \mid (x^n - a) = \prod_{k = 0}^{n - 1}(x - \omega^k\sqrt[n]{a}),
\eee
where $\omega = e^{2\pi i/n}$. So:
\bee
f(x) = \prod_{i = 1}^{n/d}(x - \omega^{k_i}\sqrt[n]{a}),
\eee
for some $k_i$. So we have:
\bee
f(x) &= x^{n/d} + \cdots \pm \omega^k \sqrt[d]{a} \in L[x], k  k_1 + \cdots + k_{n/d},
\eee
so, $\omega^k\sqrt[d]{a} \in L \sub \R$, so $\omega^k \in \R$, so $\omega^k = \pm1$. So $\pm \sqrt[d]{a} \in L$, so $\Q(\sqrt[d]{a}) \sub L$. $x^d - a$ is irreducible over $\Q$: if $x^d - a = g(x)h(x)$, then $^n - a = g(x^{n/d})h(x^{n/d})$ - impossible. So, $[\Q(\sqrt[d]{a}):\Q] = d = [L:\Q]$, so $L = \Q(\sqrt[d]{a})$. 
\end{proof}

If so ,then the only subfield of $\Q(\sqrt[8]{2})$ of degree 2 is $\Q(\qwe{2})$, but $\Q(\qwe{3}) \neq \Q(\qwe{2})$, so $\qwe{3} \notin \Q(\sqrt[8]{2})$. 

\section*{14.1 Exercises}

\begin{enumerate}[label=\arabic*.]
\counter{enumi}{5}
\item 

We have:
\bee
\aut(F(t)) = \Set{t \mapsto \fracc{at + b}{ct + d}: a,b,c,d \in F, ad - bc \neq 0}.
\eee
Here we have $f(t) \mapsto f\lpar \fracc{at + b}{ct + d} \rpar$. 

\item \textit{$\aut(\R) = ?$}

This is the trivial group. Why? Any automorphism must preserve positive numbers, since they are characterized by the square root. This property is preserved by any automorphism (having a root). $a = b^2 \Rightarrow \phi(a) = \phi(b)^2$< so $a > 0$ if and only if $\phi(a) > 0$. So $\forall \phi$ preserves the order: $a > b$ if and only if $a - b > 0$ if and only if $\phi(a - b) > 0$ if and only if $\phi(a) > \phi(b)$. And it fixes $\Q$ since each rational goes to itself. $\forall a \in \R$ is defined by $\Set{r \in \Q: r < a} = A_a$, which is preserved by $\phi$, so $\phi(a) = a$. If we have two different real numbers $a,b$, then they have two different sets of rational numbers which are less than them. Within any two real numbers, there is a rational number (density of rationals in $\R$). So $\phi(A_a) = A_{\phi(a)}$, so $\phi(a) = a$ since $\phi$ preserves order. 


Consider $\aut(\c) = \aut(\c/\Q) = $huge... Why? 
\begin{center}
\begin{tikzcd}
\Q(i) \arrow[rr, "i \mapsto -i"] \arrow[rd, no head] &  & \Q(i) \arrow[ld, no head] \\
 & \Q & 
\end{tikzcd}
\end{center}

\end{enumerate}


\section*{14.2 Exercises}

\begin{enumerate}[label=\arabic*.]

\counter{enumi}{2}
\item \textit{Determine the Galois group of $(x^2 - 2)(x^2 - 3)x^2 - 5)$. Determine all the subfields of the splitting field of this polynomial.  }


We draw the subfield lattice of $\Q(\sqrt{2},\sqrt{3},\sqrt{5})/\Q$. Note that every nonidentity element is of order 2, and the whole group is of order 8, so the Galois group is isomorphic to $\z_2^3$. 

\begin{center}
\begin{adjustbox}{center,width=10cm}
\tiny
\begin{tikzcd}
 &  &  & \mathbb{Q}(\sqrt{2},\sqrt{3},\sqrt{5}) \arrow[llld, no head] \arrow[lld, no head] \arrow[ld, no head] \arrow[rrrddd, no head] \arrow[rd, no head] \arrow[rrrd, no head] \arrow[rrd, no head] \arrow[d, no head] &  &  &  \\
\mathbb{Q}(\sqrt{2},\sqrt{3}) \arrow[rdd, no head] \arrow[rrrrdd, no head] \arrow[dd, no head] & \mathbb{Q}(\sqrt{3},\sqrt{5}) \arrow[dd, no head] \arrow[rrdd, no head] \arrow[rdd, no head] & \mathbb{Q}(\sqrt{2},\sqrt{5}) \arrow[rrrdd, no head] \arrow[lldd, no head] \arrow[dd, no head] & \mathbb{Q}(\sqrt{6},\sqrt{10}) \arrow[rdd, no head] \arrow[rrdd, no head] \arrow[dd, no head] & \mathbb{Q}(\sqrt{30},\sqrt{2}) \arrow[rrdd, no head] \arrow[lllldd, no head] \arrow[ldd, no head] & \mathbb{Q}(\sqrt{30},\sqrt{5}) \arrow[rdd, no head] \arrow[llldd, no head] \arrow[ldd, no head] & \mathbb{Q}(\sqrt{30},\sqrt{3}) \arrow[dd, no head] \arrow[llllldd, no head] \arrow[ldd, no head] \\
 &  &  &  &  &  &  \\
\mathbb{Q}(\sqrt{2}) \arrow[rrrd, no head] & \mathbb{Q}(\sqrt{3}) \arrow[rrd, no head] & \mathbb{Q}(\sqrt{5}) \arrow[rd, no head] & \mathbb{Q}(\sqrt{15}) \arrow[d, no head] & \mathbb{Q}(\sqrt{6}) \arrow[ld, no head] & \mathbb{Q}(\sqrt{10}) \arrow[lld, no head] & \mathbb{Q}(\sqrt{30}) \arrow[llld, no head] \\
 &  &  & \mathbb{Q} &  &  & 
\end{tikzcd}.
\end{adjustbox}
\end{center}




\item \textit{$p$ is prime, find $\gal(x^p - 2)$ over $\Q$. }

\begin{rem}
The Galois group of a polynomial is the Galois group of its splitting field. 
\end{rem}

So we first construct its splitting field. We have $K = \Q(\alpha, \omega)$, $\alpha = \sqrt[p]{2},\omega = e^{2\pi i/p}$. $K$ is the splitting field of $x^p - 2$. And $K = \Q(\alpha)\Q(\omega)$. 


\begin{center}
\begin{tikzcd}
 & L \arrow[ld, "p - 1"', no head] \arrow[rd, "p", no head] &  \\
\Q(\alpha) \arrow[rd, "p", no head] &  & \Q(\omega) \arrow[ld, "\phi(p) = p - 1", no head] \\
 & \Q & 
\end{tikzcd}.
\end{center}
So $[K:\Q] = p(p - 1) = |G|$. $\phi \in G$ is defined by $\phi(\alpha),\phi(\omega)$. Observe:
\bee
\phi(\alpha) &= \alpha\omega^k && \text{for some }k = 0,...,p - 1\\
\phi(\omega) &= \omega^l && \text{for some }l = 1,...,p - 1.
\eee
Let $\phi_{k,l}$ be such $\phi$. They say determine the elements of the Galois group. The elements are determined, but we do not have group yet, we need the multiplication table. So we construct it:
\bee
\phi_{k_1,l_1}\phi_{k_2,l_2}(\alpha) &= \phi_{k_1,l_1}(\alpha\omega^{k_2} = \alpha\omega^{k_1}\omega^{l_1k_2} = \alpha \omega^{k_1 + l_1k_2}\\
\phi_{k_1,l_1}\phi_{k_2,l_2}(\omega) &= \phi_{k_1,l_1}(\omega^{l_2}) = \omega^{l_1l_2}.
\eee
So $\phi_{k_1,l_1}\phi_{k_2,l_2} =\phi_{k_1 + l_1k_2 \mod p,l_1l_2\mod p}$. Recall:
\bee
\text{Hol}(\z_p) \cong \z_p\semi \z_p^*.
\eee
Observe:
\bee
(k_1,l_1)(k_2,l_2) &= (k_1 + l_1k_2,l_1l_2).
\eee
Similarly for holomorphs:
\bee
(a_1,\psi_1)(a_2,\psi_2) = (a_1 + \psi_1(a_2),\psi_1,\psi_2).
\eee
$H \semi \aut(H)$. It's important that $p$ is prime, since we need $p,\phi(p)$ to be relatively prime. 



\counter{enumi}{6}

\item \textit{$x^8 - 2$ over $\Q$. Find all subfields of the splitting field $K$ of $x^8 - 2$ which are Galois (sufficient to find normal in this case, since they are already separable) over $\Q$. }

Let $\alpha = \sqrt[8]{2},\omega = e^{2\pi i/8}$. Then $K = \Q(\alpha,\omega) = \Q(\alpha)\Q(\omega)$ (is this last equality always true?)

\begin{center}
\begin{tikzcd}
 & K \arrow[ld, "2"', no head] \arrow[rd, "4", no head] &  \\
\Q(\alpha) \arrow[rd, "4"', no head] &  & \Q(\omega) \arrow[ld, "2", no head] \\
 & \Q(\sqrt{2}) \arrow[d, "2"', no head] &  \\
 & \Q & 
\end{tikzcd}.
\end{center}

Note $\omega = \fracc{1 + i}{\sqrt{2}}, \omega^2 = i$. So $\sqrt{2} \in \Q(\omega)$. Note $\sqrt{2} = \fracc{1 + i}{\omega} = \fracc{1 + \omega^2}{\omega}$. And $\sqrt{2} = \alpha^4$. We know the top left degree is $2,$ since the extension of $K$ of over the two side fields must be nontrivial, and the degrees must be $\geq$ the degrees diagonally across from them on the diagram. Thus the other is 4. So $[K:\Q] = 16 = |G|$. Note $\Q(\alpha)/\Q$ - not normal. $\Q(\omega)/\Q$ - normal. Not normal since $\alpha \omega \notin \Q(\alpha)$. $\Q(i)/\Q$ is normal, $i = \omega^2$. 

\bb

We have:
\bee
\alpha &\mapsto \alpha\omega^k, k = 0,...,7\\
\omega &\mapsto \omega^l, l = 1,3,5,7
\eee

So let's try again. Try:
\begin{center}
\begin{tikzcd}
 & K = \Q(\alpha,i) \arrow[rd, "8", no head] \arrow[ld, "2"'] &  \\
\Q(\alpha) \arrow[rd, "8"', no head] &  & \Q(i) \arrow[ld, "2", no head] \\
 & \Q & 
\end{tikzcd}.
\end{center}
We have:
\bee
\alpha &\mapsto \alpha\omega^k\\
i &\mapsto \pm i\\
\phi: \alpha &\mapsto \alpha \omega\\
i &\mapsto i\\
\psi: \alpha &\mapsto \alpha\\
i &\mapsto -i.
\eee

And $\sqrt{2} = \alpha^4$. $\phi(\sqrt{2}) = (\alpha\omega)^4 = -\sqrt{2}$. $\phi(i) = i$, so $\phi(\omega) = -\fracc{1 + i}{\sqrt{2}} = -\omega$. Observe:
\bee
\alpha,\alpha\omega,\alpha\omega^2,\alpha\omega^3, \alpha\omega^4 = -\alpha,-\alpha\omega,-\alpha\omega^2,-\alpha\omega^3.
\eee
\bee
\phi:\alpha \mapsto \alpha\omega \mapsto -\alpha\omega^2\mapsto-\alpha\omega^3\mapsto-\alpha\omega(-\omega^3) = -2 \mapsto-\alpha\omega \mapsto \alpha\omega^2 \mapsto \alpha\omega^3 \mapsto \alpha.
\eee
So $|\phi| = 8,|\psi| = 2$. 

\bee
\psi\phi\psi^{-1} : \alpha &\overset{\phi}{\mapsto}\alpha\omega \overset{\psi}{\mapsto} \alpha \omega^3\\
i &\mapsto i
\eee

$G = \langle r,s|r^8 = s^2 = 1,srs^{-1} = r^3\rangle$. 

Not $D_{16}$. 



	
\item \textit{$K/F$ is Galois, $[K:F] = p^n$, $p$ a prime. Prove that $\forall k < n$, $K$ has a subfield $L$ such that $[L:F] = p^k$. }

\begin{proof}
Recall Sylow's theorem. $\forall k$, $G$ has a subgroup of order $p^k$. So $|G:H| = p^{n - k}$, and the corresponding subfield $L$ satisfies $[L:F] = p^{n - k}$. If $[K:F] = n$, $p^k|n$, then $\exists L$ s.t. $[K:L] - p^k$, so $[L:F] = n/p^k$. 
\end{proof}

\bb

\counter{enumi}{9}

\item \textit{Determine the Galois group of the splitting field over $\Q$ of $x^8 - 3$. }

\bb

Let $\alpha = \sqrt[8]{3}$ and $\omega = e^{2\pi i/8}$. Note:
$$
\omega^2 = e^{2\pi i/4} = e^{\pi i/2} = \sqrt{e^{\pi i}} = \sqrt{-1} = i.
$$
And $\omega = \sqrt{e^{2\pi i / 4}} =\sqrt{i} = \fracc{1 + i}{\sqrt{2}}$.  Note $\sqrt{2} = \fracc{1 + i}{\omega} = \fracc{1 + \omega^2}{\omega}$. So $\sqrt{2} \in \Q(\omega)$. Now the roots of $f(x) = x^8 - 3$ are $\alpha\omega^k$ where $k = 0,...,7$, and thus the splitting field is $\Q(\alpha,\omega)$. We write out our options for constructing the automorphisms in the Galois group. We have 8 options to map $\alpha$ to and 4 options to map $\omega$ to (since $\phi(8) = 4$):
\bee
\alpha &\mapsto \alpha\omega^k, k = 0,...,7\\
\omega & \mapsto \omega,\omega^3,\omega^5,\omega^7.
\eee
Since none of these combinations give us equivalent maps, we have exactly $8\cdot 4 = 32$ automorphisms in our group, thus $|\gal(f(x))| = |G| = 32$. So let $\phi_{k_1,l_1},\phi_{k_2,l_2} \in G$, where $\phi_{k,l}:\alpha \mapsto \alpha\omega^k,\phi_{k,l}:\omega \mapsto \omega^l$. Then we have:
\bee
\phi_{k_2,l_2}\circ\phi_{k_1,l_1}(\alpha) &= \phi_{k_2,l_2}(\alpha\omega^{k_1}) = \alpha\omega^{k_2}\omega^{k_1l_2} = \alpha\omega^{k_2 + k_1l_2}\\
\phi_{k_2,l_2}\circ\phi_{k_1,l_1}(\omega) &= \phi_{k_2,l_2}(\omega^{l_1}) = \omega^{l_1l_2}.
\eee
Thus $\phi_{k_2,l_2}\circ\phi_{k_1,l_1} = \phi_{k_2 + k_1l_2 \mod 8,l_1l_2\mod 8}$, and this multiplication rule completely defines the Galois group. Furthermore, from this we see $G \cong \z_8 \semi V_4$, the nontrival semidirect product of $\z_8$ and the Klein 4-group. 

\counter{enumi}{11}

\item \textit{$G = \gal(x^4 - 14x^2 + 9)$. }

Call it $f(x)$. The roots of $f$ are $\alpha = \sqrt{7 + 2\sqrt{10}}, -\alpha$, $\beta = \sqrt{7 - 2\sqrt{10}},-\beta$. $K = \Q(\alpha,\beta)$. Observe:
\begin{center}
\begin{tikzcd}
 & K \arrow[ld, no head] \arrow[rd, no head] &  \\
\Q(\alpha) \arrow[rd, "4"', no head] &  & \Q(\beta) \arrow[ld, "4", no head] \\
 & \Q & 
\end{tikzcd}.
\end{center}

But $\alpha\beta = 3$, since $\alpha^2\beta^2 = 49 - 40= 9$. So $\Q(\alpha) = \Q(\beta)$. So $[K:\Q] = 4$. So $G \cong \z_4$ or $V_4$. 
\bee
\phi:\alpha &\mapsto -\alpha\\
-\alpha &\mapsto \alpha\\
\beta = 3/\alpha &\mapsto -\beta\\
-\beta &\mapsto \beta\\
\phi:\alpha &\mapsto \beta\\
-\alpha &\mapsto -\beta\\
\beta = 3/\alpha &\mapsto 3/\beta = \alpha\\
-\beta &\mapsto -\alpha.
\eee
Since $\phi^2 = 1$. So it's $V_4$. 

\item \textit{Prove that if the Galois group of the splitting field of a cubic over $\Q$ is the cyclic group of order 3, then all the roots of the cubic are real. }

\begin{proof}
Suppose the Galois group of the splitting field of a cubic $f(x)$ is $\z_3$. Note since this is a group of the form $\z_p$ for $p$ prime, we know that it has non nontrivial proper subgroups. Suppose we had an non-real root. Then we know that if $K/\Q$ is the splitting field of $f$, then $i \in K \Rightarrow$ $\Q(i)/\Q$ is a subextension of $K$. But note that $\Q(i)/\Q$ has degree 2, and the Galois theorem gives us a bijection between nontrivial subgroups of the Galois group and nontrivial subextensions, hence $[K:\Q] = 3$. If $\Q(i)$ were a subextension of $K$, we would have $2|3$, a contradiction, so all roots must be real. 
\end{proof}

\item \textit{Show that $K = \Q(\qwe{2 + \qwe{2}})$ is a cyclic quartic field, i.e., is a Galois extension of degree 4 with a cyclic Galois group. }

\begin{proof}
Recall that an extension is Galois if and only if it is the splitting field of a separable polynomial. Note that $\alpha = \qwe{2 + \qwe{2}}$ is a root of $f(x) = (x^2 - 2)^2 - 2 = x^4 - 4x^2 + 2$. We show that this is the minimal polynomial of $\alpha$ by showing it is irreducible. Note that $2 \nmid 1$, the leading coefficient, and $2|-4,2$, and $2^2\nmid 2$, so it is irreducible by Eisenstein's criterion. So $\deg \alpha = 4 \Rightarrow [K:\Q] = 4$. Note the roots of $f$ are $\pm \qwe{2 \pm \qwe{2}}$, and so it has no multiple roots $\Rightarrow$ it is separable. So we need only prove that $K$ is the splitting field of $f$. Clearly we have $x - \alpha$ and $x + \alpha$ for the roots of the form $\pm\qwe{2 + \qwe{2}}$. So we need only show $\qwe{2 - \qwe{2}} \in K$. Note since $\alpha^2 = 2 + \qwe{2}$, we know $\qwe{2} \in K$. But $\fracc{\qwe{2}}{\qwe{2 + \qwe{2}}} \fracc{\qwe{2 - \qwe{2}}}{\qwe{2 - \qwe{2}}} = \fracc{\qwe{2}\qwe{2 - \qwe{2}}}{\qwe{4 - 2}} =\qwe{2 - \qwe{2}} = \beta$, thus $\pm\beta \in K$, and hence $K$ is the splitting field of $f$, so $K$ is Galois. Now note since all 3 conjugates of $\alpha$ also have degree 4, we know that all automorphsims of $K$ are given by $\alpha \mapsto \alpha,-\alpha, \beta, -\beta$. Denote these by $1,\phi_1...,\phi_3$, respectively. Then we know:
\bee
\beta = \fracc{\alpha^2 - 2}{\alpha}.
\eee
So:
\bee
\phi_2(\beta) = \phi_2\lpar\fracc{\alpha^2 - 2}{\alpha}\rpar = \fracc{\beta^2 - 2}{
\beta} = -\fracc{\qwe{2}}{\beta} = -\alpha.
\eee
Thus the order of $\phi_2$ is $> 2$ which means it must be 4 since our group has order 4, so we know that our group must be isomorphic to $\z_4$.
\end{proof}

\item \textit{(Biquadratic Extensions) Let $F$ be a field of characteristic $\neq 2$. }
\begin{enumerate}
\item \textit{if $K = F(\qwe{D_1},\qwe{D_2})$ where $D_1,D_2 \in F$ have the property that none of $D_1,D_2,D_1D_2$ is a square in $F$, prove that $K/F$ is a Galois extension with $\gal(K/F)$ isomorphic to the Klein 4-group. }

\begin{proof}
Since $D_1,D_2$ are not squares, we know $F(\qwe{D_1})/F$ and $F(\qwe{D_2})/F$ are both extensions of degree 2. And since $D_1D_2$ is not a square, we know that $F(\qwe{D_2})/F(\qwe{D_1})$ is a nontrivial extension (has degree 2). Thus we know that $K/F$ has degree 4.  We wish to first show it is Galois. Let $\alpha = \qwe{D_1},\beta = \qwe{D_2}$. Then $m_{\alpha,F} = x^2 - D_1,m_{\beta,F} = x^2 - D_2$. And since the roots of these are $\pm\alpha,\pm \beta$, we know they are both separable, so $\alpha,\beta$ are separable, so $K/F$ is separable. Also, $K$ is normal since the only conjugates of $\alpha,\beta$ are $-\alpha,-\beta$, so $K/F$ is normal and separable, thus it is Galois. We enumerate the automorphisms of $K$ in the Galois group $G$. We have choices of mappings:
\bee
\alpha &\mapsto \alpha,-\alpha\\
\beta &\mapsto \beta,-\beta.
\eee
So we have:
\bee
1&:\alpha \mapsto \alpha, && \beta \mapsto \beta\\
\phi_1&:\alpha \mapsto -\alpha, && \beta \mapsto \beta\\
\phi_2&:\alpha \mapsto \alpha,  && \beta \mapsto -\beta\\
\phi_3&: \alpha \mapsto -\alpha,  && \beta \mapsto -\beta.
\eee
Clearly, each has order 2, so it is $V_4$. 
\end{proof}
\bb

\item \textit{Conversely, suppose $K/F$ is a Galois extension with $\gal(K/F)$ isomorphic to $V_4$. Prove that $K = F(\qwe{D_1},\qwe{D_2})$ where $D_1,D_2 \in F$ have the property that none of $D_1,D_2,D_1D_2$ is a square in $F$. }

\begin{proof}
Suppose $K/F$ is Galois with $G = \gal(K/F)$ isomorphic to $V_4$. By the Galois theorem, we know that the subgroup lattice of $G$ is in (flipped) bijection with the subextension lattice of $K/F$. So since we know the lattice of $V_4$, we know the structure of the subextensions of $K$:
\begin{center}
\begin{tikzcd}
 & 1 \arrow[ld, "2"', no head] \arrow[d, "2", no head] \arrow[rd, "2", no head] &  &  & K/F \arrow[ld, "2"', no head] \arrow[d, "2", no head] \arrow[rd, "2", no head] &  \\
A \cong \z_2 \arrow[rd, "2"', no head] & B \cong \z_2 \arrow[d, "2", no head] & C \cong \z_2 \arrow[ld, "2", no head] & L_1 \arrow[rd, "2"', no head] & L_2 \arrow[d, "2", no head] & L_3 \arrow[ld, "2", no head] \\
 & V_4 &  &  & F & 
\end{tikzcd}.
\end{center}
Since they are of degree 2, all of $L_1,L_2,L_3$ must be of the form $F(\qwe{D_i})$ for some $D_i$ not a square in $F$. Furthermore $D_iD_j$ cannot be a square in $F$, otherwise $L_i,L_j$ are the same extension, which is a contradiction. 
\end{proof}
\end{enumerate}



\counter{enumi}{15}

\item \textit{Find the Galois group of $x^4 - 2x^2 - 2$. }

Roots: $\alpha_1 = \sqrt{1 + \sqrt{3}},-\alpha_1,\alpha_2 = \sqrt{1-  \sqrt{-3}},-\alpha_2$. We have the splitting field:
\begin{center}
\begin{tikzcd}
 & K = \Q(\alpha_1,\alpha_2) \arrow[ld, "2"', no head] \arrow[rd, "2", no head] &  \\
\Q(\alpha_1) \arrow[rd, "2"', no head] &  & \Q(\alpha_2) \arrow[ld, "2", no head] \\
 & \Q(\sqrt{3}) \arrow[d, "2", no head] &  \\
 & \Q & 
\end{tikzcd}.
\end{center}

So $[K:\Q] = 8$. Note $\alpha_1$ is of degree 4 since it is root of polynomial of degree 4 which is irreducible by Eisenstein's criterion. For this reason $x^n - 2$ are all irreducible. $G = \gal(K/\Q) = ?$. Note $\alpha_1\alpha_2 = \sqrt{-2} = \beta$, because $\alpha_1^2\alpha_2^2 = -2$. Then we have:
\begin{center}
\begin{tikzcd}
 & K = \Q(\alpha_1,\beta) \arrow[ld, "2"', no head] \arrow[rd, "4", no head] &  \\
\Q(\alpha_1) \arrow[rd, "4"', no head] &  & \Q(\beta) \arrow[ld, "2", no head] \\
 & \Q & 
\end{tikzcd}.
\end{center}

Then we have:
\bee
\alpha_1 &\mapsto \pm \alpha_1,\pm \alpha_2\\
\beta &\mapsto \pm \beta.
\eee

We want an automorphism of order 4. 

Try this one:
\bee
\phi:\alpha_1 &\mapsto \alpha_2\\
\beta &\mapsto -\beta.
\eee
Then:
\bee
\alpha_2 = \fracc{\beta}{\alpha_1}\mapsto - \fracc{\beta}{\alpha_2} =-\alpha_1\\
\alpha_1 \mapsto \alpha_2 \mapsto -\alpha_1 \mapsto -\alpha_2 \mapsto \alpha_1.
\eee
So $|\phi| = 4$. 

Define:
\bee
\psi: \alpha_1 & \mapsto \alpha_1\\
\beta &\mapsto -\beta.
\eee

Then $|\psi| = 2$. So we have:
\bee
\psi\phi\psi^{-1}: \alpha_1 &\mapsto -\alpha_2\\
\beta &\mapsto -\beta,
\eee
so $\psi\phi\psi^{-1} = \phi^{-1}$. And:
\bee
\psi:\alpha_2 = \fracc{\beta}{\alpha_1} \mapsto -\fracc{-\beta}{\alpha_1} = -\alpha_2.
\eee
So:
\bee
G = \langle\phi,\psi|\phi^4 = \psi^2 = 1,\psi\phi\psi^{-1} = \phi^{-1} \rangle \cong D_8.
\eee





\end{enumerate}



























\setcounter{section}{2}
\section{Finite fields}

Let $F$ be a finite field of char $p$. Then $F$ is finite-dimensional $\F_p$ vector space ($\F_p = \z_p$). If $[F:\F_p] = n$, then $F$ is $n$-dimensional $\F_p$-vector space. So $|F| = p^n$. 


Consider the group $F^*  = F\setminus \Set{0}$ under multiplication. $|F^*| = p^n - 1$. And $F^*$ is cyclic $\cong \z_{p^n - 1}$. So $\forall \alpha \neq 0$ in $F$, we have $\alpha^{p^n - 1} = 1$. So $\alpha^{p^n}
 = \alpha$, $\forall \alpha \in F$. So, all $p^n$ elements of $F$ are roots of the polynomial $x^{p^n} - x$. So $F$ is a splitting field of $x^{p^n} - x$. 
 
 
 Let $p$ be a prime, let $n \in \n$. Consider $f = x^{p^n} - x \in \F_p[x]$. Let $K$ be the splitting field of $F$. Let $F = \Set{\alpha \in K: \alpha^{p^n} = \alpha}$. Then $F$ is a subfield of $K$. 


\begin{theorem}
For any prime $p$, for any $n \in \n$, there exists a unique field of order $p^n$, which is the splitting field $x^{p^n} - x$. 
\end{theorem}


\bb

\textbf{Wednesday, March 28th}


\begin{theorem}
Let $p$ be prime. $\forall n \in \n$, there exists a unique (up to isomorphism) field having $p^n$ elements, notation $\F_p$. ($\F_p = \z_p$, but $\F_{p^n} \neq \z_{p^n}$ for $n \geq 2$) And this field is the splitting field of $x^{p^n} - x$, and consists of the roots of this polynomial. 
\end{theorem}

\begin{rem}
$\F_p^n \cong \F_p \oplus \cdots \oplus \F_p$ as an $\F_p$-module. 
\end{rem}

Now $\z_{p^n}$ is a ring, but of course it's not a field, there is not relation between it and $\F_{p^n}$. 

\begin{rem}
$\F_{p^n}^*$ is a cyclic group, $\exists \alpha \in \F_{p^n}$ such that:
$$
\Set{1,\alpha, \alpha^2,...,\alpha^{p^n - 1}} = \F_{p^n}\setminus \Set{0}.
$$
So $\F_{p^n} = \F_p(\alpha)$, so $\F_{p^n}/\F_p$ is a simple extension of degree $n$.  And $\deg_{\F_p}\alpha = n$, $\alpha$ is a root of an irreducible polynomial of degree $n$ from $\F_p$. This is because it has degree $n$ and is algebraic. We get this because the extension has degree $n$. So $\forall n$, such a polynomial exists, and $\F_{p^n}$ is the splitting field of this polynomial. So $\F_p \cong \F_p [x]/(f)$. It is not a unique polynomial, but the resulting fields will be isomorphic. 
\end{rem}

\begin{Ex}
For $p = 2$, $n = 3$, the irreducible polynomials of degree 3 are:
\bee
x^3 + x + 1, x^3 + x^2 + 1
\eee
When you factorize by these polynomials, you get the same field:
$$
\F_8 \cong \F_2[x]/(x^3 + x + 1) \cong \F_2[x]/(x^3 + x^2 + 1).
$$
\end{Ex}

So what are the relations between these fields? We have $\F_p \sub \F_{p^n}$. For what $m$ do we have $\F_{p^m} \sub \F_{p^n}$? Note $[\F_{p^m}:\F_p] = m$, and it must divide $n = [\F_{p^n}:\F_p]$ so this is a necessary condition, and in fact it is a sufficient condition as well. 

\begin{Prop}
$\forall m|n$, there exists a unique subfield of $\F_{p^n}$ isomorphic to $\F_{p^m}$. 
\end{Prop}

\begin{lem}
If $m|n$, then $x^{m - 1}|x^{n - 1}$. 
\end{lem}

\begin{proof}
Indeed, if $\fracc{n}{m} = d$, then:
$$
\fracc{x^{n - 1}}{x^{m - 1}} = 1  + x^d + \cdots + x^{m - 1}d.
$$
\end{proof}

Now we apply this Lemma twice. So, if $m|n$, then $p^{m} - 1|p^n - 1$, so $x^{p^m - 1} - 1|x^{p^n - 1} - 1$. And we could continue like this. So, $x^{p^m} - x|x^{p^n} - x$. Now $\F_{p^n}$ contains all the roots of $x^{p^n} - x$ and so all roots of $x^{p^m} - x$ which form $\F_{p^m}$. 


\begin{Ex}
Observe:
\begin{center}
\begin{tikzcd}
 &  & \F_{p^{12}} \arrow[rd, no head] \arrow[ld, no head] &  \\
 & \F_{p^6} \arrow[rd, no head] \arrow[ld, no head] &  & \F_{p^4} \arrow[ld, no head] \\
\F_{p^3} \arrow[rd, no head] &  & \F_{p^2} \arrow[ld, no head] &  \\
 & \F_p &  & 
\end{tikzcd}.
\end{center}
And this is exactly the same as the subgroup lattice for $\z_{12}$. 
\end{Ex}

\begin{rem}
Every element $\beta \in \F_{p^n}$ is a root of an irreducible over $\F_p$ polynomial of degree $m|n$. And any such polynomial splits in $\F_{p^n}$ (since the splitting field of this polynomial is $\F_{p^m} \sub \F_{p^n}$). So $x^{p^n} - x$ is the product of all monic irreducible over $\F_p$ polynomials of degree $m$ with $m|n$. 

Note $\beta$ is a root of it's minimal polynomial, which is an irreducible of degree dividing $n$. If it's not containing in the union of these subfields seen in the lattice, then it is a generating element and the polynomial has degree $n$. 

Every irreducible monic polynomial of degree $m|n$ is a factor of $x^{p^n} - x$. 
\end{rem}

\begin{rem}
$(x^{p^n} - x)' = -1$, so this polynomial has no multiple roots, so all irreducible components have exponent 1 in $x^{p^n} - x$. Note it's $-1$ since $p^n = 0$ in our field. 
\end{rem}

\begin{Ex}
$p = 2$, $n = 2$, for $m= 1$, these are $x, x+ 1$, for $m = 2$, these are just $x^2 + x + 1$, so it must be that $x^4 -x = x^4 + x = x(x + 1)(x^2 + x + 1)$, since we are over $\F_2$. 
\end{Ex}

\begin{Def}
Let $\forall n$, $\psi(n)$ be the number of monic irreducible over $\F_p$ polynomials of degree $n$. Then:
$$
\deg(x^{p^n} - x) = p^n = \sum_{m|n}m\cdot \psi(m).
$$
So:
$$\psi(n) = \fracc{1}{n}\left[ p^n - \sum_{\substack{m|n\\m<n}}m\cdot \psi(m)\right].
$$
\end{Def}

\begin{Ex}
For $n = 1$: $\psi(1) = p$ ($x - a:a \in \F_p$). 

$n = 2$, $\psi(2) = (p^2 - p)/2$. And $p = 2 \Rightarrow \psi(2) = 1$, and $p = 3 \Rightarrow \psi(2) = 3$. And for this case, they are:
\bee
\begin{cases}
x^2 + 1\\
x^2 - x - 1\\
x^2 + x - 1
\end{cases}.
\eee
\end{Ex}

\begin{rem}
Algebraic closure of $\F_p$. It's enough to adjoin all roots of all irreducible monic polynomials. But they don't form a sequence, if you want to get a sequence, you get the algebraic closure of $\F_p$ is:
\bee
\bar{\F_p} = \bigcup_{n  = 1}^\infty \F_{p^{n!}}.
\eee
And we have:
\bee
\F_p \sub \F_{p^2} \sub \F_{p^{3!}} \sub \F_{p^{4!}} \sub ...
\eee
Each element of this field is an element of one of these in this subset sequence. It wasn't even necessary for them to form a sequence to make this definition, but it's easier to see this way. 

\textbf{Paul: }Why is it factorial?
\textbf{Leibman: }We want it to form a sequence, if we don't use factorial, they form a \textbf{net}, not a sequence. We'd get something like this:
\begin{center}
\begin{tikzcd}
\ddots &  & \vdots \\
 & \F_{p^{12}} \arrow[rd, no head] \arrow[ld, no head] \arrow[ru, no head] \arrow[lu] &  \\
\F_{p^6} \arrow[rd, no head] &  & \F_{p^4} \arrow[ld, no head] \\
 & {} & 
\end{tikzcd}.
\end{center}
\end{rem}

The homework will depend on what we get through tomorrow. 





\section*{14.3 Exercises}

\begin{enumerate}[label=\arabic*.]
\counter{enumi}{3}

\item \textit{Construct a finite field of 16 elements and find a generator for the multiplicative group. How many generators are there?}

We simply need to construct an irreducible polynomial of degree 4 over $\F_2$. Consider $f(x) = x^4 + x^3 + x^2 + x + 1$. Clearly $1,0$ are not roots. So we need to check if it is divisible by any irreducible quadratics. So it would have to b $(x^2 + x + 1)^2$, as this is the only such quadratic. We have:
\bee
(x^2 + x + 1)^2 &= x^4 + x^3 + x^2 + x^3 + x^2 + x + x^2 + x + 1\\
&= x^4 + x^2 + 1.
\eee

So $f$ is irreducible. Thus $\F_2[x]/(f) \cong \F_{2^4}$, a finite field of $16$ elements. Note that the multiplicative group of this field is isomorphic to $\z_{15}$ since we have 15 nonzero elements. Since we want to know how many generators we have, recall that the generators of $\z_{15}$ are exactly those whose equivalence classes are coprime with the order. So we have $\phi(15) = 8$ generators. 

\bee
(x + 1)^2 &= x^2 + 1\\
(x + 1)(x^2 + 1) &= x^3 + x + x^2 + 1\\
(x + 1)(x^3 + x^2 + x + 1) &= x^4 + x^3 + x^2 + x + x^3 + x^2 + x + 1\\
&= x^4 + 1\\
(x + 1)^5 &= (x + 1)(x^4 + 1) = x^5 + x + x^4 + 1.\\
\eee

And since $\z_5$ is the largest subgroup in the lattice of $\z_15$, we know that the elements with largest order not equal to 15 have order 5, and this element has order $> 5$ since $(x + 1)^5 \neq 1$. So it must have order 15. Thus $x + 1$ is a generator. 


\counter{enumi}{7}

\item \textit{Determine the splitting field of the polynomial $f(x) = x^p - x - a$ over $\F_p$ where $a \neq 0,a \in \F_p$. Show explicitly that the Galois group is cyclic. [Show $\alpha \mapsto \alpha + 1$ is an automorphism.]}

\begin{proof}
Suppose $\alpha$ is a root. Then we have $\alpha^p - \alpha + a = 0$. Behold: 
\bee
(\alpha + 1)^p - (\alpha + 1) - a &= \lpar\sum_{k = 0}^p \binom{p}{k}\alpha^k\rpar - \alpha - 1 - a\\
&= \lpar \sum_{k = 1}^{p - 1} \binom{p}{k}\alpha^k \rpar + \alpha^p - \alpha - a\\
&= \sum_{k = 1}^{p - 1} \binom{p}{k}\alpha^k \\
&= \sum_{k = 1}^{p - 1} \fracc{p!}{k!(p - k)!}\alpha^k.
\eee
We claim that $\fracc{p!}{k!(p - k)!}$ is divisible by $p$ for all integer values of $k$ in the range $[1,p - 1]$. Note for these values of $k$ that $p\nmid (k!(p - k)!)$ but that $p|p!$, and the binomial coefficient is an integer, so we must have that $p|\lpar \fracc{p!}{k!(p - k)!} \rpar $. Thus: 
$$
\sum_{k = 1}^{p - 1} \fracc{p!}{k!(p - k)!}\alpha^k \mod p \equiv 0.
$$
And since we are over $\F_p$, we know that $\alpha + 1$ must then be a root. The roots of $f$ are $\alpha + k$ for $k = 0,...,p - 1$, hence $f$ is separable, and so $\F_p(\alpha)$ is the splitting field of a separable polynomial, and thus is Galois. And we have an automorphism $\phi:\alpha \mapsto \alpha + 1$ because an inverse is given by $\alpha \mapsto \alpha - 1 = \alpha + p - 1$, these are both field homomorphisms, and they map $\F_p(\alpha) \to \F_p(\alpha)$, so then $G$ must be cyclic since any other automorphism maps $\alpha \mapsto \alpha + k$ which is $\phi^k$. 
\end{proof}
\end{enumerate}










\section{Composite extensions and simple extensions}

\textbf{Wednesday, April 11th}


Consider $\qwe{p_1},\qwe{p_2},...,\qwe{p_k}$, where $\pi \in \n$, primes. 

\begin{Claim}
If $n_1,...,n_l \in \n$ are square-free and distinct, then $\qwe{n_1},...,\qwe{n_l}$ are $\Q$-linearly independent. 
\end{Claim}

\begin{proof}
Let $p_1,...,p_k$ be all prime divisors of $n_1,...,n_l$:
\bee
n_1,...,n_l \in \Set{\prod p_{i_1}\cdots p_{i_j}},
\eee
where $\prod_{i \in S}p_i = p_S, S \sub \Set{1,...,K}$. So $p_{\Set{1,3,4}} = p_1p_3p_4$. 

$\Leftarrow$\begin{Claim}
If $p_1,...,p_k$ are distinct primes, then $\Set{\qwe{p_S}:S \sub \Set{1,...,k}}$ are $\Q$-linearly independent. 
\end{Claim}
$\Uparrow$

\begin{Claim}
\bee
\left[\Q\lpar \qwe{p_1},...,\qwe{p_k} \rpar :\Q \right] = 2^k.
\eee
\end{Claim}

This claim gives us $\gal(\Q()/\Q) \cong \z_2^k$. 

\begin{proof}
Induction on $k$. Assume true for $p_1,...,p_k$. Let $p_{k + 1}$ be another prime. Assume that $\qwe{p_{k + 1}} \in \Q(\qwe{p_1},...,\qwe{p_k})$. So $\qwe{p_{k + 1}}= \sum_{S \sub\Set{1,...,k}}a_s \qwe{p_S}, a_S \in \Q$. If $a_S \neq 0$ for only one $S$, then $p_{k + 1} = a_S^2p_S$-impossible. Note $p_{\emptyset} = 1$. 
If $a_S \neq 0$ for at least two $S$, $a_{S_1},a_{S_2} \neq 0$, then $\exists i,S_1,S_2$ such that $i \in S_1,i \notin S_2,a_{S_1},a_{S_2}\neq 0$. Let $\phi \in \gal(\Q(\qwe{p_1},...,\qwe{p_k})/\Q) = \z_2^k$ be defined by:
\bee
\phi:\begin{cases}
\qwe{p_i}\mapsto -\qwe{p_i}\\
\qwe{p_j}\mapsto \qwe{p_j}, \forall j \neq i
\end{cases}.
\eee
Then:
\bee
\phi(\qwe{p_{S_1}}) &= -\qwe{p_{S_1}}\\
\phi(\qwe{p_{S_2}}) &= \qwe{p_{S_2}}\\
\phi(\qwe{p_{k + 1}} &= \pm \qwe{p_{k + 1}}.
\eee
So $\phi(k): \pm \qwe{p_{k + 1}} = \sum_{S}a_S(\pm\qwe{p_S})$, where we have $+$ for $\qwe{p_{S_2}}$ and $-$ for $\qwe{p_{S_2}}$. Subtract or add $(*)$ and $\phi(*)$ to kill $\qwe{p_{k + 1}}$ then we will have a nontrivial linear combination of $\qwe{p_S}$ which is $=0$, contradiction. 
\end{proof}
\end{proof}

Note $\qwe{7} = a_0 + a_1 \qwe{3} + a_2 \qwe{15}$. 

Observe:
\begin{center}
\begin{tikzcd}
 & K = K_1K_2 \arrow[ld, "n_2"', no head] \arrow[rd, "n_1", no head] &  &  &  & 1 \arrow[ld, "n_2"', no head] \arrow[rd, "n_1", no head] &  \\
K_1 \arrow[rd, "n_1"', no head] &  & K_2 \arrow[ld, "n_2", no head] & \leftrightarrow & H_1 \arrow[rd, "n_1"', no head] &  & H_2 \arrow[ld, "n_2", no head] \\
 & F = K_1\cap K_2 &  &  &  & G & 
\end{tikzcd}.
\end{center}
We have $H_1\cap H_2 = 1$, $\langle H_1,H_2 \rangle  = G$. If $H_1 \norm G$, then $G = H_1H_2$. 

\begin{rem}
If $H_1,H_2 \norm G$, then $G = H_1 \times H_2$. 


So if $K_1/F,K_2/F$ are both normal, then:
\bee
\gal(K/F) \cong \gal(K/K_1)\times \gal(K/K_2).
\eee
Also, $G/H_1 \cong H_2,G/H_2 \cong H_1$. So:
\bee
\gal(K_1/F)& \cong \gal(K/K_2)\\
\gal(K_2/F)& \cong \gal(K/K_1).
\eee
So $\gal(K/F) \cong \gal(K_1/F) \times \gal(K_2/F)$. 
\end{rem}

Observe:
\begin{center}
\begin{tikzcd}
 & \Q\lpar \qwe{p_1},...,\qwe{p_k} \rpar  =K \arrow[ld, no head] \arrow[rd, no head] &  \\
\Q(\qwe{p_1}) \arrow[rd, "2"', no head] & \cdots \arrow[d, no head] & \Q(\qwe{p_k}) \arrow[ld, no head] \\
 & \Q & 
\end{tikzcd}.
\end{center}
Since $[K:\Q] = 2^k$, $\Q(\qwe{p_i}) \cap \Q(\qwe{p_1},..\cancel{\qwe{p_i}}..,\qwe{p_k}) = \Q$. S:
 \bee
 \gal(\Q(\qwe{p_1},...,\qwe{p_k})/\Q) \cong \prod_{i = 1}^k \gal(\Q(\qwe{p_i})/\Q)\cong \z_2^k.
 \eee
 
 Consider:
 \bee
 \Q(\qwe{p_1},...,\qwe{p_k}) \overset{\supseteq}{=} \Q(\qwe{p_1}+ \cdots + \qwe{p_k} = \alpha).
 \eee
 Then $\alpha$ has $2^k$ conjugates: $\pm \qwe{p_1},...,\pm \qwe{p_k}$ - all distinct. So $\deg_{\Q}\alpha = 2^k$, so $\Q(\alpha) = \Q(\qwe{p_1},...,\qwe{p_k})$. 
 
 \begin{theorem}
 If $K/F$ is finite and separable, then it has only finitely many subextensions. 
 \end{theorem}
 
 \begin{proof}
 If it is Galois, then subextensions correspond to subgroups, and finitely many subgroups gives us finitely many subextensions. But we do not assume this. Let $E/F$ be the Galois closure (= normal closure) of $K/F$. Then $E/F$ has only finitely many subextensions (since $\gal(E/F)$ has only finitely many subgroups), so $K/F$ has finitely many subextensions. 
 \end{proof}
 
 If this is not separable, it is not true, it may be that there are infinitely many subgroups. 
 

 
 \begin{theorem}[\textbf{On the primitive element}]\index{theorem!on the primitive element}
 If $K/F$ is finite and separable, then there is $\alpha \in K$ such that $K = \F(\alpha)$. ($\alpha$ is called ``primitive" for K.)
 \end{theorem}
 
 \begin{proof}
 \textbf{Case 1: }$F$ is finite.
 
 \bb
 Then $K$ is also finite, $K = \F_{p^r}$ for some $p,r$, and then $K = \F_p(\alpha)$, where $\alpha$ is a generator of $K^*$. For finite fields we know they are simple over $\F_p$. 
 
 \bb
 
 \textbf{Case 2: }$F$ is infinite. 
 
 \bb
 
 Then $K$, as an $F$-vector space, is not a union of finitely many proper subspaces. So there is no $\alpha$ which does not belong to any subextension, so $K = F(\alpha)$. (\textbf{not sure if this statement is correct, please help})
 \end{proof}
 
 
 \textbf{Thursday, April 12th}
 
 
 There was a question about nonexistence of a primitive element in a non-separable extension. 
 
 \begin{Ex}
 Let $K = \F_p(x,y)$, $F = \F_p(x^p,y^p)$. $K/F$ is non-separable, and $[K:F] = p^2$. Note $x$ satisfies $t^p - x^p$. $y$ satisfies $t^p - y^p$. So total degree is $p^2$. The property is that $\forall \alpha \in K\setminus F$, $[F(\alpha):F] = p$. So $\alpha$ is not primitive. There are infinitely many subfields. What is $\alpha$? 
 \bee
 \alpha &= \fracc{f(x,y)}{g(x,y)}, f,g \in \F_p(x,y)\\
 \alpha^p &= \fracc{f(x^p,y^p)}{g(x^p,y^p)} \in F.
 \eee
 \end{Ex}
 
 When Professor Leibman proved the theorem of the primitive element, we used the following Lemma:
 \begin{lem}
 Let $F$ be an infinite field, $V$ is an $F$-vector space, and $V_1,...,V_k$ be proper subspaces. Then $V \neq \bigcup_{i = 1}^k V_i$. 
 \end{lem}
 
 \begin{proof}
 Assume $V_1 \nsubseteq \bigcup_{i = 2}^k V_i$. If it belongs to the union then we can nix it from the list. Take $u \in V_1\setminus \bigcup_{i = 1}^k V_i$, $v \notin V_1$. Let $L = \Set{u + av:a \in F}$, the straight line through  these two points. Then $L \cap V_1 = \Set{u}$. And $\forall i = 2,...,k$, $|L\cap V_I| \leq 1$, since if $u + a_1v,u + a_2v \in V_i$, then $L \sub V_i$ (it is a straight line), but $u \in V_i$. So:
 
 \bee
 \left|L \cap \lpar \bigcup_{i = 1}^k V_i \rpar  \right| \leq k,
 \eee
 
 but $L$ is infinite, so $L \nsubseteq \bigcup_{i = 1}^k V_i$. 
 \end{proof}
 
 From this it follows that if an extension has only finitely many subextensions, then it has a primitive element. And from Galois theory, it follows that if you have a Galois extension, it has only finitely many subextensions, since the Galois group has only finitely many subgroups. 
 
 
 We discuss the fundamental theorem of algebra: that $\c$ is algebraically closed. Equivalently: $\c$ is the algebraic closure of $\R$. Any polynomial $f \in \R[x]$ splits in $\c$. 
 
 \begin{enumerate}
 \item If $\deg f$ is odd, then $f$ has a root in $\R$, so is reducible unless $\deg f = 1$. 
 \item Any quadratic extension of $\R$ is isomorphic to $\c$. 
 \item $\c$ has no nontrivial quadratic extensions. 
 \end{enumerate}
 
 $x^2 + ax + b$ means roots are $\fracc{-a \pm \qwe{a^2 - 4b}}{2} = \fracc{-a \pm ci}{2} \in \c$. If $a^2 - 4b \geq 0$ then roots are in $\R$. 
 
 \begin{proof}
 Let $f \in \R[x]$, let $K$ be it's splittinf field. Let $G = \gal(f) = \gal(K/\R)$. Let $H$ be the Sylow $2$-subgroup of $G$, so that $|G:H|$ is odd. Recall this is a maximal subgroup whose order is a power of $2$. And $|G| = 2^kp_1^{r_1}\cdots p_l^{r_l}$. $|H| = 2^k$. Let $L = \fix(H)$. Then $[L:\R] = |G:H|$ is odd. But this is impossible since take any element $\alpha \in L \setminus \R$. Then $\deg m_{\alpha,\R}$ is odd. It divides the degree of $L$. So it is reducible, impossible unless $\alpha \in \R$ (linear polynomials are irreducible). So $G = H$, and is a 2-group, $|G| = 2^k$. 
 Any 2-group has a normal series of subgroups, $1 = N_0 < N_1 < ... < N_k = G$ such that $\forall i$, $N_{i + 1}/N_i \cong \z_2$. So $K$ has a tower of subextensions:
 
 \begin{center}
\begin{tikzcd}
1 \arrow[d, no head] &  & K \arrow[d, no head] \\
N_1 \arrow[d, no head] &  & K_1 \arrow[d, no head] \\
\vdots \arrow[d, no head] &  & \vdots \arrow[d, no head] \\
G = N_k &  & K_k = \R
\end{tikzcd},
 \end{center}
 such that $\forall i$, $[K_i:K_{ i +1}] = 2$. But then $K_{k - 1} \cong \c$, so $K_{k - 2}/K_{k - 1}$ cannot exist, and then tower is just $\c/\R$. 
 
  \end{proof}
  
  
  
  
  
  
  
  
  
  
  \section*{14.4 Exercises}
  
\begin{enumerate}[label=\arabic*.]
\item \textit{Find the Galois closure of $\Q(\qwe{1 + \qwe{2}})$ over $\Q$. }

We have:
\begin{center}
\begin{tikzcd}
\Q(\qwe{1 + \qwe{2}}) \arrow[d, no head] & \Q(\qwe{1 - \qwe{2}}) \\
\Q(\qwe{2}) \arrow[d, no head] & \Q(-\qwe{2}) \arrow[u, no head] \\
\Q \arrow[ru, no head] & 
\end{tikzcd}.
\end{center}
We need to find the conjugates of the generator. So the answer is:
\bee 
K = \Q(\qwe{1 + \qwe{2}},\qwe{1 - \qwe{2}}).
\eee
Note the conjugates of $\qwe{1 + \qwe{D}}$ are $\pm\qwe{1 \pm \qwe{2}}$. Note $\alpha = \qwe{1 + \qwe{2}}$. This satisfies $x^2 - (1 + \qwe{2}) = m_{\alpha,\Q(\qwe{2})}$. Also $x^2 - (1 - \qwe{2})$. To make sure the top extension is nontrivial, we need to check that there is no element in $\Q(\qwe{2})$ such that $(a + b\qwe{2})^2 \overset{?}{=} 1 + \qwe{2}$. 
\counter{enumi}{4}
\item \textit{$p$-extensions. ($p$ is prime). }
These are polycyclic extensions where each extension has degree $p$. Equivalently, Galois closure has degree $p^k$, and Galois group is a $p$-group. Picture is the same as that for constructible numbers, except instead of 2, we have $p$. The theory is parallel to the theory of $p$-groups. Note $p$-extension of $p$-extension is $p$-extension. 

\end{enumerate}
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
 
 
 
 \section{Cyclotomic extensions and abelian extensions over $\Q$}
 
 \begin{Def}\index{abelian!extension}
 A Galois extension $K/F$ is \textbf{abelian} if $\gal(K/F)$ is abelian. 
 \end{Def}
 
 \begin{Def}\index{direct composite}
 If $G$ is abelian, $G \cong \z_{n_1} \times \cdots \times \z_{n_k}$, so if $K/F$ is abelian, then $K$ is a \textbf{direct composite}, $K = K_1K_2\cdots K_k$ of cyclic subextensions. 

 
 We have:
 \begin{center}
 \begin{tikzcd}
 &  & K \arrow[lld, no head] \arrow[ld, no head] \arrow[d, no head] \arrow[rd, no head] \arrow[rrd, no head] &  &  \\
K_1 \arrow[rrd, "n_1"', no head] & K_2 \arrow[rd, "n_2", no head] & K_3 \arrow[d, "n_3", no head] & \cdots \arrow[ld, no head] & K_k \arrow[lld, "n_k", no head] \\
 &  & F &  & 
\end{tikzcd},
 \end{center}
 such that $\forall i, K_i \cap \prod_{j \neq i}K_j = F$. Then $K_1 = \fix(\z_{n_2}\times \cdots \times \z_{n_k})$. Let's say that $K$ is a \textbf{direct composite} of $K_1$ and $K_2$ if:
$$
 [K:F] = [K_1:F][K_2:F].
$$
 
  \end{Def}
  
  \begin{rem}
  IF $K/F$ is abelian, then $\forall$ subextension $L/F$ is normal. In particular, $\forall \alpha \in K$. $F(\alpha)/F$ is normal (all conjugates of $\alpha$ are in $F(\alpha)$). 
  \end{rem}
  
  \begin{Ex}
  Cyclotomic extension $\Q(\omega)/\Q$, $\omega = e^{2\pi i/n}$. Conjugates of $\omega$ are $\omega^k$, $(k,n) = 1$. 
  \end{Ex}



We discuss cyclotomic extensions. Note:
\bee
\gal(\Q(\omega)/\Q) \cong \z_n^* \cong \z_{p_1^{r_1}}^* \times \cdots \times \z_{p_k^{r_k}}^*,
\eee
where $n = p_1^{r_1}\cdots p_k^{r_k}$. 





\section{Galois groups of polynomials}


\textbf{Monday, April 16th}

\begin{Def}\index{symmetric!polynomial}
A polynomial $f(x_1,...,x_n)$ is \textbf{symmetric} if $$f(x_{\sigma(1)},...,x_{\sigma(n)}) = f(x_1,...,x_n)$$ 
$\forall \sigma \in S_n$. 
\end{Def}

\begin{Ex}
$x_1x_2^2 + x_2x_1^2 + x-1x_3^2 + x_3x_1^2 + x_2x_3^2 + x_3x_2^2 \in F[x_1,x_2,x_3]$. 

In $F[x_1,x_2]$, $x_1^3 + x_2^3 + 2x_1x_2 - 3x_1^2x_2 - 3x_1x_2^2$. 
\end{Ex}

\begin{rem}
Elementary symmetric polynomials: 
\bee
s_1 &= x_1 + x_2 + \cdots + x_n\\
s_2 &= x_1x_2 + x_1x_3 + x_2x_3 + \cdots + x_{n - 1}x_n\\
s_3 &= \sum_{i < j < k}x_ix_jx_k,\\
& \vdots\\
s_n &= x_1x_2\cdots x_n. 
\eee
\end{rem}


\bb

\textbf{Tuesday, April 17th}

$x_1,...,x_n$ are variables. 

\begin{Def}\index{elementary symmetric polynomials}
\textbf{Elementary symmetric polynomials}: 
\bee
s_1 &= x_1 + x_2 + \cdots + x_n\\
s_2 &= x_1x_2 + x_1x_3 + x_2x_3 + \cdots + x_{n - 1}x_n\\
s_3 &= \sum_{i < j < k}x_ix_jx_k,\\
& \vdots\\
s_n &= x_1x_2\cdots x_n. 
\eee
\end{Def}


\begin{rem}
If:
\bee
f(x) &= x^n + a_{n - 1}x^{n - 1} + \cdots + a_1x + a_0\\
&= (x - \alpha_1)(x - \alpha_2)\cdots(x - \alpha_n)\\
&= x^n - (\alpha_1 + \cdots + \alpha_n)x^{n - 1} + (\alpha_1\alpha_2 + \cdots + \alpha_{n - 1}\alpha_n)x^{n - 2} - \cdots \pm \alpha_1\alpha_2\cdots\alpha_n.
\eee

So:
\bee
a_{n - 1} &= -s_1(\alpha_1,...,\alpha_n)\\
a_{n - 2} &= s_2(\alpha_1,...,\alpha_n)\\
a_{n - k} &= (-1)^ks_k(\alpha_1,...,\alpha_n).
\eee

So the coefficients of a polynomial are $\pm$ elementary symmetric polynomials of its roots. 
\end{rem}

\begin{theorem}
Any symmetric polynomial is a polynomial in elementary symmetric polynomials:
\bee
f(x_1,...,x_n) &= g(s_1(x_1,...,x_n),...,s_n(x_1,...,x_n)),
\eee
for some $g$. 
\end{theorem}

\begin{Cor}
Any symmetric expression (polynomial) of the roots of a polynomial $f = x_n + a_{n - 1}x^{n - 1} + \cdots + a_1x + a_0$ is a polynomial expression in $a_{n - 1},...,a_0$. 
\end{Cor}

\begin{Ex}
For $f = x^2 + bx + c$, with roots $\alpha_1,\alpha_2$, we have:
\bee
(\alpha_1 - \alpha_2)^2 = (\alpha_1 + \alpha_2)^2 - 4\alpha_1\alpha_2 = (-b)^2 - 4c = b^2 - 4c. 
\eee
\end{Ex}

Consider the field $K = F(x_1,...,x_n)$ - field of rational functions in $x_i$. Take:
\bee
f = (x - x_1)\cdots (x - x_n) = x^n - s_1x^{n - 1} + s_2x^{n - 2} + \cdots \pm s_n,
\eee
$f \in L[x]$, where $l = F(s_1,...,s_n)$, $s_i = s_i(x_1,...,x_n)$. Then $K$ is the splitting field of $f$ over $L$. So, $[K:F] \leq N!$. The group $S_n$ acts on $K$ by permuting $x_1,...,x_n$, and $L \sub \fix(S_n)$, which is the field of symmetric rational functions. We proved that $[K:\fix(S_n)] = |S_n| = n!$. So $L = \fix(S_n)$, so any symmetric rational function is in $L$. So we get that any symmetric rational function in $x_1,...,x_n$ is a rational function in $s_1,...,s_n$. 

\begin{Cor}
\bee
\gal(K/L) = S_n.
\eee
\end{Cor}

\begin{Cor}
For any finite group $G$, there exists a Galois extension $K/E$ with $\gal(K/E) \cong G$. 
\end{Cor}

\begin{proof}
Let $K/L$ be as above. For $n$ such that $G \leq S_n$, let $E = \fix(G)$. Then $\gal(K/E) = G$. 
\end{proof}

\textbf{Solvability of polynomials in radicals. }

Are we allowed to use $\wer[n]{1}$? We will start with allowing this, but each root of unity can be expressed as classical radicals. But let's start by assuming that all roots of unity are obtainable. We want to solve polynomials in radicals, find the roots as expressions in radicals. We will allow using roots of unity first. We are in an abstract environment, not $\c$. We start by assuming that $F$ contains roots of unity. Let $n$ be given, and let $F$ be a field of characteristic not dividing $n$. What are roots of unity in $F$? Roots of unity of degree $n$ in $F$ are roots of the polynomial $x^n - 1$. If the characteristic doesn't divide $n$, then the polynomial is separable. Assume that they are all in $F$, so we have $n$ roots of unity. They form a group under multiplication, which is cyclic. This follows from the following fact:
\begin{lem}
Any finite subgroup of $F^*$ is cyclic. 
\end{lem}

\begin{rem}
The group of roots of unity in $\c$ is not cyclic nor finite, but the group of roots on unity of degree $n$ is both. 
\end{rem}


So there is a primitive root of unity, $\omega$, such that all roots are $1,\omega,\omega^2,...,\omega^{n  -1}$. Let $a \in F$, and consider the field $K = F(\wer[n]{a})$ - a \textbf{simple radical extension}. 

\begin{theorem}
$K/F$ is Galois for the just-defined $K$, and $\gal(K/F)$ is cyclic. 
\end{theorem}

\begin{proof}
Let $\alpha = \wer[n]{a}$ - root of $x^n - a$. All other roots of $x^n - a$ are 
$$\alpha,\alpha\omega,\alpha\omega^2,...,\alpha\omega^{n - 1}.$$ So the polynomial splits in $K$, so $K/F$ is Galois. 
\end{proof}

We have automorphisms $\phi_m:K \to K$, $\alpha \mapsto \alpha\omega^m$ for some $m$, and any $\phi \in \gal(K/F)$ is of this sort. Also note:
\bee
\phi_m\phi_l = \phi_{m + l}:\alpha \mapsto \alpha\omega^{m + l}.
\eee
So we have a homomorphism $\gal(K/F) \to \z_n$ defined by $\phi_m \mapsto m$, which is injective since $\phi_m$ is uniquely defined by $m$. So, $\gal(K/F)  \cong$ a subgroup of $\z_n$, so is cyclic. This is under the assumption that $\omega \in F$. If not, we note that $\gal(x^3 - 2/\Q) \cong S_3$ - since $\omega = e^{2\pi i/3} \notin \Q$. And $\gal(x^3 - 2/\Q(\omega)) \cong \z_3$. 


It turns out that the converse is true: any cyclic extension is radical. 

\begin{theorem}
(Assuming $\omega \in F$.) Let $K/F$ be cyclic ($K/F$ is Galois, and $\gal(K/F)$ is cyclic). And also, we always assume that the characteristic of the $\mathrm{char} F \nmid n$. Then $K/F$ is simple radical: $K = F(\alpha)$ such that $\alpha^n \in F$. 
\end{theorem}

\begin{proof}\index{Lagrange resolvent}
For $\alpha \in K$, the \textbf{Lagrange resolvent} of $\alpha$ is:
\bee
(\alpha,\omega) = \alpha + \omega\phi(\alpha) + \omega^2 \phi^2(\alpha)  + \cdots + \omega^{n - 1}\phi^{n - 1}(\alpha).
\eee

Let $\phi$ be such that $\gal(K/F)$ is generated by $\phi$. So $\gal(K/F) = \Set{1,\phi,...,\phi^{n - 1}}$. 


Let $\gamma = (\alpha,\omega)$. ($\gamma \neq 0$?) Then:
\bee
\phi(\gamma) &= \phi(\alpha) + \omega\phi^2(\alpha) + \cdots + \omega^{n - 1}\phi^n(\alpha)\\
&= \omega^{-1}\gamma.
\eee
Note $\phi^n(\alpha) = \alpha$. So we have $\phi(\gamma^n) = \omega^{-n}\gamma^n = \gamma^n$. So $\gamma^n$ is fixed by $\phi$, so by $\gal(K/F)$, so $\gamma^n \in F$. 

$\gamma$ has $n$ distinct conjugates $\gamma,\omega^{-1}\gamma, \omega^{-2}\gamma,...,\omega^{-(n - 1)}\gamma$, so $\deg_F\gamma = n$, so $K = F(\gamma)$ with $\gamma^n \in F$. 
\end{proof}





\bb\bb

\textbf{Thursday, April 19th}

\begin{theorem}
$f \in F[x]$ is solvable in radicals if and only if $\gal(f/F)$ is solvable. 
\end{theorem}

\begin{proof}
\textbf{($\Rightarrow$)} Assume that all roots of $f$ are contained in polyradical extensions. If $f$ is irreducible, it's enough if just one root is contained in a polyradical extension. Take the Galois closure of the composite of all these extensions, it will be a polyradical Galois extension (because the composite of polyradical extensions is polyradical). So, let $K/F$ be a Galois polyradical extension containing the splitting field of $f$. Let $N \leq [K:F]$, let $\omega$ be a primitive root of unity of degree $n$. Let $F' = F(\omega)$, $K' = KF' = K(\omega)$. So we have the diagram:
\begin{center}
\begin{tikzcd}
 & K' \arrow[d, no head] \arrow[ld, no head] \\
K \arrow[d, "polyradical"', no head] & F' \arrow[ld, no head] \\
F & 
\end{tikzcd},
\end{center} 
so $K'/F'$ is also polyradical, since if $K_{i + 1} = K_{i}(\wer[n]{a_i})$, then $F'K_{i + 1} = F'K_i(\wer[n]{a_i})$. And $K'/F'$ is Galois, $\omega \in F'$, so $K'/F'$ is Galois polyquadratic extension, so $\gal(K'/F')$ is polycyclic = solvable. $\gal(K'/F') \norm \gal(K'/F)$ since $F'/F$ is Galois. And $\gal(K'/F)/\gal(K'/F') = \gal(F'/F)$ - abelian (cyclotomic extension). So, 
\bee
1 \to \gal(K'/F')\to \gal(K'/F)\to \gal(F'/F) \to 1
\eee
where the groups on the left and right are solvable, so the middle must be solvable. Note $\gal(f/F)$ is a factor group of $\gal(K'/F)$, so is also solvable. 
\bb

\textbf{($\Leftarrow$)} Assume $\gal(f/F)$ is solvable. Let $K$ be the splitting field of $f$, let $N = [K:F]$, let $\omega$ be a primitive root of unity of degree $N$. Let $F' = F(\omega)$, $K' = K(\omega) = KF'$. We have
\begin{center}
\begin{tikzcd}
 & K' \arrow[d, no head] \arrow[ld, no head] \\
K \arrow[d, ""', no head] & F' \arrow[ld, no head] \\
F & 
\end{tikzcd}.
\end{center} 
And $K'/F'$ is Galois. 
Now $K/F$ is polycyclic. 

\begin{Claim}
$K'/F'$ is also polycyclic. 
\end{Claim} 
\begin{lem}
If $K_2/K_1$ is cyclic, then $\forall F'$, $K_2F'/K_1F'$ is cyclic. 
\end{lem}


\begin{lem}
If $G$ is finite, $G$ is solvable if and only if it is polycyclic. 
\end{lem}
\end{proof}

\begin{proof}
Why? Because if $G$ is solvable,
\bee
1 = H_0 \propnorm H_1 \propnorm H_2 \propnorm ... \propnorm H_m = G
\eee
\bee
H_{i + 1}/H_i\text{ - abelian } \forall i
\eee
and these are products of cyclic groups and have series with cyclic factors. So we have:
\bee
N_1 \propnorm N_2 \propnorm ... = H_{i + 1}/H_i,
\eee
with $N_{j + 1}/N_j$ cyclic for all $j$. Then:
\bee
H_i \propnorm H_iN_1 \propnorm H_iN_2 \propnorm ...\propnorm H_{i+ 1} \propnorm...
\eee
\end{proof}

\begin{Def}\index{polyradical!extension}
A \textbf{polyradical }extension is an extension of the form:
\begin{center}
\begin{tikzcd}
K_n \arrow[d, no head] \arrow[ddd, "polyradical", no head, bend left] \\
\vdots \arrow[d, no head] \\
K_1 \arrow[d, no head] \\
F
\end{tikzcd}.
\end{center}
And the short lines are the simple radical extensions. 
\end{Def}

\begin{rem}
If $\gal(f/F) \cong S_n$ with $n \geq 5$, then $f$ is not solvable in radicals. 
\end{rem}

Consider polynomials of the form $f(x) = x^n + a_{n - 1}x^{n - 1} + \cdots + a_1x + a_0$, where $a_i$ are symbols. Consider $f(x) \in \Q(a_0,a_1,..,a_{n - 1})[x]$. Then we have:
\bee
\gal(f/\Q(a_0,...,a_{n - 1})) \cong S_n,
\eee
so for $n \geq 5$, the ``general" polynomial is not solvable. 

Consider $A_n \propnorm S_n$, we have $|S_n:A_n| = 2$. So, if $\gal(f/F) \cong S_n$, then $K/F$ must contain a subextension of degree $2$ (where $K$ is the splitting field of $f$). $E = F(d)$, $d^2 \in F$.  $\forall$ even $ \sigma \in S_n$, $\sigma(d) = d$. $\forall $ odd $\sigma \in S_n$, $\sigma(d) = -d$. So we have $d = \prod_{i<j}(\alpha_j - \alpha_i)$. Now $d^2$ is a symmetric polynomial in $\alpha_i$, so $d^2$ is a polynomial in coefficients of $f$, so $d^2 \in F$. 

\begin{Def}\index{discriminant}
$D = d^2 = \prod_{i < j}(\alpha_j - \alpha_i)^2$ is called the \textbf{discriminant} of a polynomial $f$, whose foots are $\alpha_1,...,\alpha_n$. 
\end{Def}

\begin{Ex}
$n = 2$. $f(x) = x^2 + bx + c$.  $d = (\alpha_2 - \alpha_1)$. Then 

\bee
D = (\alpha_2 - \alpha_1)^2 = (\alpha_1 + \alpha_2)^2  - 4\alpha_1\alpha_2 = (-b^2) - 4c = b^2 - 4c.
\eee
\end{Ex}


\bb

\textbf{Friday, April 20th}

Let $f \in F[x]$, $f(x) = x^n + a_{n - 1}x^{n - 1} + \cdots + a_1x + a_0 = (x - \alpha_1)\cdots(x - \alpha_n)$. The discriminant of $f$ is $\prod_{i < j}(\alpha_i - \alpha_j)^2$, and is a polynomial in $a_i$. 

For $n = 2$, $f = x^2 + b + c$, $D = b^2 - 4c$. 

For $n = 3$, $f = x^3 + px + q$, $D = -4p^3 - 27q^2$. 

If $f = x^3 + ax^2 + bx + c$, then replacing $x + \fracc{a}{3}$ by $x$, we get the above). 

For $n = 4$, $f = x^4 + px^2 + qx + r$, $D = 16p^4r -4p^3q^2 - \cdots$.


\begin{theorem}
$\gal(f/F)\leq A_n$ if and only if $\qwe{D} \in F$. ($D = d^2$ for some $d \in F$).
\end{theorem}

\begin{proof}
Let $d = \prod_{i < j}(\alpha_i - \alpha_j)$, $d = \qwe{D}$. 
Any permutation of the roots of $f$:
\bee
\sigma(d) &= 
\begin{cases}
d & \text{ if }\sigma \text{ is even},\\
-d & \text{ if }\sigma \text{ is odd}.
\end{cases}
\eee 
$\gal(f) \leq A_n$ if and only if $d$ is fixed by  $G = \gal(f)$, if and only if $d \in F$. 
\end{proof}

\textbf{Cubic: }$f(x) = x^3 + px + q$ - irreducible. Let $G = \gal(f) \leq S_3$. Any root can be sent to any other root. Or we can say that the order of the group is the degree of the splitting field. We want to show that $G$ contains a $3$-cycle for sure. $G$ is transitive on the roots, or $3||G|$, so $G \geq A_3 \cong \z_3$. So either $G \cong S_3$, or $G \cong A_3 \cong \z_3$. 

\begin{Def}\index{transitive}
For groups, a group action being \textbf{transitive} means any element can be sent to any other element. 
\end{Def}

Note $A_3 = \langle (1,2,3) \rangle$. So $G \cong S_3$ if and only if $\qwe{D} \notin F$, $G \cong \z_3$ if and only if $\qwe{D} \in F$. 

\bb

If $F \sub \R$, if $f$ has 1 real and 2 non-real roots, then $\gal(f) \cong S_3$, because $\z \mapsto \bar{z}$ acts as a transposition of $2$ roots of $f$. 

\begin{Ex}
\begin{enumerate}
\item $x^3 + x + 1$. $D < 0$, $\gal(f/\Q) \cong S_3$. 

\item $x^3 - 3x + 1$. $D = 81$, $\qwe{D} \in \Q$, so $\gal(f/\Q) \cong \z_3$. If you adjoin one root, you already get all the roots, since they are expressible in the first root. 
\end{enumerate}
\end{Ex}

Note $\i \mapsto -i$ is an automorphism of all of $\c$, so it's definitely an automorphism of other stuff...?

\begin{theorem}[\textbf{Casus irreducibilis}]
If $f$ is irreducible, has all 3 roots real, then you cannot express these roots as only real radicals, you need to use complex numbers. You need $\omega = \fracc{-1 + \qwe{-3}}{2}$. 
\end{theorem}

\textbf{Jake: }Can you give us an example of this fact?

\textbf{Leibman: }This is a negative fact, there are no examples. 

\begin{Def}
\textbf{Cardano formulas: }

\bee
A &= \wer[3]{-\fracc{27}{2}q + \fracc{3}{2}\qwe{-3}D},\\
B &= \wer[3]{-\fracc{27}{2} - \fracc{3}{2}\qwe{-3}D},\\
AB &= -3p.\\
\alpha_1 &= \fracc{A + B}{2},\\
\alpha_2 &= \fracc{\omega^2A + \omega B}{3},\\
\alpha_3 &= \fracc{\omega A + \omega^2B}{3}.
\eee
\end{Def}

\textbf{Quartics: } $x^4 + px^2 + qx + r$ - irreducible. 

$S_4$ is solvable: 
\begin{center}
\begin{tikzcd}
1 \arrow[rd, no head] \arrow[dd, "4"', no head] &  &  & K \arrow[dd, "4"', no head] \\
 & \z_2 \arrow[ld, no head] &  &  \\
V_4 \arrow[d, "3", no head] &  & \rightarrow & L \arrow[d, "3"', no head] \\
A_4 \arrow[d, "2"', no head] &  &  & F(\qwe{D}) \arrow[d, "2"', no head] \\
S_4 &  &  & F
\end{tikzcd}.
\end{center}
If $\qwe{D} \in F$, then $G \leq A_4$.  Note $L/F$ is the splitting field of some polynomial whose roots are:
\bee
\Theta_1 &= (\alpha_1 + \alpha_2)(\alpha_3 + \alpha_4)\\
\Theta_2 &= (\alpha_1 + \alpha_3)(\alpha_2 + \alpha_4)\\
\Theta_3 &= (\alpha_1 + \alpha_4)(\alpha_2 + \alpha+3).
\eee
This polynomial is: $h(x) = x^3 + 2px^2 + (p^2 - 4r)x + q^2$ is the resolvent cubic of $f$. A nice fact is that $D(h) = D(f)$. 



\bb

Let $G = \gal(f)$. If $h$ is irreducible, then if $\qwe{D} \notin F$, then $G \nleq A_4$, $6 \mid |G|$, so $G \cong S_4$. If $\qwe{D} \in F$, then $G \leq A_4$, $3||G|$, so $G = A_4$. 

If $h$ is reducible, then: if $\Theta_1,\Theta_2,\Theta_3 \in F$, then $G \cong V_4$. If only $\Theta_1 \in F$, then $G \cong D_8$, or $\z_4$. 

Take $K = F(x_1,...,x_n)$, $G$ acts on these variables, $\fix(F) = L$, $\gal(K/L) \cong G$, then take $\alpha$-primitive of $K/L$, $f = m_{\alpha,L}$. I don't know what this means. 



















\appendix
%    Include appendix "chapters" here.
%\include{}




\chapter{Category theory}


We discuss objects, morphisms. 

$A \to B$. 

We call this pair of an object and a morphism a \textbf{category}. An object $A$ is \textbf{repelling} if for any other object $B$, there is a single morphism from $A$ to this object. 

And \textbf{attracting} if there exists a single morphism from $B \to A$. 

Another terminology: \textbf{initial} and \textbf{terminal} objects. 

If such an object exists in a category, it is unique. 

\begin{lem}
Any such (repelling or attracting) object is unique up to isomorphism.  

\end{lem}
\begin{proof}
If there are two such universal objects, then there is a single unique morphism $\phi_1:A_1 \to A_2$. And a single unique morphism $\phi_2:A_2 \to A_1$ and their composition is a single morphism $\phi_1\phi_2:A_2 \to A_2$, which must be the identity on $A_2$, and $\phi_2\phi_1$ is the identity on $A_1$. 
\end{proof}

Consider the category of groups with $n$ marked elements, where a morphism between $(G,a_1,...,a_n)$ and $(H,b_1,...,b_n)$ is a hom-sm $\phi:G \to H$ s.t. $\phi(a_i) = b_i$ for all elements. 

Then $F_n = \langle a_1,...,a_n \rangle$, the free group with $n$ generators, is a universal repelling object in this category. 

Given $\forall H$, $b_1,...,b_n \in H$, we have a unique hom-sm $\phi:F_n \to H$ s.t. $\phi(a_i) = b_i$ $\forall i$. 

So the category is a set of all pairs of objects an morphisms which satisfies the properties of the definition of the category. So the category above is the type of group (groups with n marked elements) and the type of morphism. 

We define a new category, where $R$ is a unital ring:
\begin{itemize}
\item objects = $R$-modules with $n$ marked elements $(a_1,...,a_n)$. 
\item morphisms = $R$-hom-sms s.t. $\phi(a_i) =b_i$ $\forall i$. 
\end{itemize}
Then the universal repelling object is:
$$
(R^n,e_1,...,e_n),
$$
where $e_1 = (1,0,...,0)$, $e_2 = (0,1,0,...,0)$ and so on. 

Given $(M,u_1,...,u_n)$, define $\phi:R^n \to M$ by $(a_1,...,a_n) \to a_1u_1 + \cdots + a_nu_n$. And note that $\phi(e_i) = i_i$. 

\begin{Def}
The direct product of $R$-modules $M_1,M_2$ is:
 $$
M_1\times M_2 = \{(u_1,u_2):u_i \in M_i\},
$$
with:
$$
(u_1,u_2) + (v_1,v_2) = (u_1 + v_1, u_2 + v_2),
$$
$$
a(u_1,u_2) = (au_1,au_2),
$$
where $a \in R$. It is also called the \textbf{direct sum}, and denoted by $M_1 \oplus M_2$. 
\end{Def}

Now we define the category. 

\textbf{Category.} Objects are modules $M$ with hom-sms $\phi_1:M_1 \to M$, $\phi_2:M_2 \to M$. 

Morphisms: hom-sms $\phi_1:M \to N$ identical on $M_1,M_2$:

\begin{center}
\begin{tikzcd}
 & M_1 &  \\
M \arrow[ru, "\phi_1"] \arrow[rr, "\phi"] &  & N \arrow[lu, "\psi_1"] \\
 & M_2 \arrow[lu, "\phi_2"] \arrow[ru, "\psi_2"] & 
\end{tikzcd}. 
\end{center}







\textbf{Friday, January 12th}


We give an example of a category where the morphisms are not well-defined mappings. 

Define:
Objects = groups. 

Morphisms = classes of conjugate hom-sms. 

\begin{Def}
Reacll that $\phi \equiv \psi$ (these two hom-sms are \textbf{conjugate}) if $\psi(g) = a\phi(g)a^{-1}$ for some $a \in H$, where $\phi,\psi:G \to H$. 
\end{Def}

We give an other example of a category: 

Objects = topological spaces. 

Morphisms = classes of homotopic continuous mappings. Note that these morphisms are not mappings because images of points are not uniquely defined. 


\chapter{Sample problems to midterm I}


\begin{enumerate}[label=\arabic*.]
\item \textit{Prove that if $R$ is an integral domain, then $Tor(M)$ is a submodule of $M$ (called the torsion submodule of $M$). }
\begin{proof}
We know Tor$(M)$ is a subset of $M$ by its definition. We first prove it is an additive subgroup. Let $m \in $ Tor$(M)$. Then $\exists r \in R$, $r \neq 0$ s.t. $rm = 0$. Then consider $-m \in M$. From exercise 1 we know 
$
-m = (-1)m$, so we have:
$$
r(-m) = r(-1)m = (-1)rm = (-1)0 = 0,
$$
 since $R$ is commutative. So we have that $-m \in $ Tor$(M)$ as well, hence we have additive inverses. We check that it has additive closure. Let $m,n \in $ Tor$(M)$. Then we have $r,s \in R$, neither being zero, s.t. $rm = 0, sn = 0$. Now consider $m + n$. We have:
$$
rs(m + n) = rsm + rsn = srm + rsn = s0 + r0 = 0.
$$
Since we have no zero divisors, since $R$ is an integral domain, we know $rs \neq 0$, so $m  +n \in $ Tor($M$), we have additive closure, and Tor$(M)$ is a subgroup of $M$. Now we need only check that it is closed under the left action of $R$. So let $r \in R$ and $m \in $ Tor$(M)$. Then consider $rm$. We assume $r \neq 0$, since otherwise $rm = 0$ which is in our subgroup. And we know $\exists s \in R$, $s \neq 0$ s.t. $sm = 0$. Now we have $srm = rsm = r0 = 0$, so $rm$ is in Tor$(M)$. So it's a submodule. 
\end{proof}

\begin{proof}
An easier proof. Using the submodule criterion, we just need an $s \in R$ s.t. $s(x + ry) = 0$ by definition of $Tor(M)$ since we want to show an arbitrary $x  +ry \in Tor(M)$. But taking $s$ to be the product of the two annihilators of $x,y$ we have that $s$ and it is nonzero since integral domain. Done. 
\end{proof}

\item \textit{If $R$ is a PID and $M$ an $R$-module, and $a_1,a_2$ relatively prime, prove $Ann(a_1a_2) = Ann(a_1) \oplus Ann(a_2)$. }

\begin{proof}
Let $I = (a_1),J = (a_2)$. Then since we are in a PID, we know $Ann(a_1) = Ann(I)$ and the same for $J$. Then note that $I,J$ are comaximal since $a_1,a_2$ are relatively prime, and we are in a PID, thus $I + J = (1) = R$. Also note that $Ann(a_1a_2) = Ann(I \cap J)$ since $(a_1a_2) = (a_1) \cap (a_2)$.  Let $m \in Ann(I + J) = Ann((1)) = Ann(R)$ since $I,J$ are comaximal, and $R$ is commutative and unital. So $rm = 0$ for all $r \in R$. So then $m \in Ann(I)$, and since $0 \in Ann(J)$, we may write $m = m + 0$, so $m \in Ann(I) + Ann(J)$. And thus $Ann(I + J) \sub Ann(I) + Ann(J)$. The other inclusion is trivial. So we have that $Ann(a_1a_2) = Ann(a_1) + Ann(a_2)$. By Theorem \ref{thm10.67}, we have have that their intersection is trivial since if $m \neq 0$ and $a_1m = 0$ and $a_2m = 0$. Since $a_1,a_2$ are coprime we have $r,s \in R$ s.t. $ra_1 + sa_2 = 1$. So we also have $ra_1m = 0$ and $sa_2m = 0$.  So then we have:
$$
(ra_1 + sa_2)m = 1m = m = 0.
$$
So $Ann(a_1) \cap Ann(a_2) = 0$. So by Theorem \ref{thm10.67} we know $Ann(a_1a_2) = Ann(a_1) \oplus Ann(a_2)$. 
\end{proof}

\item \textit{Let $M$ be a module $S$ be a subset of $R$, and $I$ be the ideal generated by $S$. Prove that $Ann(S) = Ann(I)$ in $M$. }

\begin{proof}
Let $m \in Ann(S)$. Then $sm = 0$ $\forall s \in S$. Note $I = RS$. Let $i \in I$. Then $i = rs'$ for some $s' \in S$. Then we have: 
$$
im = rs'm = r\cdot 0 = 0,
$$
 since $m$ annihilates $s'$. Now let $m \in Ann(I)$. Then $im = 0$ $\forall i \in I$. Note $S \sub I$ since we just take $r = 1$ in the expression $rs$ which is the form taken by every element in $I$. So $m \in Ann(S)$. 
\end{proof}

\item 
\begin{enumerate}
\item 
\textit{Give an example of a submodule that is not a direct summand: $L \sub M$, but $M = L \oplus N$ for no submodule $N$ of $M$. }

Let $M = \z^2$. Note that two linearly independent vectors $(a,b),(c,d)$ span a direct summand if and only if the determinant of:
$$\left(
\begin{matrix}
a & c\\
b & d\\
\end{matrix}
\right)
$$
is $\pm 1$. Let $L = (2,3)$, the subgroup generated by $(2,3)$ and let $K = (2,5)$. Then we have:
$$
\det\left(
\begin{matrix}
2 & 2\\
3 & 5\\
\end{matrix}
\right) = 4 \neq \pm 1,
$$
so the subgroup $L + K$ is not a direct summand. It is easy to see that it is in fact a submodule. \bb

\textbf{BETTER EXAMPLE} Consider $2\z$. This is easily seen to be a submodule of $\z$ over itself. But $2\z$ is not a direct summand, since any other nontrivial submodule $K$ has $K \cap 2\z = 0$, since you can just multiply by $2$ since $2 \in \z = R$. 
\bb
\item \textit{Give an example of a torsion free module which is not a free module. }

Consider $\Q$ over $\z$. It is not a free module since any two nonzero rationals are linearly dependent, we can find integers such that a linear combination of them is zero. And thus if it was free, it would be free of rank 1. But $Q \ncong \z$. And it is torsion free since the product of any two nonzero rationals is nonzero. 
\end{enumerate}
\bb

\item \textit{Establish the universal property of the direct sum: for any module homomorphisms $\phi:M \to K$ and $\psi:N \to K$ there exists a unique homomorphism $\eta:M \oplus N \to K$ s.t. $\eta|_M = \phi$ and $\eta|_N = \psi$. }

\begin{proof}
Let $\phi:M \to K$ and let $\psi:N \to K$ be hom-sms. Let $\eta(m,n) = \phi(m) + \psi(n)$. Suppose there were another map $\nu$ which also has this property. Call it $
\gamma$. Then we must have $\gamma(u,v) = \gamma(u,0) + \gamma(0,v) = \phi(u) + \psi(v) = \eta(u,v)$. So its unique. 
\end{proof}

\item \textit{Prove that for any three modules $M,N$ and $K$ we have:
$$
Hom(M \otimes N,K) \cong Hom(M,K) \oplus Hom(N,K).
$$}

\begin{proof}
Let $H = \text{Hom}_R(A\times B,M)$, $H_A = \text{Hom}_R(A,M)$, and $H_B = \text{Hom}_R(B,M)$. Let $\Phi: H_A \times H_B\to H$ be given by $\Phi((\phi,\psi)) = \phi + \psi$, where $\phi \in H_A,\psi \in H_B$. We prove this is an isomorphism of $R$-modules. 

\textbf{Homomorphism: }Observe: 
\begin{equation}
\begin{aligned}
\Phi((\phi_1,\psi_1) + (\phi_2,\psi_2)) &= \Phi((\phi_1 + \phi_2,\psi_1 + \psi_2)) = \phi_1 + \psi_1 + \phi_2 + \psi_2\\ &= \Phi((\phi_1,\psi_1)) + \Phi((\phi_2,\psi_2)).
\end{aligned}
\end{equation}

In the above expression, the first equality comes from the definition of addition in $H_A \times H_B$. The second and third equalities comes from the definition of $\Phi$. And we also know: 
$$
\Phi(r(\phi,\psi)) = \Phi((r\phi,r\psi)) = r\phi + r\psi = r(\phi + \psi) = r\Phi((\phi,\psi)),
$$
hence $\Phi$ preserves mult. by $R$, by the definition of scalar multiplication on the $R$-module $H_A \times H_B$, and the definition of $\Phi$. 

\textbf{Surjectivity: } Let $\phi \in H$. Then $\phi:A\times B \to M$. So let $\phi \in H_A$ be given by $\phi(a) = \phi(a,0)$,
and let $\psi \in H_B$ be given by $\phi(b) = \phi(0,b)$. Then we have: $\Phi((\phi,\psi)) = \phi$.  Then $\Phi$ is surjective. 


\textbf{Injectivity: } Let $\Phi((\phi_1,\psi_1)) = \phi_1 + \psi_1 = \phi_2 + \psi_2 = \Phi((\phi_2,\psi_2)) \in H_A \times H_B$. Then note that 
$$
(\phi_1 + \psi_1)(a,0) = \phi_1(a) = \phi_2(a) = (\phi_2 + \psi_2)(a,0),
$$
and the same holds when we let $a = 0$, and use an arbitrary $b$ value, so we get that $\psi_1 = \psi_2$ as well. Hence $\Phi$ is injective. And thus it is an isomorphism. 
\end{proof}

\item \textit{If $M$ is an $R$-module, prove that $Hom_R(R^n,M) \cong M^n$ as $R$-modules. }

\begin{proof}


Now Hom$(R^n,M) \cong M^n$, since we map $\phi \mapsto (\phi(e_1),...,\phi(e_n))$. \textbf{THIS IS THE WAY TO DO IT, THE CHECKS ARE EASY, JUST REMEMBER BASIS BASIS BASIS. }Or we can use the exercise from the last homework:
$$
Hom(A \oplus B,M) \cong Hom(A,M) \oplus Hom(B,M),
$$
since:
$$
Hom(R^n,M) \cong Hom(R,M)^n \cong  Hom(R,M) \oplus \cdots \oplus Hom(R,M) \cong M^n,
$$
by induction using the above statement, and considering $R$ as an $R$-module over itself. 
\end{proof}

\item \textit{Assume that a module has a finite basis: a linearly independent set $B = \Set{u_1,...,u_n}$ that generates $M$, $M = RB$. Prove that $M$ is free, $M \cong R^n$. }\label{B.8}

\begin{proof}
Let $\phi:R^n \to M$ be given by $(e_1,...,e_n) \mapsto (u_1,...,u_n)$. Sapienti sat. 
\end{proof}

\item \textit{Let $M$ be a module and let $B$ be a maximal linearly independent subset of $M$ (exists by Zorn's lemma). }

\begin{enumerate}
\item \textit{Prove that the module $RB$ is free. }

\begin{proof}
Note $B$ doesn't have to generate $M$. So what can we say about the submodule generated by $B$. The submodule $RB$ has as basis $(B)$, a linearly independent system which generates this module. \textbf{So, it's free.} Since we can use the map defined in Exercise \ref{B.8} to show that $RB \cong R^n$ where $n$ is the cardinality of $B$ or $RB \cong \prod_{\alpha \in \Lambda} R_\alpha$ if $B$ is infinite. 
\end{proof}

\begin{proof}
Leibman's proof. Any $u \in RB$ can be written:
$$
u = \sum_{\alpha \in \Lambda} a_\alpha v_\alpha,
$$
$a_\alpha \in R,v_\alpha \in B$, $a_\alpha = 0$ for all but finitely many $\alpha \in \Lambda$. It is unique since $B$ is linearly independent. Since if you have two representations, subtract one from the other, you have a linear combination, it will contradict some stuff. Isomorphism $RB \leftrightarrow \bigoplus_{\alpha \in \Lambda} R$ where $u \leftrightarrow (a_\alpha)_{\alpha \in \Lambda}$. 
\end{proof}

\item \textit{Prove that $M/RB$ is a torsion module. }

\begin{proof}
We assume $R$ is unital, since otherwise, $B$ may not be in $RB$. Or we could define $RB$ as $RB \cup B$. 
Indeed, suppose for contradiction that $\exists u \in M$ s.t. $\overline{u} \equiv u \mod RB$ is not a torsion element, this means that $au \notin RB$ $\forall a \neq 0 \in R$. This is because "0" in the quotient module is the kernel, $RB$ so $au$ cannot be in $RB$. Then we just set these to zero, allowing the coefficients to be arbitrary, we are just trying to show that they are linearly independent:
$$
au + c_1v_1 + \cdots + c_kv_k = 0,
$$
 with $v_i \in B, c_i \in R, a \in R$, then $a = 0$, since if $a$ was nonzero, then:
 $$
 au = -c_1v_1 - \cdots - c_kv_k \in RB.
 $$ 
 so:
 $$
 c_1v_1 + \cdots c_kv_k = 0,
 $$
 so $c_i = 0$ for all $i$, so $\{u\}\cup B$ is linearly independent, contradiction, since $B$ was the largest linearly independent set in $M$. So we have a contradiction. 
 
 So for any $u$, there exists a nonzero $a$ s.t. $a\bar{u} \in RB$, so $au + c_1v_1 + \cdots c_kv_k = 0$ for some $c_i \in R,v_i \in B$. Hence $a\bar{u} = 0 \in M/RB$, and thus $M/RB$ is a torsion module. 
\end{proof}

\begin{proof}
Leibman's proof. $M/RB$ is torsion module. If $v \in M$ is such that $av \neq 0 \mod RB$ $\forall a \neq 0$, then $B \cup \Set{v}$ is linearly independent. Why? If $av + \sum a_\alpha v_\alpha = 0$ for some $a_\alpha,v_\alpha \in B$, then $a = 0$, so... Done. If $R$ has zero divisors then no element is linearly dependent, so you cannot find them, since we can divide 0. So must have $R$ is an integral domain. 
\end{proof}
\end{enumerate}
\bb

\item \textit{Prove that $\z_n \tens \z_m \cong \z_d$ as groups, where $d = \gcd(n,m)$. }

\begin{proof}
Define $\phi: \z_n \tens \z_m \to \z_d$ by $\phi(\bar{k}\tens \bar{l}) = kl \mod d$. If you add a multiple of $n$ to $k$, the result will be the same because $d|n$, and same for $m$ ($(\bar{k},\bar{l}) \mapsto kl \mod d$ is bilinear). Why is it a homomorphism? This is easy to check so we omit it, just check that the additive subgroup is preserved and the action of $\z$ is preserved.  Let's check that it is surjective. Note that $1 \tens 1 \mapsto 1$, and $1$ generates $\z_d$. So done: $\phi(1 \tens a) = a \mod d$, so $\phi$ is surjective. Or maybe better to just define an inverse, since injectivity looks hard to prove. So let $\phi(l \tens k) = lk \equiv 0 \mod d$. Then $lk = jd$ for some integer $j$. And note that we have:
$$
l \tens k = lk(1 \tens 1) = jd(1 \tens 1).
$$
And since $d$ is the gcd, we can write $d = xn + ym$ for some integers $x,y$. So we have:
\bee
l \tens k &= jd(1 \tens 1) = j(xn + ym)(1 \tens 1)\\
&= j(xn(1 \tens 1) + ym(1 \tens 1))\\
&= j(x(n \tens 1) + y(1 \tens m))\\
&= j(0 \tens 0 + 0 \tens 0) = 0.
\eee
So $ker\phi = 0$ and it is injective, and hence an isomorphism. 
\end{proof}
\bb

\item \textit{Let $G = \z_2 \oplus \z_2 \oplus \z_4 \oplus \z^2$. }
\begin{enumerate}
\item \textit{Find the dimension of the $\z_2$ vector space $\z_2 \tens_\z G$. }

$\z_2 \tens G \cong (\z_2 \tens \z_2) \oplus (\z_2 \tens \z_3) \oplus (\z_2 \tens \z_4) \oplus (\z_2 \tens \z)^2$ by the next exercise 12. 

So its $\cong \z_2 \oplus 0 \oplus \z_2 \oplus \z_2^2$. So its $\cong \z_2^4$. 

\item

When you multiply by $\Q$, it will kill all torsions, so we only have $\z^2$ left. And so we have $G \tens \Q \cong \Q^2$. 
\end{enumerate}

\item \textit{Prove that for any three modules $M,N,K$:
$$
(M \oplus N) \tens K \cong (M \tens K) \oplus (N \tens K).
$$ }

\begin{proof}
Map $((m,n),k) \mapsto ((m \tens k) ,(n \tens k))$. This map is clearly seen to be bilinear, and so it induces a module hom-sm:
 $$
 \phi:(M \oplus N) \tens K \to (M \tens K) \oplus (N \tens K)
 $$
 given by $\sum_i(m_i,n_i) \tens k_i \to (\sum_i m_i \tens k_i,\sum_i n_i \tens k_i)$. Now consider the mappings $(m,k) \to (m,0) \tens k$ and $(n \tens k) \to (0,n) \tens k$, which map from $M \times K$ and $N \times K$ respectively to $(M \oplus N) \tens K$. These are also easily seen to be bilinear, and they induce module homomorphisms $\phi_1$,$\phi_2$ defined as expected. So define $\psi:(M \tens K) \oplus (N \tens K) \to (M \oplus N) \tens K$ for which:
 \bee
( m \tens k_1,n \tens k_2) \mapsto \phi_1(m \tens k_1) + \phi_2(n \tens k_2)& = (m,0) \tens k_1 + (0,n) \tens k_2\\
 \eee
 Now note that 
 \bee
 \phi(\psi(m \tens k_1,n \tens k_2) &= \phi((m,0) \tens k_1 + (0,n) \tens k_2)\\
 &= (m \tens k_1 + 0 \tens k_2,0 \tens k_1 + n \tens k_2)\\
 &= (m \tens k_1 + 0,0 + n \tens k_2)\\
 &= (m \tens k_1,n \tens k_2).
 \eee
 And so $\phi = \psi^{-1}$. And so $\psi$ is invertible and thus is a module isomorphism. 
\end{proof}


\item \textit{Let $V$ be an $n$-dimensional vector space with basis $\Set{u_1,...,u_n}$. Explain why $\c \tens_\R V$ has structure of a $\c$-vector space. }

Consider $V \cong \R^n$, basis $\Set{u_1,...,u_n}$. We have $\c$-basis in $\c \tens_\R V$ which is $\Set{1 \tens u_1,...,1 \tens u_n}$. And over $\R$ we have 
$\c \tens_\R V$: 

$$
\Set{1 \tens u_1,...,1 \tens u_n,i \tens u_1,...,i \tens u_n}.
$$ 
Note $\R^2 \tens V \cong V^2 = V \oplus V$. This is because $R^2 = R \oplus R$. And then we have $(\R \oplus \R) \tens V = (\R \tens V) \oplus (\R \tens V)$. And since we are taking the tensor over $\R$ already $(\R \tens V) \cong V$.  

Note that $V \cong R^n$ so $\c \tens V \cong (\c \tens_\R \R)^n \cong C^n$. So rewriting:
$$
\c \tens(\oplus\R u_i) \cong \oplus(\c \tens \R u_i) \cong \oplus \c u_i.
$$



\setcounter{enumi}{13}
\item \textit{$\Q \tens_\z \Q \cong \Q$. }

\begin{proof}
Define an isomorphism by $\fracc{m}{n} \tens \fracc{k}{l} \mapsto \fracc{nk}{ml}$. Just check. But let's use our advanced knowledge instead:
$$
0 \to \z \to \Q \to \Q/\z \to 0,
$$
and multiply this by $\Q$:
$$
0 \to \z \tens \Q \to \Q \tens \Q \to (\Q/\z)\tens \Q \to 0.
$$
We have exactness on left since $\Q$ is flat. If $R$ integral domain field of fractions is flat refer to Lemma \ref{lem10.151}. So $0 \to \z \tens \Q \to \Q \tens \Q \to 0$ is exact. And $\Q/\z = 0$. So we have $\Q \cong \z \tens \Q \cong \Q \tens \Q$. 
\end{proof}

\setcounter{enumi}{15}

\item \textit{Prove that $R[x] \tens R[x] \cong R[x,y]$ as $R$-algebras. }

\begin{proof}
Take $x^n \tens y^m \mapsto x^ny^m$. 
\end{proof}

\setcounter{enumi}{18}

 \item 

\begin{proof}
$M = \bigoplus M_\alpha$. 
$$
0 \to A \to_\phi B.
$$

$$
0 \to A \tens M \to_{\phi \tens Id} B \tens M \cong \bigoplus_{\alpha \in \Lambda}(A \tens M_\alpha) \to \bigoplus_{\alpha \in \Lambda}(B \tens M_\alpha).
$$
On the left we have $a \tens u \mapsto \phi(a) \tens u$, and on the right we have $(a \tens u_\alpha)_{\alpha \in \Lambda} \mapsto(\phi(a \tens u_\alpha)_{\alpha \in \Lambda}$. Because the map commutes with the direct sum, its injective if and only if each component is. So then we have:
$$
\bigoplus N_\alpha \overset{\psi}{\to} \bigoplus K_\alpha,
$$
where $(v_\alpha) \mapsto(\psi_\alpha(v_\alpha))$. $\psi$ is injective if and only if $\psi_\alpha$ are all injective. For all $\alpha$ $\psi_\alpha = \psi|_{N_\alpha}$. So if $\psi$ is injective, $\psi_\alpha$ is injective. If $\psi_\alpha$ are all injective:
$$
\psi((v_\alpha)_{\alpha \in \Lambda}) = (\psi_\alpha(v_\alpha))_{\alpha \in \Lambda} = 0.
$$
if and lonly if $\psi_\alpha(v_\alpha) = 0$ for all $\alpha$ if and only if $v_\alpha = 0$ for all $\alpha$. So $\psi$ is injective. 
\end{proof}


\setcounter{enumi}{20}
\item \textit{Give an example of a non-flat torsion-free module. }

Consider $M = I = (x,y) \sub R = F[x,y]$. Then we have:
$$
0 \to I \to R.
$$
And 
$$
M \tens I \to R \tens I \cong I
$$
 is not injective, since $x \tens y - y \tens x \mapsto 0$. \textbf{ASK LEIBMAN}
 



\setcounter{enumi}{21}
\item \textit{Every projective module is flat. }

\begin{proof}
First show it must be a direct summand of a free module, and this it is flat. 
\end{proof}




\end{enumerate}


\chapter{Common exam mistakes}

On 4(a). Define $(R/I) \tens M \to M$ by $\bar{a} \tens u \to au$. Take $b$ s.t. $\bar{b} = \bar{a}$. Then we have $\bar{b} \tens u \to bu$ but $bu \neq au$. 

On 3, constructing $\phi:S \tens R[x] \to S[x]$. To prove injectivity, you can't just check simple tensors!!!!! You need to construct an inverse homomorphism. You define $sx^n \mapsto s \tens x^n$, since $S[x]$ is a free module over $S$. 

Recall:
\begin{Def}
$A$ is an $S$-algebra if and only if it is an $S$-module and a ring s.t. $(s\alpha)\beta = \alpha(s\beta) = s(\alpha\beta)$ for $s \in S, \alpha,\beta \in A$. So $S[x]$ in the above is an $S$-algebra if $S$ is commutative. 
\end{Def}














\chapter{Sample problems to midterm II}

\textbf{Friday, March 2nd, 2018}

\begin{enumerate}[label=\arabic*.]

\item \textit{Let $M,N$ be $R$-modules. Define a homomorphism $\Phi:M^* \tens N \to Hom(M,N)$ by $\Phi(f \tens v)(u) = f(u)v$. }

\begin{enumerate}


\item \textit{Prove that $\Phi$ is well-defined. }

\begin{proof}
We use the universal property. If we have a bilinear map $\gamma$ such that:
\begin{center}
\begin{tikzcd}
 & M^* \times N \arrow[rd, "\gamma"] \arrow[ld, "\beta"'] &  \\
M^* \otimes N \arrow[rr, "\Phi"] &  & \text{Hom}(M,N)
\end{tikzcd}
\end{center}
is commutative, then we have induced homomorphism $\Phi$ which is well-defined. So let $\gamma:M^*\times N \to Hom(M,N)$ be given by $(f \times v)(u) \mapsto f(u)v$. We check that it is bilinear:
\bee
(f + g)(u)v &\mapsto (f + g)(u)v = f(u)v + g(u)v\\
af(u) \times v &\mapsto af(u)v = a(f(u)v)\\
f(u) \times av &\mapsto f(u)av = af(u)v\\
f \times (v + w) &\mapsto f(u)(v + w) = f(u)v + f(u)w.
\eee
And so we see that it is bilinear by the definition of a module and since $R$ is commutative. Hence the induced map $\Phi$ is well-defined. 
\end{proof}
\bb
\item \textit{If $M$ and $N$ are free modules of finite rank, $\Set{u_1,...,u_m}$ is a basis in $M$, $\Set{f_1,...,f_m}$ is the dual basis in $M^*$, and $\Set{v_1,...,v_n}$ is a basis in $N$, prove that
$$
\Set{\Phi(f_i \tens v_j): i = 1,...,m, j = 1,...,n}
$$
is a basis in Hom$(M,N)$. Deduce that $\Phi$ is an isomorphism. 
}

\begin{proof}
Let $f_i \in M^*$ and $v_j \in N$ s.t. $\Phi(f_i \tens v_j)(u) = 0 \in Hom(M,N)$. We wish to prove that $\Phi$ is injective, so we will show $f_i \tens v_j = 0$. Note we have $\Phi(f_i \tens v_j)(u) = f_i(u)v_j = 0$ by the definition of $\Phi$. So note since $N$ is free, it is torsion-free, and thus we know that $f_i(u)v_j = 0$ means $f_i(u) = 0$ $\forall u$, or $v_j = 0 \in N$. But in either of these cases, we then know $f_i \tens v_j = 0 \in M^* \tens N$. So we have shown $\Phi$ is injective. Now note we know that $M^* \cong R^m$, and $N \cong R^n$. So $M^* \tens N \cong R^{mn}$ by Remark \ref{1097}, and since $M$ is free of rank $m$, we know $Hom(M,N) \cong Hom(R^m,R^n) \cong R^{mn}$ by Corollary \ref{cor113}. Thus we know $M^* \tens N \cong Hom(M,N)$. And since we have an injective map between these two, it must be an isomorphism. 
\end{proof}

\end{enumerate}



\bb            

\item \textit{Let $M,N,K$
be free modules of finite rank over a commutative unital ring
$R$, and let $\phi:M \to N$
and $\psi:N \to K$
be homomorphisms considered as tensors from $N \tens M^*$
and $K \tens N^*$
respectively. Show that the composition
$\psi \circ \phi:M \to K$ 
corresponds to the contraction
of the
$N^* \tens N$-part of the tensor
$\psi \tens \phi \in K \tens N^* \tens N \tens M^*$. (
Hint:
Either check this for simple
tensors and use linearity, or use the coordinate presentation of tensors.
)}


\begin{proof}
Recall that the contraction homomorphism is $f \tens v \mapsto f(v) \in R$. We take simple tensors $k \tens g \in K \tens N^*$ and $n \tens f \in N \tens M^*$. Then we have:
\bee
 \psi \tens \phi &= (k \tens g) \tens (n \tens f) \mapsto k \tens g(n) \tens f \\
 &= k \tens g(n)f \in K \tens M^* \cong M^* \tens K \cong Hom(M,K)
 \eee
  by the previous exercise. Define $\psi = g(u)k \in Hom(N,K)$ and $\phi = f(u)n \in Hom(M,N)$. Then $(\psi \circ \phi)(u) = \psi(\phi(u)) = g(f(u)n)k = f(u)g(n)k$. So we define $\Phi:K \tens M^* \to Hom(M,K)$ by $\Phi(k \tens g(n)f)(u) = f(u)g(n)k$. And since these are all free modules, hence torsion free, we know if $\Phi(k \tens g(n)f)(u) = 0$, then one of $f(u),g(n),k$ must be identically zero, which means $k \tens g(n)f = 0$. So it's injective and this an isomorphism since we already showed that $K \tens M^* \cong M^* \tens K \cong Hom(M,K)$. Hence for any such $\psi,\phi$, extending from simple tensors by linearity, we know they are in 1-1 correspondence with the simple tensors $k \tens g$ and $n \tens f$, and since we know the contraction maps $\psi \tens \phi \mapsto k \tens g(n)f$, and the isomorphism $\Phi$ maps $k \tens g(n)f \mapsto f(u)g(n)k = (\psi \circ \phi)(u)$, we are done. 
\end{proof}



\item \textit{Let $M$ be a free module of rank 2 over a commutative unital ring $R$, let $\Set{u_1,u_2}$ be a basis of $M$, let $\Set{u_1^*,u_2^*}$ be the dual basis of $M^*$, and let $f \in M^*$ have coordinates $(4,5)$ in this basis. What are the coordinates of $f$ in the basis dual to $\Set{2u_1,3u_2}$? }

\bb
We define $\Set{g_1,g_2}$ to be our new dual of $\Set{2u_1,3u_2}$. Define $g_1 =  \fracc{1}{2}u_1^*$ and $g_2 = \fracc{1}{3}u_2^*$. Then $g_1(2u_1) = \fracc{1}{2}u_1^*(2u_1) = \fracc{1}{2}2 = 1$ and so on, it works for the new basis. So $f = (4u_1^*,5u_2^*) = 8g_1+15g_2$. 

\bb

\item \textit{Let $M$ be a free module of finite rank over a commutative unital ring $R$; then bilinear forms on $M$ are represented by tensors from $M^* \tens M^*$. Prove that a bilinear form $\beta$ on $M$ is symmetric (that is $\beta(u,v) = \beta(v,u)$ for all $u,v \in M$) if and only if the corresponding tensor is symmetric. }

Note that bilinear forms can be represented as $n \times n$ matrices, and they are symmetric if and only if the corresponding matrix is symmetric. Recall that this means that $A = A^T$. But the entries of the matrix are just the coordinates of the corresponding tensor written in the natural basis. 


\textbf{Leibman's tip: }Map $f_1 \tens f_2\mapsto \beta(u_{1},u_{2})=f_{1}(u_{1})f_{2}(u_{2})$. 




















%The following stuff is trash
\begin{comment}
\begin{proof}
We are given that $Hom(M \times M,R) \cong M^* \tens M^*$, and it can be easily checked that $\gamma(u,v) \mapsto \gamma(u,0) \tens \gamma(0,v)$ is an isomorphism using the universal property. So suppose $\gamma$ is symmetric. Then we have $\gamma(u,v) = \gamma(v,u)$ which gives us:
$$
\gamma(u,0) \tens \gamma(0,v) = \gamma(0,u) \tens \gamma(v,0),
$$
and so the image tensor is symmetric by definition. And by the exact same reasoning, we know that any tensor $\alpha(u) \tens \beta(v) \in M^* \tens M^*$ has a corresponding $\gamma(u,v)$ in $Hom(M \times M,R)$ s.t. $\gamma(u,0) = \alpha(u)$ and $\gamma(0,v) = \beta(v)$ by surjectivity of our isomorphism. Then we know if $\alpha(u) \tens \beta(v) = \beta(u) \tens \alpha(v)$ then 
$$
\gamma(u,0) \tens \gamma(0,v) = \alpha(u) \tens \beta(v) = \beta(u) \tens \alpha(v) = \gamma(0,u) \tens \gamma(v,0),
$$
and then by injectivity of our isomorphism, we have $\gamma(u,v) = \gamma(v,u)$. 
\end{proof}

\end{comment}


















\setcounter{enumi}{4}

\item \textit{Let
$M$
be a module over a commutative unital ring
$R$
and let
$w$
be a symmetric covariant
$n$-tensor (that is, a symmetric tensor from
$\Tau^n
(
M
∗
))$. Prove that
$w$
defines a homomorphism
$\mc{S}^n(M) \to R$
(that is, an element of $(
\mc{S}^n(M)$*).}

Let $w \in \Tau^n(M^*)$, $w$ is symmetric, and $\sigma(w) = w$ for all $\sigma \in S_n$. Then $S^n(M)$ which is called the \textbf{symmetric tensor product}, is equal to $\Tau^n(M)/\mc{C}^n$, where $\mc{C}^n(M)$ is generated by the relation $u \tens v - v \tens u$, it is the submodule generated by $\alpha - \sigma(\alpha)$, for $\alpha \in \Tau^n(M)$, for $\sigma \in S_n$. These two definitions of $\mc{C}^n(M)$ are equivalent, you can use whichever you like. We claim that if we have a symmetric, covariant tensor, it acts on $\Tau^n(M)$:
$$
(f_1 \tens \cdots \tens f_n)(u_1 \tens \cdots \tens u_n) = f_1(u_1)\cdots f_n(u_n) \in R.
$$
Where the first part of the product is in $\Tau^n(M^*)$ and the second half of the product is in $\Tau^n(M)$. So the tensor product of $f$'s is our $w$ and the tensor product of $u$'s is our $\alpha$. 
Covariant tensors act on contravariant tensors. The question is whether or not $w$ acts on the factor module. So we want to know if we have $w:\Tau^n(M)/\mc{C}^n(M) \to R$. It is true when $w(\mc{C}^n(M)) = 0$. Note we already had $w:\Tau^n(M) \to R$ by our action. Now w check that $w$ maps $\mc{C}^n(M)$ to zero. So take:
$$
w(\alpha - \sigma(\alpha) = w(\alpha) - w(\sigma(\alpha)) = w(\alpha) - (\sigma^{_1}(w))(\\alpha) = 0. 
$$
But we assumed $\sigma(w) = w$, so we must have $\sigma^{-1}(w) = w$. 

\begin{Ex}
Let $\sigma = (123)$. 
\bee
(f_1 \tens f_2 \tens f_3)(\sigma(u_1 \tens u_2 \tens u_3)) &= (f_1 \tens f_2 \tens f_3)(u_2 \tens u_3 \tens u_1) \\
&= f_1(u_2)f_2(u_3)f_3(u_1)\\
&= \sigma^{-1}(f_1 \tens f_2 \tens f_3)(u_1 \tens u_2 \tens u_3)\\
&= (f_3 \tens f_1 \tens f_2)(u_1 \tens u_2 \tens u_3)\\
&= f_3(u_1)f_1(u_2)f_2(u_3).
\eee
\end{Ex}
Note we applied $\sigma$ to the indices, not to the locations. 

\begin{proof}
Let $N$ be a module, $K \sub N$. Let $f \in N^*$, $f:N \to R$. Is $f$ defined on $N/R$? In other words, does $f$ induce a homomorphism $N/K \to R$? So do we have $f \in (N/K)^*$? This is so if $f(K) = 0$ (then $\forall u \in N$, $f(u + k) = f(u)$) The problem was to show that if you have a symmetric tensor $w$, that it maps elements of $\mc{C}^n(M) \to 0$. 


He would be happy with the row $w(\alpha - \sigma(\alpha)) = \cdots = 0$. But you need to know that definition of $\mc{C}^n(M)$. 
\end{proof}

\item \textit{Let $\Set{u_1,u_2,u_3}$ be a basis in a vector space $V$. }


\textbf{The main idea of this problem is to know the definition of $V \wedge V = \Lambda^k(V)$. }
Note here our $V = M$ and $\dim V = n = 3$. 
\begin{rem}
 The basis in $\Lambda^k(M)$ is:
 $$
 \Set{u_{i_1} \wedge u_{i_2} \wedge \cdots \wedge u_{i_k}: i_1 < i_2 < ... < i_k}.
 $$
 \end{rem}
 
 \begin{enumerate}
 \item \textit{Find a basis in the space $V \wedge V$ (don't justify!)}

 So this part is just asking us to write exactly the above set. Note $V \wedge V = \Lambda^2(V)$. 
 
 Our basis is: $\Set{u_1 \wedge u_2,u_2 \wedge u_3, u_1 \wedge u_3}$. There are always $\binom{n}{k}$ basis wedges, where $n = \dim V$ and $k$ is the power of $\Lambda$. 
 
 \item \textit{Find the coordinates of the tensor $x_1u_1 + x_2u_2 + x_3u_3 \wedge y_1u_1 + y_2u_2 + y_3u_3$ in this basis. }
 
 Observe:
 \bee
 (x_1u_1 \wedge y_1u_1 + y_2u_2 + y_3u_3) +( x_2u_2 \wedge y_1u_1 + y_2u_2 + y_3u_3) + (x_3u_3 \wedge y_1u_1 + y_2u_2 + y_3u_3)\\
 =x_1u_1 \wedge y_2u_2 + x_1u_1 \wedge y_3u_3 + x_2u_2 \wedge y_1u_1 \\
 + x_2u_2 \wedge y_3u_3 + x_3u_3 \wedge y_1u_1 + x_3u_3 \wedge y_2u_2\\
 =(x_1y_2 -x_2y_1)(u_1 \wedge u_2) + (x_2y_3 - y_2x_3)(u_2 \wedge u_3) +( x_1y_3 - x_3y_1)(u_1 \wedge u_3).
 \eee
 \end{enumerate}
 
 
 \item \textit{}
 
 \textbf{The main idea of this problem is to know how the $\wedge$ operator works, namely that a vector wedge itself is 0 and that you get a negative when you try to commute. }
 
 \begin{proof}
 So we have two bases and for each $v_i$ we can write $v_1 = a_{11}u_1 + \cdots + a_{1n}u_n$ and so on for all $v_i$'s. Then we wedge them all together and write:
 $$
 v_1 \wedge ... \wedge v_n = a_{11}u_1 + \cdots + a_{1n}u_n \wedge ... \wedge a_{n1}u_1 + \cdots + a_{nn}u_n,
 $$
 and recall that $u_i \wedge u_i = 0$ and $u \wedge v = -v \wedge u$. So all the terms with repeated $u_i$'s will die, and we get $c(u_1 \wedge ... \wedge u_n)$ for some ring element $c$. And we can do the same for the $u_i$'s writing them as linear combinations of the $v$'s and wedgeing all these combinations together, getting $u_1 \wedge ... \wedge u_n = b(u_1 \wedge ... \wedge u_n)$ and so we must have $b = c^{-1}$ and so it's a unit. 
 \end{proof}

\setcounter{enumi}{7}

\item 
\begin{enumerate}
\item Consider $\z \to \z$ given by multiplication by two. 


\item Let $\phi:M \to N \to 0$, both modules free, same rank, surjective since exact on right. We are over an integral domain. Assume not injective, so $K = ker\phi \neq 0$ And:
$$
0 \to K \to M \to N \to 0
$$
 where $\phi:M \to N$ and so $K$ nonzero implies the image of the embedding from $k \to M$ is nonzero, so then $K\neq 0$, so $rank(K) \geq 1$. But we know ranks are additive. $rank(M) = rank(K) + rank(N) > rank(N)$, contradiction. 
\end{enumerate}

\begin{rem}
$rank(M) = 0$ if and only if it is a torsion module. 
\end{rem}

\item 

\begin{rem}
if you have $\phi:M \to N$ a monomorphism, then since its injective the image of an independent set is independent, so if we take a maximal set $\Set{u_1,...,u_n} \to \Set{\phi(u_1),...,\phi(u_n)}$ in $M$, its image is linearly independent in $N$. Since $n = rank(N)$, $\Set{\phi(u_1),...,\phi(u_n)}$ is a maximal linearly independent subset, which is equivalent to saying that the quotient is a torsion module. 
\end{rem}

\setcounter{enumi}{9}

\item \textit{Let $M_1 \cong \z^4/N_1$ and $M_2 \cong \z^4/N_2$, where $N_1$ is a submodule of $\z^4$ generated, in some basis $\Set{u_1,...,u_4}$ of $\z^4$, by $\Set{u_1,2u_2,6u_3}$, and $N_2$ is a subgroup of $\z^4$ generated, in some basis $\Set{v_1,...,v_4}$ of $\z^4$, by $\Set{v_1,3v_2,6v_3}$. }

\begin{enumerate}
\item \textit{Are $N_1,N_2$ isomorphic?}
Note that $N_1,N_2$ are submodules of $\z^4$ which is free, so they must be free because \textbf{we have a theorem that says over a PID, submodules of free modules are free}, and so since they are generated by 3-element sets they are both free of rank 3 and so they are isomorphic. 

\item \textit{What are the ranks of the modules $M_1,M_2$?}
They are both rank 1. We know this because we can write:
\bee
M_1 \cong \z_2 \oplus \z_6 \oplus \z\\
M_2 \cong \z_3 \oplus \z_6 \oplus \z
\eee
which is only true when we are over a PID by the fundamental theorem. 

\item \textit{Are $M_1$ and $M_2$ isomorphic?}


\textbf{NO!}
We have:
\bee
M_1 \cong \z_2 \oplus \z_6 \oplus \z\\
M_2 \cong \z_3 \oplus \z_6 \oplus \z
\eee

Two modules are isomorphic if and only if they have the same rank and the same invariant factors, so these are not isomorphic, since they have different invariant factors, recall the invariant factors are the $a_1,...,a_k$ that generate the ideals that we mod by. 
\end{enumerate}

\item \textit{Let $N$ be the sublattice of $\z^3$ generated by (insert vectors here), and let $M = \z^3/N$. }

\begin{enumerate}

\item \textit{Find the invariant factors of $M$. }

\textbf{Step 1: } Write the matrix of the generators. It is:
$$
\lpar 
\begin{matrix}
1 & -1 & 1\\
1 & -1 & -1\\
1 & 1 & 3
\end{matrix} \rpar.
$$

\textbf{Step 2: }Use elementary row operations to write the matrix in the form:
$$
\lpar 
\begin{matrix}
a_1 & & \\
& a_2 & \\
& & a_3
\end{matrix} \rpar = \lpar 
\begin{matrix}
1 & & \\
& 2 & \\
& & -2
\end{matrix} \rpar
$$

Then the invariant factors are exactly the diagonal elements $1,2,-2$. Recall that when we're working over $F[x]$, invariant factors are polynomials, but since we are over $\z$ here, invariant factors are just elements of the ring $\z$ (the $a_i$'s from the Fundamental theorem). 



\textbf{(Leibman's solution) }We have:
$$
\lpar 
\begin{matrix}
1 & -1 & 1\\
1 & -1 & -1\\
1 & 1 & 3
\end{matrix} \rpar \mapsto \lpar 
\begin{matrix}
a_1 & & \\
& a_2 & \\
& & a_3
\end{matrix} \rpar.
$$

And we need $a_1|a_2|a_3$. So then there is a basis $\Set{u_1,u_2,u_3}$ such that $\Set{a_1u_1,a_2u_2,a_3u_3}$ is a basis in $N$. And we have $a_1,a_2,a_3$ invariant factors. And $M \cong \z_{a_1} \oplus \z_{a_2} \oplus \z_{a_3}$. Then $|M| = a_1a_2a_3$. 

\item \textit{Find the cardinality of $M$. }

Over $\z$, the cardinality of $M$ is $|M| = a_1a_2a_3$, the absolute value of product of invariant factors. But we can check this by writing:
$$
M \cong \z/(1) \oplus \z/(2) \oplus \z/(-2) \cong 0 \oplus \z_2 \oplus \z_2 \cong V_4.
$$


\end{enumerate}



\item \textit{Let $G$ be an additively written abelian group defined by its generators $u_1,u_2,u_3$ and relations $2u_1 + 4u_2 + 10u_3 = 0$ and $4u_1 = 2u_2$. Represent $G$ as a product of cyclic groups. }

Just do the same thing we did above. We have a group $G \cong \z^3/N$ where $N$ is the relations module and:
$$
N = \z\Set{\lpar 
\begin{matrix}
2\\4\\10
\end{matrix} \rpar ,\lpar 
\begin{matrix}
4\\-2\\0
\end{matrix} \rpar }.
$$
Now we map:
$$
\lpar 
\begin{matrix}
2 & 4\\
4 & -2\\
10 & 0
\end{matrix} \rpar \mapsto \lpar 
\begin{matrix}
a & 0\\
0 & b\\
0 & 0
\end{matrix} \rpar.
$$
And we have $a = 2$ somehow and $\z_a \times \z_b$. 

We write:
$$
\begin{cases}
a_{11}u_1 + a_{21}u_2 + a_{31}u_3 = 0\\
a_{12}u_1 + a_{22}u_2 + a_{32}u_3 = 0\\
\end{cases}
$$
Then $e_i \mapsto u_i$ then $a_{11}e_1 + a_{21}e_2 + a_{31}e_3,a_{12}e_1 + a_{22}e_2 + a_{32}e_3$ span the kernel of the homomorphism $\z^3 \to M$. Then the relations matrix is:
$$
\lpar 
\begin{matrix}
a_{11} & a_{12}\\
a_{21} & a_{22}\\
a_{31} & a_{32}
\end{matrix} \rpar.
$$
Then we have:
$$
\lpar 
\begin{matrix}
b_1 & 0\\
0 & b_2\\
0 & 0
\end{matrix} \rpar \Rightarrow \begin{cases}
b_1v_1 = 0\\
b_2v_2 = 0
\end{cases}
$$


Where these things on the right are the "new" relations. And they imply $\z_{b_1} \times \z_{b_2} \times \z$. 





\item \textit{Prove that over a PID, a finitely generated module is projective if and only if it is free. }


\begin{proof}
Note that over a PID, we know that any finitely generated module is torsion-free if and only if it is free. 


\bb
We show that a free module is projective. Note that $M$ is projective if and only if $M$ is a direct summand of a free module, i.e. if there exists an $R$-module $N$ s.t. $M \oplus N$ is free. 


\bb
But $M$ is already free, so $M \oplus {0}$ is free and so $M$ is projective. 


\bb
Now we prove a projective module is torsion-free. Now note the ring $R$ itself is always flat, and $M_1 \oplus M_2$ is flat if and only if $M_1,M_2$ are both flat, so $M \cong R^n$ is flat since all the copies of $R$ are flat. 


\bb
Now recall that a property of flat modules is that if $Tor(M) \neq 0$, then $M$ is not flat, so if $M$ is flat, we must have $Tor(M) = 0$, so $M$ must be torsion free. And thus by the theorem stated at the beginning since it is finitely-generated, we know it is free. So $M$ is free if and only if it is projective. 
\end{proof}





\setcounter{enumi}{13}

\item \textit{The Smith normal form of a matrix $A$ is:
$$
\lpar
\begin{matrix}
1 & 0 & 0& 0\\
0 & 1 & 0 & 0\\
0 & 0 & x + 2 & 0\\
0 & 0 & 0 & (x + 2)(x^2 + 3)\\
\end{matrix}
\rpar.
$$ Find the rational canonical form of $A$ and the characteristic polynomial of $A$. 
}

Recall that the characteristic polynomial of $A$, $c_A = \prod f_i$ where $f_i$ are the invariant factors. And in the Smith normal form, the $f_i$'s along the diagonal which are not constants are the invariant factors. So $f_1 = x + 2$ and $f_2 = (x + 2)(x^2 + 3)$. So $c_A = f_1 \cdot f_2 = (x + 2)(x + 2)(x^2 + 3)$. So the elementary divisors are the factors of the invariant factors with powers, so the elementary divisors are: $(x + 2),(x + 2),(x^2 + 3)$. The companion matrix of an invariant factor is a $k \times k$ matrix where $k$ is the highest degree of the polynomial. Recall: 
\begin{Def}
And we have the \textbf{companion matrix} of $f = x^k + a_{k - 1}x^{k - 1} + \cdots + a_1x + a_0$ where $f$ is monic, given by:
$$
A_i 
= \lpar 
\begin{array}{cccc|c}
0 & 0 & \cdots & 0 &-a_0\\
1 & 0 & & & -a_1\\
0 & 1 &\ddots & &\vdots\\
0 & 0 & \ddots & & \vdots\\
\vdots & \vdots & & 1 & -a_{k - 1}
\end{array} \rpar. 
$$
\end{Def}
So for $f_1 = x + 2$ we have: $A_1 = (2)$ and for $f_2 = (x + 2)(x^2 + 3) = x^3 + 2x^2 + 3x + 6$, we have:
$$
A_2 = \lpar 
\begin{matrix}
0 & 0  & -6\\
1 & 0  & -3\\
0 & 1  & -2\\
\end{matrix} \rpar. 
$$
Then our rational canonical form is in general given by:
$$
\newcommand*{\boxone}{\multicolumn{1}{c|}{A_1}}
\newcommand*{\boxtwo}{\multicolumn{1}{|c|}{A_2}}
\newcommand*{\boxk}{\multicolumn{1}{|c}{A_k}}
\newcommand*{\boxdots}{\multicolumn{1}{|c|}{\ddots}}
\newcommand\bigzero{\makebox(0,0){\text{\huge0}}}
A = \lpar
\begin{matrix}
\boxone &  &  & \text{\huge0}\\
\cline{1-2}
 & \boxtwo &  & \\
 \cline{2-2}
 &  &  \ddots & \\
  \cline{4-4}
\text{\huge0} &  & & \boxk
\end{matrix} \rpar,
$$
Hence:
$$
A = \lpar 
\begin{matrix}
2 & 0 & 0 & 0\\
0 & 0 & 0  & -6\\
0 & 1 & 0  & -3\\
0 & 0 & 1  & -2\\
\end{matrix} \rpar. 
$$

\item \textit{Determine all similarity classes of matrices over $\Q$ with characteristic polynomial $x^4 - 4x + 3 = (x^2 + 2x + 3)(x - 1)^2$. }


Recall that the \textbf{characteristic polynomial is just the product of all the invariant factors}. So we find all possible lists of invariant factors, keeping in mind that we must have $f_1|f_2|...|f_m$. First let $IF = \Set{x^4 - 4x + 3}$. Then our matrix in RCF is just the single companion matrix:
$$
A_1
= \lpar 
\begin{array}{ccc|c}
0 & 0 & 0 & -3\\
1 & 0 & 0 & 4\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0
\end{array} \rpar. 
$$
And recall that \textbf{similar matrices have the same invariant factors}. Now let $IF = \Set{(x^2 + 2x + 3)(x - 1),(x - 1)}$. Clearly this is the only other list which will satisfy the definition of invariant factors (to check this just take all combinations of factors of the characteristic polynomial and check if list divides sequentially). And in this case we write the invariant factors from smallest to largest $f_1 = x - 1$ and $f_2 = x^3 + x^2 + x - 3$ and find their companion matrices: 
\bee
A_1 = (1)\\
A_2 = \lpar 
\begin{matrix}
0 & 0 & 3\\
1 & 0 & -1\\
0 & 1 & -1\\
\end{matrix} \rpar.
\eee

Then the matrix written in rational canonical form is:
$$
\lpar 
\begin{matrix}
1 & 0 & 0 & 0\\
0 & 0 & 0 & 3\\
0 & 1 & 0 & -1\\
0 & 0 & 1 & -1\\
\end{matrix} \rpar. 
$$
And these are the two similarity classes. 


\setcounter{enumi}{15}

\item \textit{Assume that a linear tranformation $T$ of a 5-dimensional space satisfies $T^3 = 1$. What can be the rational canonical forms of $T$?}
\bb


My solution. Note that $f(x) = x^3 - 1$ is a polynomial s.t. $f(T) = 0$. But note for any such polynomial we must have that $m_T|f$, by definition of the minimal polynomial. So we know $m_T|x^3 - 1$. Since we factor $x^3 - 1 = (x - 1)(x^2 + x + 1)$, we know:
$$
m_T \in \Set{x^3 - 1,x - 1,x^2 + x  +1}.
$$
But also note that $m_T = f_m$, the largest invariant factor, and we must have that $f_1|f_2|...|f_m$ and that $f_1\cdots f_m = c_T$, the characteristic polynomial of $T$, which has degree the same as the dimension of our space, which is given as 5. 

\bb
\textbf{Case 1: }Let $m_T = x^2 + x + 1$. Then since this does not factor over $\Q$, we know that $f_1 = \cdots = f_m = x^2 + x + 1$. Then we know $c_T = (x^2 + x + 1)^n$ for some integer $n$, which is impossible because that will always have even degree, and $c_T$ must have degree 5. So this is impossible. 
\bb

\textbf{Case 2: }let $m_T = x - 1$. Then $T - I = 0$ which means $T = I$. And we know \textbf{RCF of the identity matrix $I$ is exactly $I$. }

\bb

\textbf{Case 3: }Let $m_T = x^3 - 1$. Then we have two possible subcases for the lists of invariant factors. We note here that we can have two invariant factors which are the same. 
\textbf{Subcase A: } $f_3 = x^3 - 1,f_2 = x - 1,f_3 = x - 1$. Then we have $A_1 = A_2 = (1)$ companion matrices. And we have:
$$
A_3 = \lpar 
\begin{matrix}
0 & 0 & 1\\
1 & 0 & 0\\
0 & 1 & 0
\end{matrix} \rpar.
$$
Hence the RCF is:
$$
A = \lpar 
\begin{matrix}
1 & 0 & 0 & 0 & 0\\
0 & 1 & 0 & 0 & 0\\
0 & 0 & 0 & 0 & 1\\
0 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & 0
\end{matrix} \rpar. 
$$

\textbf{Subcase B: } $f_2 = x^3 - 1, f_1 = x^2 + x + 1$. Then we have:
$$
A_1 = \lpar 
\begin{matrix}
0 & -1\\
1 & -1\\
\end{matrix} \rpar.
$$
And the RCF matrix is just the 5 by 5 matrix with $A_1$ as the top block and the companion matrix $A_3$ from the previous subcase as the next block. 












\bb\bb\bb
Leibman's solution: $T: V\to V$. $T$ satisfies $x^3 - 1 = (x - 1)(x^2 + x + 1)$. But what field this is over matters since we don't know if this right part factors, so we assume it's over $\Q$. Over $\c$ it splits completely, and if one of the factors is the minimal polynomial, then it's just a scalar matrix. So we assume $V$ is a $\Q$-vector space. If $m_T(x) = x - 1$, then we have $T = I$. If $m_T(x) = x^2 + x + 1$, then we have $f_1 = f_2 = f_3 = x^2 + x + 1$, so deg$(\prod f_i)$ is even, and $\neq 5$. Last case is $m_T(x) = x^3 - 1$, then either $f_1 = f_2 = x-1$, $f_3 = x^3 - 1$, or $f_1 = x^2 + x + 1$, $f_3 = x^3 = 1$. We have:
$$
\newcommand*{\boxone}{\multicolumn{1}{c|}{1}}
\newcommand*{\boxtwo}{\multicolumn{1}{|c|}{1}}
\newcommand*{\boxk}{\multicolumn{1}{|c}{A_k}}
\newcommand*{\boxdots}{\multicolumn{1}{|c|}{\ddots}}
\newcommand\bigzero{\makebox(0,0){\text{\huge0}}}
A = \lpar
\begin{array}{cc|ccc}
\boxone & 0 & & & \text{\huge0}\\
\cline{1-2}
0 & \boxtwo &  & \\
\hline
 & & 0&0   & 1\\
 &&1 & 0 & 0\\
\text{\huge0} &  &0 &1 &1
\end{array} \rpar. 
$$
Or:
$$
\newcommand*{\boxone}{\multicolumn{1}{c|}{0}}
\newcommand*{\boxtwo}{\multicolumn{1}{|c|}{-1}}
\newcommand*{\boxk}{\multicolumn{1}{|c}{A_k}}
\newcommand*{\boxdots}{\multicolumn{1}{|c|}{\ddots}}
\newcommand\bigzero{\makebox(0,0){\text{\huge0}}}
A = \lpar
\begin{array}{cc|ccc}
\boxone & -1 & & & \text{\huge0}\\
\cline{1-2}
1 & \boxtwo &  & \\
\hline
 & & 0&0   & 1\\
 &&1 & 0 & 0\\
\text{\huge0} &  &0 &1 &1
\end{array} \rpar. 
$$

And elementary divisors $x^2 + x + 1$, $x^2 + x + 1,x - 1$. 
\bb

\item \textit{The characteristic polynomial of an $\R$-matrix $A$ is $c_A = (x - 2)(x^2 + 3)$. }

\begin{enumerate}
\item  \textit{Find the minimal polynomial of $A$. }

Note we must have that the largest invariant factor $f_m = m_A$, the minimal polynomial, and that the invariant factors multiply to $c_A$. So $m_A|c_A$. So in \textbf{Case 1}, we have that $m_A = x - 2$, then all other invariant factors must also be $x - 2$, but $c_A$ is not a power of $x - 2$, so this is impossible. So then in \textbf{Case 2} we have $m_A = (x^2  + 3)$, and we have a similar problem, so we are left with \textbf{Case 3} in which $m_A = c_A = (x - 2)(x^2 + 3) = x^3 + 3x - 2x^2 - 6$. Thus we only have a single invariant factor. 

\item \textit{Find the rational canonical form of $A$. }

Recall that the RCF of $A$ is the matrix with each of the companion matrices of the invariant factors as its diagonal blocks. But we only have the one invariant factor, so:
$$
A_1 = \lpar 
\begin{matrix}
0 & 0 & 6\\
1 & 0 & -3\\
0 & 1 & 2
\end{matrix} \rpar. 
$$

\item \textit{Find a matrix of the form $\lpar \begin{matrix}
* & 0 & 0\\
0 & * & *\\
0 & * & *\\
\end{matrix} \rpar$ which is similar to $A$. }

We just find the companion matrices of the factors of the characteristic polynomial $c_A$ up to powers. So the companion matrix of $x - 2$ is just $(2)$ and the companion matrix of $x^2 + 3$ is $\lpar 
\begin{matrix}
0 & -3\\
1 & 0
\end{matrix} \rpar$. So we have:
$$
\lpar 
\begin{matrix}
2 & 0 & 0\\
0 & 0 & -3\\
0 & 1 & 0\\
\end{matrix} \rpar.
$$
Which is similar to $A$. Recall that \textbf{two matrices are similar if and only if they have the same characteristic polynomial and the same characteristic polynomial and $n \leq 3$}. Note this does not work for $n > 3$. 


\item \textit{If $A$ has Jordan canonical form, find it. }

\begin{rem}
Jordan canonical form = canonical Jordan form = Jordan normal form. 
\end{rem}

Recall that we have Jordan form if and only if we can write the \textbf{minimal polynomial} as a product of powers of linear factors (splits). But this is not true since we are not over an algebraically closed field, and $x^2+3$ does not split. 






\end{enumerate}

\item \textit{Find the Jordan canonical form of the matrix whose rational canonical form is $\lpar \begin{matrix}
0 & 0 & 1\\
1 & 0 & -3\\
0 & 1 & 3
\end{matrix} \rpar$. }

Since we clearly only have one block by inspection, our minimal polynomial is the only invariant factor, and it's also $c_A$. 
So this invariant factor must have degree 3 since our single companion matrix is 3 by 3, and it is monic and the remaining coefficients are $-3,3,-1$ so it is $m_T = x^3 -3x^2 + 3x - 1$. Now we want to factor this. Try $x = 0$, it's not a root, so try $x = 1$. It is a factor, so it's divisible by $x - 1$. And we get $(x - 1)(x^2 -2x + 1) = (x - 1)^3$. So we have a single elementary divisor. And its Jordan block is the whole Jordan canonical form. Recall that the Jordan form of a divisor $(x - \lambda)^k$ is a $k \times k$ matrix with $\lambda$'s on the diagonal, and $1$'s above the diagonal. So our Jordan form is:
$$
\lpar 
\begin{matrix}
1 & 1 & 0\\
0 & 1 & 1\\
0 & 0 & 1
\end{matrix} \rpar. 
$$

\item \textbf{Possible exam problem: Prove rank$\phi^* = rank\phi$. }



\end{enumerate}



\chapter{Tensors outside of algebra}

\textbf{Wednesday, February 14th \heart}



Let $M$ be an $R$-module. Consider $M \tens M^* \to R$ which is naturally induced by $(u,f) \mapsto f(u)$, which is bilinear. 

\begin{Def}
The above homomorphism is called \textbf{contraction of tensors}. 
\end{Def}

Let $M$ is a free module of finite rank (for example, a finite dimensional vector space). Usually we define contraction restricted to vector spaces. We choose a basis $\Set{u_1,...,u_n}$ in $M$ a dual basis $\Set{f_1,...,f_n}$ in $M^*$. Then what is contraction in coordinates? First what elements of $M \tens M^*$? Then:
$$
\Set{u_i \tens f_j:i,j = 1,..,n}
$$
form a basis in $M \tens M^*$. $\forall w \in M \tens M^*$:
$$
w = \sum_{i,j = 1}^n a_{ij}u_i \tens f_j.
$$
How do we know this is a basis in $M \tens M^*$? Recall: 
$$
(M_1 \oplus M_2) \tens N \cong (M_1 \tens N) \oplus (M_2 \tens N).
$$
Then let $M \oplus Ru_i,N = \oplus Rv_j$, then what is the tensor product of these? Obvious. Done. 

Going back to the task at hand, we define $a_{ij}$ as the \textbf{coordinates of $w$}. 

\begin{Def}
The \textbf{contraction }of $w$ is:
$$
\sum a_{ij}f_j(u_i) = \sum a_{ij}\delta_{ij} = \sum_{i  =1}^na_{ij} = trace\lpar 
\begin{matrix}
a_{11}& \cdots& a_{1n}\\
\vdots & & \vdots\\
a_{n1}& \cdots& a_{nn}\\
\end{matrix} \rpar.
$$
So "trace" is a function of a tensor, not its matrix. 
\end{Def}

Let $M,N$ be $R$-modules. Consider $N \tens M^*$. We have a homomorphism $N \tens M^* \to Hom(M,N)$ given by $v \tens f \mapsto \phi \in Hom(M,N)$, where $\phi(u) = f(u)v$. In fact this is a composition of contraction. We take $v \tens f \tens u$ and contract it to get $f(u)v$. So in fact we take $N \tens M^* \tens M$ and contract it. Note the above map is bilinear. In general it doesn't have to be an isomorphism. If $N,M$ are free modules of finite rank, then we claim that it is an isomorphism. 

\begin{lem}
If $N,M$ are free modules of finite rank, then $N \tens M^* \to Hom(M,N)$ given by $v \tens f \mapsto \phi \in Hom(M,N)$, where $\phi(u) = f(u)v$ is an isomorphism. 
\end{lem}

\begin{proof}
let $M,N$ be free of ranks $m,n$ and let $\Set{u_1,...,u_n}$ and $\Set{v_1,...,v_m}$ be their bases. Let $\Set{f_1,...,f_n}$ be the dual basis in $M^*$. Then $\Set{v_j \tens f_i}$ is the basis in $N \tens M^*$ as proven above. So let's check what homomorphism corresponds to this tensor. For any $i,j$: $$
v_j \tens f_i \mapsto \phi_{ij}, \phi_{ij}(u_k) = f_i(u_k)v_j = \delta_{ik}v_j = \begin{cases}
v_j & i = k\\
0 & i \neq k
\end{cases}. 
$$
where the matrix of $\phi_{ij}$ is a matrix with $n$ rows and $m$ columns with zeroes everywhere except the $i$-th column of the $j$-th row, where there is a $1$. So $\Set{\phi_{ij}}$ is a basis in $Hom(M,N)$. So, $N \tens M^* \to Hom(M,N)$ is an isomorphism. 
\end{proof}

So homomorphisms $M \to N$ are tensors from $N \tens M^*$. 

So we have $Hom(M,M) \cong M \tens M^*$. 

"Trace" is defined on $M \tens M^*$. So in $Hom(M,M)$, and doesn't depend on the choice of basis. So "trace of a hom-sm" is well-defined. 

\textbf{What is trace??}

\begin{rem}
You have a mapping $M \tens M^* \to R$ defined by $u \tens f \mapsto f(u)$. And note that this is independent of basis:
\begin{center}
\begin{tikzcd}
 & M \otimes M^* \arrow[ldd] \arrow[r] & R \\
 & u \otimes f \arrow[r, maps to] \arrow[ldd] & f(u) \\
Hom(M,M) \arrow[ruu] &  &  \\
\phi(v) = f(v)u \arrow[ruu] &  & 
\end{tikzcd}.
\end{center}
\end{rem}

Composition of homomorphisms: $M \to N \to K$ - they are given by tensors from $N \tens M^*$ and $K \tens N^*$. 

\begin{Def}
\textbf{Composition} is contraction of a tensor from $K \tens N^* \tens N \tens M^*$ with respect to $N$. 
\end{Def}

If we have $M \overset{\phi}{\to}N \overset{\psi}{\to}K$. Apply $\psi \circ \phi(u)$, take $\psi \in K \tens N^*$, and $\phi \in N \tens M^*$, and $u \in M$ so we have:
$$
\psi \tens (\phi \tens u).
$$
Contract $M^* \tens M$ and then $N^* \tens N$, so the composition $\psi \circ \phi$ is the result of contraction of $N^* \tens N$. 

In coordinates: 
\bee 
\phi &\leftrightarrow (a_{ij})\\
\psi &\leftrightarrow (b_{kl})\\
\psi \tens \phi &\leftrightarrow (b_{kl}a_{ij})\\
\psi \circ \phi &\leftrightarrow \lpar \sum_{i = 1}^nb_{ki}a_{ij} \rpar  = (c_{kj}).
\eee


\bb\bb\bb

\begin{Def}
A \textbf{bilinear form} is a mapping $\beta:M \times M \to R$ s.t. everything is preserved. 
\end{Def}

\begin{rem}
Bilinear forms $\leftrightarrow$ tensors from $M^* \tens M^*$ given by $f \tens g \mapsto \beta$, and $\beta(u,v) = f(u) g(v)$. The trace is not defined here. \textbf{Seriously wtf is the trace?}. 
\end{rem}

\begin{Def}
$M \tens M \tens \cdots \tens M \tens M^* \tens \cdots \tens M^*$ where we have $k$ copies of $M$ and $l$ copies of $M^*$ are called $(k,l)$-tensors, or tensors of $(k,l)$-type. And $\dim = n^{k + l}$, where $n = \dim M$. Then it is $k$-times contravariant and $l$ times covariant tensors. Bilinear forms are $(0,2)$-tensors. 
\end{Def}

\begin{rem}
Each tensor from this space is represented by a $k + l$-dimensional matrix. $n \times n \times \cdots \times n$, $k + l$ times. There is a tradition to write coordinates: $$
a_{j_1,...,j_l}^{i_1,...,i_k}.
$$
where contravariant on top, and covariant on bottom. 
\end{rem}

Consider $\nabla f = df$ - $(0,1)$-tensor, covector. Christofel symbols $\Gamma_{j,l}^i$. Curvature tensor: $R_{j,k,l}^i$. 







\chapter{Sample problems to midterm III}

\begin{enumerate}[label=\arabic*.]
\item \textit{If $K/F$ is a finite extension, prove that it is algebraic. }

\begin{proof}
Let $F$ be a field and $K$ an extension with $[K:F] = n < \infty$. Let $\alpha \in K$. Then the dimension of $K$ over $F$ is $n$. So we know that $\Set{1,\alpha,...,\alpha^n}$ are linearly dependent. So there exists $a_i$ not all zero such that $a_0 + a_1\alpha + \cdots + a_n\alpha^n = 0$. Then we know $f(x) = a_0 + a_1x + \cdots + a_nx^n$ is a nonzero polynomial in $F[x]$ for which $\alpha$ is a root, so $\alpha$ is algebraic. So $K/F$ is algebraic since this holds for all $\alpha$. 
\end{proof}

\item \textit{Find the minimal polynomial over $\Q$ of $\sqrt{2} + 2\sqrt{5}$. }

Let's just try taking powers. Observe:
\bee
(\sqrt{2} + 2\sqrt{5})^2 &=  2 + 4\sqrt{10} + 20\\
&= 22 + 4\sqrt{10}.
\eee
So take $x^2 - 22$ and we get $4\sqrt{10}$. So square this and we get:
$$
16\cdot 10 = 160.
$$
So if we take $f(x) = (x^2 - 22)^2 - 160$, we get zero when we plug in $\sqrt{2} + 2\sqrt{5}$. So we know $m_{\alpha, \Q}|f(x)$. We know $[Q(\sqrt{2}):\Q] = 2$, and $[\Q(2\sqrt{5}):Q(\sqrt{2})] = 2$, since these two elements are linearly independent over $\Q$. So the degree of $\Q(\sqrt{2},2\sqrt{5})$ over $\Q$ is 4. It can easily be shown that $\Q(\sqrt{2},2\sqrt{5}) = \Q(\sqrt{2} + 2\sqrt{5})$. And since our polynomial has degree 4, it must be minimal. 


\item \textit{Find the degree and minimal polynomial over $\Q$ of a root $\alpha$ of the polynomial $x^2 + \beta x + 2$, where $\beta$ is a root of $x^2 + x + 1$. }

Observe:
\bee
\beta &= \fracc{-1 \pm \sqrt{-3}}{2}\\
\beta\bar{\beta} &= \fracc{-1 + \sqrt{-3}}{2}\cdot \fracc{-1 - \sqrt{-3}}{2}\\
&= \fracc{1 + 3}{4} = 1.
\eee
Then we take:
\bee
(x^2 + \beta x + 2)(x^2 + \bar{\beta}x + 2) &= x^4 - x^3  + 5x^2 - 2x + 4.
\eee
By the rational root theorem, all rational roots of this polynomial are of the form $\pm a/\pm 1$ where $a|4$. And none of these work. 





\bb


\item \textit{Find the degree of the extension $\Q(\alpha)/\Q$ where $\alpha = \sqrt{1 + \sqrt[3]{2}}$. }

We find the minimal polynomial. Note:
\bee
\alpha^2 &= 1 + \sqrt[3]{2}\\
(\alpha^2  - 1)^3 = 2\\
(\alpha^2  - 1)^3 - 2 = 0\\
\eee

So we know $f(\alpha) = 0$ for:
\bee
f(x) &= (x^2 - 1)^3 - 2\\
&= (x^4 - 2x^2 + 1)(x^2 - 1) - 2\\
&= x^6 - x^4 - 2x^4 + 2x^2 + x^2 - 1 - 2\\
&= x^6 - 3x^4 + 3x^2 - 3.
\eee

By the rational root theorem, all the rational roots are of the form $\fracc{\pm p}{\pm q}$ where $p|(-3)$ and $q|1$, training coefficient, leading coefficient. So then our possible roots are $-3,3,-1,1$. 

Since all terms have even degree, we have:
\bee
f(3) &= f(-3) = 729 - 3(81) + 27 - 3 = 510\\
f(1) &= f(-1) = 1 - 3 + 3 - 3 = -2.
\eee

Now by Eisenstein's, it's irreducible since 3 divides all but leading coefficient, doesn't divide leading coefficient, and 9 doesn't divide last. 


\item \textit{Prove that any extension of prime degree $p$ has no nontrivial proper subextensions. }

\begin{proof}
Let $E/K/F$ be a tower of extensions. Then let $[E:F] = p$, prime. Then since it's finite, we know $[E:F] = [E:K][K:F]$. So $[K:F]$ is either 1 or $p$. Either way it's done. 
\end{proof}

\item \textit{If $K/F$ and $L/F$ are subextensions of an extension $E/F$ with $[K:F] = n$ and $[L:F] = m$ being relatively prime, prove that $[KL:F] = nm$. }

\begin{proof}
We have:
\begin{center}
\begin{tikzcd}
 & K_1K_2 \arrow[ld, "\leq m"', no head] \arrow[rd, "\leq n", no head] &  \\
K_1 \arrow[rd, "n"', no head] &  & K_2 \arrow[ld, "m", no head] \\
 & F & 
\end{tikzcd}.
\end{center}
Now note that $n|[KL:F]$ and $m|[KL:F]$. But since $(n,m) = 1$, we must have $nm$. 
\end{proof}

\item \textit{Find a basis of the field $\Q(\sqrt[3]{2},\sqrt{3})$ as a vector space over $\Q$. }

It is $\Set{1, \sqrt[3]{2}, \sqrt[3]{4}, \sqrt{3}, \sqrt[3]{2}\sqrt{3}, \sqrt[3]{4}, \sqrt{3}}$, since these two generators are linearly independent. 

\item \textit{Find a basis of the field $\Q(\sqrt[3]{2},\sqrt[3]{3})$ as a vector space over $\Q$. }

Multiply them together, you should get 9. 




\counter{enumi}{12}

\item \textit{Let $\bar{F}$ be an algebraic closure of a field $F$. Prove that for every finite extension $K/F$ there is a copy of it in $\bar{F}/F$. (That is, there is a monomorphism $\phi:K \to \bar{F}$ such that $\phi|_F = Id_F$. }

\begin{proof}
Let $K = F(\alpha_1,...,\alpha_n)$. $\exists \phi: F(\alpha_1) \leftrightarrow \bar{F}$ by $\alpha_1 \mapsto \beta_1$ - root of $m_{\alpha_1,F}$ in $\bar{F}$. There exists extension of $\phi:F(\alpha_1,\alpha_2) \to \bar{F}$, by $\alpha_2 \mapsto \beta_2$- root of $m_{\alpha_2,F(\alpha_1)}$ in $\bar{F}$. 
\end{proof}

\item \textit{Moreover, (using Zorn's Lemma) prove that for every algebraic extension $K/F$ there is a copy of it in $\bar{F}/F$. }


\begin{proof}
Consider the set $S$ of embeddings $\phi:L \to \bar{F}$ where $F$ maps to both of these as well, where $K/L/F$. Define $\phi_1 \leq \phi_2$ if $L_1 \sub L_2$, and $\phi_2|_{L_1} = \phi_1$. 

By Zorn, $\exists$ max element in $S$. $\phi:L \to \bar{F}$. If $L \neq K$, $\alpha \in K \setminus L$. then $\phi$ can be extended to $L(\alpha)$. 
\end{proof}

\counter{enumi}{15}

\item \textbf{Statement is wrong, ask Ryan. }

\item \textit{$K = F(\alpha_1,...,\alpha_k)$. }

Adjoin all conjugates of $\alpha_1,...\alpha_k$. Let $f = m_{\alpha_1,F}\cdots m_{\alpha_k,F}$. Let $E$ be the splitting field of $F$. These are all equal. 


\item We need to use induction for this one. 

We have: 
\begin{center}
\begin{tikzcd}
F(\alpha_1,...,\alpha_k) = K \arrow[d, "n_k", no head] &  &  &  & E \arrow[ld, no head] \\
\vdots \arrow[d, "n_3", no head] &  &  & \udots \arrow[ld, no head] &  \\
F(\alpha_1,\alpha_2) \arrow[d, "n_2", no head] &  & E_2 &  &  \\
F(\alpha_1) \arrow[d, "n_1", no head] & E_1 \arrow[ru, "\leq n_2 !"', no head] &  &  &  \\
F \arrow[ru, "\leq n_1 !"', no head] &  &  &  & 
\end{tikzcd}.
\end{center}




We define $[K:F] = n$. $E_1$ is the splitting field of $m_{\alpha_1,F}$. And we have $[E_1:F] \leq n_1!$. We have $E_2$  - the splitting field of $m_{\alpha_2,F}$ such that $[E_2:F] \leq n_1!n_2!$. 

We know that $n = n_1\cdots n_k$. We have $n_1!\cdots n_k! \leq n!$. 

\counter{enumi}{19}

\item \textit{$K_1/F,K_2/F$ normal. Prove that $K_1K_2/F$ are normal. }

Recall the theorem that says \textbf{normal extension is exactly the splitting field. } Define an element is \textbf{normal} if all its conjugates are in the same field. Take generators of $K_1,K_2$, take their minimal polynomials, multiply them all, and this will be the splitting field of that polynomial. 

\counter{enumi}{21}

\item 

\begin{enumerate}
\item \textit{Prove that the polynomial $x^n - x$ is separable over every field. }

\begin{proof}
$f' = nx^{n - 1} - 1$. If the characteristic is $n - 1$, then $f'|f$. Then $n = 1$. Then we have an issue. \textbf{So this problem statement is incorrect. }
\end{proof}
\end{enumerate}



















\counter{enumi}{23}
\item \textit{Give an example of a non-separable field extension. }

Consider $x^p - t = f$. We have $f' = 0$. So $f$ is inseparable. We claim $f$ is irreducible. 

\begin{proof}
Let $\alpha = \sqrt[p]{t}$. Then $f = (x - \alpha)^p$ in $F(\alpha)$. If $f = gh$, then $g = (x - \alpha)^k$, where $h = (x - \alpha)^{p - k}$. We have $g,h \in F[x]$, with $g$ irreducible. Then $\deg \alpha = \deg g = k$. Then $\alpha$ is separable. But this is a contradiction. But $g$ is separable, so cannot have multiple roots. Inseparable polynomials only contain powers of $p$ as exponents. $g$ is separable since it is not of the form:
$$
a_nx^{np} + a_{n - 1}x^{(n - 1)p} + \cdots.
$$
Recall that we are trying to prove that $f$ is irreducible. We need an inseparable, irreducible polynomial. 
\end{proof}


\counter{enumi}{25}
\item \textit{Prove that for any prime $p$ and any $n \in \n$, there exists an irreducible polynomial $f \in \F_p[x]$ of degree $n$. }

\begin{proof}
We proved that $\forall p, \forall n$, $\exists F$ s.t. $[F:\F_p] = n$ ($F$ is the splitting field of $x^{p^n} - x$)

Then $f^* = \langle \alpha \rangle$, so $F = \F_p(\alpha)$. So $\deg_{\F_p}\alpha = n$, so $f = m_{\alpha,\F_p}$ is irreducible of $\deg$ $n$. 
\end{proof}
\end{enumerate}





\chapter{Sample problems to final}

\textbf{Monday, April 23rd}

\begin{enumerate}[label=\arabic*.]

\item \textit{Using the theorem on the primitive element, prove easily that if $K/F$ is a separable extension of degree $n$ and $E$ is a field containing $K$, then there are at most $n$ distinct embeddings of $K$ into $E$, and if $E$ is normal, then there are exactly $n$ such embeddings. }

\begin{proof}
Recall the theorem on the primitive element states: 
\bb

\begin{quotation}
If $K/F$ is finite and separable, then there is $\alpha \in K$ such that $K = F(\alpha)$. ($\alpha$ is called ``primitive" for K.)
\end{quotation}
\bb


Note we already have that $K/F$ is finite and separable. So we know there exists $\alpha \in K$ such $K = F(\alpha)$. Note that any embedding $\phi$ of $K$ into $E$ must be such that $\phi|_F = \mathrm{Id}_F$. So we are mapping $\phi:F(\alpha) \to E$. Now $\phi$ is uniquely defined by its action on $\alpha$, and it must send $\alpha$ to a conjugate of $\alpha$. Since our extension is of degree $n$, we know there are at most $n$ conjugates of $\alpha$ in $K$. Thus we know there are at most $n$ embeddings of $K$ into $E$. 

\bb
Now if $E$ is normal, we know all conjugates of $\alpha$ are in $E$, so we have all $n$, so there are exactly $n$ embeddings. 
\end{proof}

\bb

\item \textit{Explain why the Galois group of a separable polynomial of degree $n$ is isomorphic to a subgroup of $S_n$. }

\begin{proof}
Let $f$ be a separable polynomial of degree $n$. Recall that the Galois group of $f$ is the Galois group of its splitting field. Also recall that if $K$ is the splitting field of a separable polynomial, then $K$ is normal and separable. Thus $K$ is Galois by definition. Now $\forall \phi \in \gal(K)$, we know that $\phi$ is uniquely defined by its action on $\alpha_i$, the roots of $f$, and since it is an automorphism, we know that $\phi$ maps $\alpha_i$ to some other root of $f$. Since $f$ is separable, we know it has no multiple roots, and thus each automorphism is simply a permutation of the indices of the roots, and since there are $n$ roots. Since $\gal(K/F)$ is a group, we can injectively map to $S_n$, and the image under this map will be a subgroup of $S_n$ isomorphic to $\gal(K)$. 
\end{proof}

\counter{enumi}{2}

\item \textit{$K/F$ separable, $[K:F] = n$. Prove that $E$-Galois closure $\Rightarrow [E:F] \leq n!$. }

\begin{proof}
Let $K = F(\alpha_1,...,\alpha_k)$. Since $K$ is finite and separable, we may apply the primitive element theorem, so $K = F(\alpha)$ for some $\alpha \in K$, so let $f = m_{\alpha,F}$. Now since $K$ is separable, by definition, we know that $f$ is separable. We claim that $E$ is the splitting field of $f$. So we must check, does $f$ split in $E$? It does because ALL roots of $f$ are in $E$ since we get all conjugates of $\alpha$ since it is Galous (normal) closure. And since $E$ is minimal such closure, we know that it is a splitting field. Since $\deg f = n$, we know $[E:F] \leq n!$ by a result from group theory. (\textbf{ask leibman})
\end{proof}

\begin{proof}
\textbf{Leibman's proof: }
The Galois closure is the splitting field of something. Why? Write $K = F(\alpha_1,...,\alpha_k)$. Take $f = m_{\alpha_1}\cdots m_{\alpha_k}$. Then $E$ is a splitting field of $f$. Find $\alpha$ such that $K = F(\alpha)$. Let $f = m_{\alpha,F}$, then $E$ is a splitting field of $f$. $H\leq G$ such that $|G:H| \leq n$, then $\exists N \leq H$ such that $N \norm G$, and $|G:N| \leq n!$. This result is from group theory. Note $H \leftrightarrow K$, $N \leftrightarrow E$, then $E/F$ is normal, $K \sub E$, and $[E:F] \leq n!$. $E$ is Galois closure since it is normal. 




And $K = \bigcap_{a \in G}aHa^{-1}$. We have $\gal \to S_n$ by $\phi \mapsto$ action on the conjugates of $\alpha$. So $|G| \leq n!$, so $[E:F] \leq n!$.
\end{proof}

\item \textit{Let $K/F$ be a Galois extension with $[K:F] = n$, if $p$ is a prime such that $p^r|n$, prove that there is a subextension $L/F$ of $K/F$ with $[L:F] = n/p^r$. }

\begin{proof}

\end{proof}




\counter{enumi}{5}

\item 
Note $K/F$ is Galois. 
\begin{center}
\begin{tikzcd}
 & K = K_1K_2 \arrow[ld, no head] \arrow[rd, no head] &  &  &  & 1 \arrow[ld, no head] \arrow[rd, no head] &  \\
K_1 \arrow[rd, no head, dashed] &  & K_2 \arrow[ld, no head] & \rightarrow & H_1 \arrow[rd, no head, dashed] &  & H_2 \arrow[ld, no head] \\
 & F &  &  &  & G & 
\end{tikzcd}
\end{center}
Note the dotted line represents normal inclusion. So $H_1 \norm G$. And we have:
\bee
K = K_1K_2 &\iff H_1 \cap H_2 = 1\\
K_1 \cap K_2 = F &\iff \langle H_1,H_2 \rangle = G\\
K_1/F \text{is normal} &\iff H_1 \norm G.
\eee
So we know $\langle H_1,H_2 \rangle = H_1H_2$, and $G = H_1 \semi H_2$. And thus $G/H_1 \cong H_2$. So $G \cong H_1 \semi (G/H_1)$. 
So we have $H_1 = \gal(K/K_1)$ and $H_2 = \gal(K/K_2)$. 

\end{enumerate}

\begin{rem}
$\Q(\wer[n]{a})/\Q$ not normal for $n \geq 3$, (if $x^n - a$ is irreducible). Conjugates of $\wer[n]{a}$ are $\omega^k\wer[n]{a}$, $k = 0,...,n - 1$, and $\omega \notin \R$, so $\omega\wer[n]{a} \notin\Q(\wer[n]{a})$. So the Galois closure of $\Q(\wer[n]{a})$ is not abelian. 
\end{rem}

We discuss the Galois group of $x^n - a$, where $a \in \Q$, $a > 0$. let $\alpha = \wer[n]{a}$. The splitting field is $\Q(\alpha,\omega)$. And $\omega = e^{2\pi i/n}$. 
Observe:
\begin{center}
\begin{tikzcd}
 & K \arrow[rd, "\z_n", no head] \arrow[ld, no head] &  \\
\Q(\alpha) \arrow[rd, no head] &  & \Q(\omega) \arrow[ld, "z_n^*", no head, dashed] \\
 & \Q & 
\end{tikzcd}.
\end{center}
Note $K$ is a radical extension, it is obtained from $\Q(\omega)$ by adjoining $\alpha$. And note $\Q(\omega)$ contains a primitive root of unity of degree $n$. We prove that the stuff on the edge from $\Q(\omega)$ to $K$ ($\gal(K/\Q(\omega))$) is cyclic. Note we have $\phi_k:\alpha \to \omega^k\alpha$. So $G = \Set{\phi_k \text{ for some } k}$. So we have a homomorphism $G \to \z_n$, given by $k \to \phi_k$. It is injective since the $\phi$ are uniquely defined. Since target space is finite, it's an isomorphism.  We know that the top right bar is cyclic. 
$\Q = \Q(\alpha) \cap \Q(\omega)$?
If so, $\gal \cong \z_n \semi \z_n^*$. The claim is that if $n$ is odd, $\Q(\alpha) \cap \Q(\omega) = \Q$. Let's postpone this, Leibman does not know. If we assume $n$ is prime, then $\Q(\alpha)$ has no subextensions, so it's trivial. 

\chapter{Module theory definitions and results}

\chapter{Field theory definitions and results}

\begin{Def} 
$\forall$ field $F$, roots of $x^n = 1$ are called the \textbf{roots of unity}. 
\end{Def}
\begin{Def}
The splitting field of $x^n - 1$ is called the $n$-th \textbf{cyclotomic extension of $F$}. 
\end{Def}

\begin{Def}
Generators of this group are called \textbf{primitive roots of unity}. 
\end{Def}

\begin{Def}
The \textbf{$n$-th cyclotomic field} is the splitting field of $x^n - 1 \in \Q[x]$. 
\end{Def}

\begin{Def}
Galois extension $K/F$ is \textbf{abelian} if Gal$(K/F)$ is abelian. $K/F$ is <insert nice property here> if Gal$(K/F)$ is <insert nice property here>. (cyclic, nilpotent, solvable,...)
\end{Def}

\begin{Def}
An extension $K/F$ is \textbf{algebraic} if $\forall \alpha \in K$, $\alpha$ is algebraic over $F$. 
\end{Def}

\begin{rem}
If $K/F$ is finite, then it is algebraic (since $\forall \alpha \in K$, $F(\alpha)/F$ is finite. 
\end{rem}

\begin{Def}
The \textbf{normal closure} of an extension $K/F$ is the minimal extension which contains all conjugates of all elements of $K/F$. 
\end{Def}

\begin{Def}
A \textbf{conjugate} is a root of the same minimal polynomial: $\sqrt{2}\mapsto -\sqrt{2}$; $\sqrt[3]{2} \mapsto \omega\sqrt[3]{2},\omega^2\sqrt[3]{2}$. 
\end{Def}

 \begin{Def}
 If $G$ is abelian, $G \cong \z_{n_1} \times \cdots \times \z_{n_k}$, so if $K/F$ is abelian, then $K$ is a \textbf{direct composite}, $K = K_1K_2\cdots K_k$ of cyclic subextensions. 

 
 We have:
 \begin{center}
 \begin{tikzcd}
 &  & K \arrow[lld, no head] \arrow[ld, no head] \arrow[d, no head] \arrow[rd, no head] \arrow[rrd, no head] &  &  \\
K_1 \arrow[rrd, "n_1"', no head] & K_2 \arrow[rd, "n_2", no head] & K_3 \arrow[d, "n_3", no head] & \cdots \arrow[ld, no head] & K_k \arrow[lld, "n_k", no head] \\
 &  & F &  & 
\end{tikzcd},
 \end{center}
 such that $\forall i, K_i \cap \prod_{j \neq i}K_j = F$. Then $K_1 = \fix(\z_{n_2}\times \cdots \times \z_{n_k})$. Let's say that $K$ is a \textbf{direct composite} of $K_1$ and $K_2$ if:
$$
 [K:F] = [K_1:F][K_2:F].
$$
 
  \end{Def}

\begin{Def}
A \textbf{polyradical }extension is an extension of the form:
\begin{center}
\begin{tikzcd}
K_n \arrow[d, no head] \arrow[ddd, "polyradical", no head, bend left] \\
\vdots \arrow[d, no head] \\
K_1 \arrow[d, no head] \\
F
\end{tikzcd}.
\end{center}
And the short lines are the simple radical extensions. 
\end{Def}

\begin{Def}
$D = d^2 = \prod_{i < j}(\alpha_j - \alpha_i)^2$ is called the \textbf{discriminant} of a polynomial $f$, whose foots are $\alpha_1,...,\alpha_n$. 
\end{Def}

\begin{Def}
\textbf{Elementary symmetric polynomials}: 
\bee
s_1 &= x_1 + x_2 + \cdots + x_n\\
s_2 &= x_1x_2 + x_1x_3 + x_2x_3 + \cdots + x_{n - 1}x_n\\
s_3 &= \sum_{i < j < k}x_ix_jx_k,\\
& \vdots\\
s_n &= x_1x_2\cdots x_n. 
\eee
\end{Def}


Let $E/F$ be an extension, $K/F$ be a finite subextension ($F\sub K \sub E$). 

\begin{Def}
\textbf{Embeddings} of $K$ to $E$ over $F$:
homomorphisms $\phi:K \to E$ such that $\phi|_F = \text{Id}_F$. It may be that $\phi(K) = K$, but $\phi$ is non-trivial: $\phi \neq \text{Id}_K$. 
\end{Def}

\begin{Def}
If $\phi$ is such an embedding of $K$, then $\phi(K)$ is called a \textbf{conjugate} of $K$. 
\end{Def}

\begin{Def}
If $K/F$ is an extension, then $K$ is an $F$-vector space. $\dim_FK$ is called the \textbf{degree of $K$ over $F$}, $\deg_FK = [K:F]$. It may be finite or infinite. 
\end{Def}

\begin{Def}
If $\deg_FK < \infty$, then $K/F$ is a \textbf{finite extension}. 
\end{Def}

\begin{Def}
If $\deg_FK = \infty$, then $K/F$ is an \textbf{infinite extension}. 
\end{Def}

\begin{Def}
The mapping $\phi:F \to F$, $\phi(a) = a^p$, is called the \textbf{Frobenius endomorphism} of $F$. It is a homomorphism: $\forall a,b, (ab)^p = a^pb^p$. And:
\bee
(a = b)^p &= a^p + pa^{p - 1}b + \binom{p}{2}a^{p - 2}b^2 + \cdots + pab^{p - 1} + b^p\\
&= a^p + b^p,
\eee
since all the middle terms go to zero. 
\end{Def}

\begin{Def}
If it is surjective, it is called the \textbf{Frobenius automorphism} of $F$. 
\end{Def}

\begin{Def}
Let $K/F$ be finite, $[K:F] = n$. $K/F$ is \textbf{Galois} if:
\begin{enumerate}
\item it is normal and separable. 
\item $|\text{Aut}(K/F)| = n$.
\item $K = F(\alpha_1,...,\alpha_k)$ s.t. $\alpha_i$ are separable and all their conjugates are in $K$.
\item $K$ is a splitting field of a separable polynomial.
\end{enumerate}
So we have 4 equivalent definitions. 
\end{Def}


\begin{Def}
If $L/F$ is finite and separable, let $L = F(\alpha_1,...\alpha_k)$. Adjoin all conjugates of $\alpha_i$, they are still separable. Then we get an extension $K$, generated by separable elements whose conjugates are in $K$, so $K$ is Galois (the minimal Galois extension of $F$ containing $K$). $K/F$ is called the \textbf{Galois closure} of $L/F$. (It is the normal closure of $L/F$). 
\end{Def}

\begin{rem}
If $K = F(\alpha)$, then the Galois closure of $K$ is the splitting field of $m_{\alpha,F}$. 
\end{rem}

\begin{proof}
Note that $K$ itself is not necessarily the splitting field of $f$ because the conjugates of $\alpha$ may not be in $K$. It is the normality of the Galois closure $E$ (the fact that $E$ contains all conjugates of $\alpha$) which gives us that $f$ must decompose into linear factors. Otherwise we could have some roots of $F$ but not all (think imaginary). 
\end{proof}

\begin{Def}
Recall that an element $\alpha$ is \textbf{separable} if and only if it is a root of a separable polynomial. 
\end{Def}

\begin{Def}
Let $f \in F[x]$. An extension $K/F$ is a \textbf{splitting field of $f$ }if in $K$, $f$ splits ``completely": $f = f_1\cdots f_k$, where $f_i$ are linear, so $f = c(x - \alpha_1)\cdots(x - \alpha_k)$, and $K$ is the minimal field with this property. 
\end{Def}

\begin{Def}
Consider the group of permutations of $\alpha_1,...,\alpha_n$ that preserves all relations between $\alpha_1,...,\alpha_n$. This is the group of automorphisms Aut$(R/\Q) = \text{Aut}(F)$ and is called the \textbf{Galois group} of $F$, Gal$(F) = \text{Gal}(F/\Q)$. 
\end{Def}

\begin{rem}
Any element $\sigma \in \gal(K)$ is uniquely defined by its action on $\Set{\alpha_i}$, the roots of the polynomial of which $K$ is the splitting field. 
\end{rem}

\begin{Def} 
$K/F$ is said to be \textbf{normal} if:
\begin{enumerate}
\item it is algebraic and $\forall \alpha \in K$, $m_{\alpha,F}$ splits completely over $K$.
\item Or: if an irreducible polynomial over $F$ has a root in $K$, then it splits over $K$. 
\item Or: $\forall \alpha \in K$, \textbf{all conjugates of $\alpha$ are in $K$}. This is to say, if you take an element $\alpha \in K$, all its conjugates in any \textit{larger} field are in $K$. 
\end{enumerate}  
\end{Def} 

\begin{theorem}[\textbf{On the primitive element}]
 If $K/F$ is finite and separable, then there is $\alpha \in K$ such that $K = \F(\alpha)$. ($\alpha$ is called ``primitive" for K.)
 \end{theorem}
 
 \begin{Def}
An extension $K/F$ is \textbf{separable} if and only if:
\begin{enumerate}
\item the minimal polynomial over $F$ of every element is separable. 
\item every element of $K$ is the root of a separable polynomial over $F$.
\end{enumerate} 
\end{Def}

\begin{theorem}[\textbf{Galois theorem - full version}]
Let $K/F$ be a Galois extension, let $g = \gal(K/F)$. Then \begin{enumerate}
\item The correspondence: subextension $L/F \leftrightarrow$ subgroup $H \leq G$ defined by $H = \gal(K/L)$, $L = \fix(H)$ is injective with  $|H| = [K:L],|G:H| = [L:F]$. We postpone the proof until Monday. The idea is to prove that if we define subextension this way, then the degree of $K/L$ will be exactly the order of $H$, and this is the key point of the proof. 
\item If $L_1 \leftrightarrow H_1, L_2 \leftrightarrow H_2$, then $L_1 \sub L_2$ if and only if $H_1 \geq H_2$ and $[L_2:L_1] = |H_1:H_2|$. So there exists only finitely many subextensions of $K/F$, and the diagram of subextensions is the same as the diagram of subgroups of $G$ drawn \textbf{upside down}:
\begin{center}
\begin{tikzcd}
 & 1 \arrow[ld, "n_1"', no head] \arrow[d, "n_2"', no head] \arrow[rd, "n_4", no head] &  &  &  & K \arrow[ld, "n_1"', no head] \arrow[d, "n_2", no head] \arrow[rd, "n_4", no head] &  \\
H_1 \arrow[rd, "m"', no head] & H_2 \arrow[d, no head] & H_4 \arrow[ldd, no head] & \leftrightarrow & L_2 \arrow[rd, "m"', no head] & L_2 \arrow[d, no head] & L_4 \arrow[ldd, no head] \\
 & H_3 \arrow[d, no head] &  &  &  & L_3 \arrow[d, no head] &  \\
 & G &  &  &  & F & 
\end{tikzcd}.
\end{center}
\item If $L_1 \leftrightarrow H_1, L_2 \leftrightarrow H_2$, then $L_1 \cap L_2 \leftrightarrow \langle H_1,H_2 \rangle$ and $L_1L_2\leftrightarrow H_1 \cap H_2$. And the following diagram is the proof:
\begin{center}
\begin{tikzcd}
 & L_1L_2 \arrow[rrr] \arrow[ld, no head] \arrow[rd, no head] &  &  & H_1 \cap H_2 \arrow[lll] \arrow[ld, no head] \arrow[rd, no head] &  \\
L_1 \arrow[rd, no head] &  & L_2 \arrow[ld, no head] & H_1 \arrow[rd, no head] &  & H_2 \arrow[ld, no head] \\
 & L_1\cap L_2 &  &  & \langle H_1,H_2 \rangle & 
\end{tikzcd}.
\end{center}
Since $L_1 \cap L_2$ is the max subfield contained in $L_1$ and $L_2$, it corresponds to the minimum subgroup countaining $H_1$ and $H_2$, which is $\langle H_1, H_2 \rangle$. And...

\item If $L \leftrightarrow H$, then any embedding $L \leftrightarrow L$ is given by some $\phi \in G$. $\phi_1,\phi_2 \in G$ define the same embedding $\phi|_{L} = \phi_2|_L$ if and only if $\phi_1 = \phi_2 \mod H$. So embeddings $\leftrightarrow$ cosets $G/H$. 

 \item Any conjugate of $L$ (result of an embedding) is of the form $\phi(L)$, $\phi \in G$. The subgroup, corresponding to $\phi(L)$ is $\phi H \phi^{-1}$. So conjugate subextensions $\leftrightarrow$ conjugate subgroups:
$$
\phi(L) \leftrightarrow \phi H\phi^{-1}.
$$

\item If $L \leftrightarrow H$, then $L$ is normal if and only if $H \norm G$. In this case, $L/F$ is Galois, and $\gal(L/F) = G/H$. If $L$ is normal, then the extension from $F$ to $L$ is normal. 
\end{enumerate}
\end{theorem}






\backmatter
%    Bibliography styles amsplain or harvard are also acceptable.
\bibliographystyle{amsalpha}







\backmatter
%    Bibliography styles amsplain or harvard are also acceptable.
\bibliographystyle{amsalpha}
\bibliography{}
%    See note above about multiple indexes.
\printindex

\end{document}

%-----------------------------------------------------------------------
% End of amsbook-template.tex
%-----------------------------------------------------------------------
