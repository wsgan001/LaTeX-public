
%	options include 12pt or 11pt or 10pt
%	classes include article, report, book, letter, thesis

\title{\Huge Brendan Whitaker}



\author{\huge CSE 2221 Homework 17 \linebreak \\\\ \Large Professor Bucci}

\date{5 April 2017}
\documentclass[10pt]{article}



\usepackage{amsmath,amsthm,amssymb}
\usepackage{listings}

\usepackage[margin=0.5in]{geometry}


\begin{document}
\maketitle


\begin{center}{\bf STAT 4202 Final Cheat Sheet}\end{center}
A statistic $\hat\theta$ is {\bf unbiased estimator} of $\theta$ if and only if $E(\hat\theta)=\theta$.\\
$\hat\theta$ is an unbias estim of $\theta$ and $Var(\hat\theta)=\Big[n E\Big[\Big(\frac{\partial\ln f(X)}{\partial \theta}\Big)^2\Big]\Big]^{-1}$ then $\hat\theta $ is {\bf min var. unbias estim}.\\
$\hat\theta$ {\bf consistent estimator} if and only if $\lim_{n\to\infty}\mathbb P(|\hat\theta-\theta|<c)=1$.\\
$\hat\theta$ unbiased (asymptotically) estimator and $Var(\hat\theta)\to0$ as $n\to\infty$ then $\hat\theta$ is {\bf consistent}.\\
$\hat\theta$ consistent but $Var(\hat\theta)$ doesn't $\to0$ is \\
$\hat\theta$ is {\bf sufficient }iff the joint probability can be factored to $f(x_i,\theta)=g(\hat\theta,\theta)h(x_i)$.\\
If $U_1=h(U_2)$ and both sufficient, then its false that $Var(\hat\theta_1)\geq Var(\hat\theta_2)$.\\
$k^{th}$ {\bf sample moment} $m'_k=\frac{1}{n}\sum_i x_i^k$.\\
{\bf Method of Moments} $\mu_1'=m_1',\ \mu_2'=m_2'\implies \mu_1'=E(X)=\bar x$, $\mu_2'=E(X^2)=\frac{1}{n}\sum x_i^2$. Solve for $m_i's$.\\
{\bf MLE} $L(\theta)=f(x_i)$, $\partial_{\theta}\ln L(\theta)=0$ and solve for $\theta$. Check $\partial^2_{\theta}<0$. Binom MLE $\hat\theta=X/n$. Unif is $n$th order stat. lots are $\bar X$. If two variables, check $J>0$.\\
{\bf Estimation of Mean} known $\sigma$, then $\bar X\pm z_{\alpha/2}\frac{\sigma}{\sqrt n}$ is $1-\alpha$ confidence estimate for $\mu$.\\
If from a normal pop., then $\bar X\pm t_{\alpha/2,n-1}\frac{s}{\sqrt n}$ is $1-\alpha$ confidence.\\
{\bf Difference of Means} Known $\sigma_1,\sigma_2$, $(\bar x_1-\bar x_2)\pm z_{\alpha/2}\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}$ is $1-\alpha$ confidence.\\
If from normal,small samps assume $\sigma_1=\sigma_2=\sigma$, then $(\bar x_1-\bar x_2)\pm t_{\alpha/2,n_1+n_2-2}\sqrt{\frac{(n_1-1)S_1^2+(n_2-1)S_2^2}{n_1+n_2-2}}\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}$.\\
{\bf Estimation of Props} $X\sim Binom$ and $n$ large, $\frac{X}{n}\pm z_{\alpha/2}\sqrt{\frac{\frac{X}{n}(1-\frac{X}{n})}{n}}$ is $1-\alpha$.\\
{\bf Difference of Props} $X_1,X_2 Binom$ and $n_1,n_2$ large, $\frac{x_1}{n_1}-\frac{x_2}{n_2}\pm z_{\alpha/2}\sqrt{\frac{\frac{x_1}{n_1}(1-\frac{x_1}{n_1})}{n_1}+\frac{\frac{x_2}{n_2}(1-\frac{x_2}{n_2})}{n_2}}$ is $1-\alpha$.\\ {\bf Estimation of Variance} $s^2$ from normal then $\frac{(n-1)s^2}{\chi^2_{\alpha/2,n-1}}<\sigma^2<\frac{(n-1)s^2}{\chi^2_{1-\alpha/2,n-1}}$ is $1-\alpha$.\\
{\bf Ratio of Var} $s_1^2,s_2^2$ from normal then $\frac{s_1^2}{s_2^2}\frac{1}{f_{\alpha/2,n_1-1,n_2-1}}<\frac{\sigma_1^2}{\sigma_2^2}<\frac{s_1^2}{s_2^2}f_{\alpha/2,n_2-1,n_1-1}$ is $1-\alpha$.\\
$\alpha=$ {\bf Type I Error} $=\mathbb P($reject $H_0| H_0$ true). $\beta=$ {\bf Type II Error} $=\mathbb P($accept $H_0| H_0$ false).\\
{\bf Neyman Pearson Lemma} Testing $H_0:\theta=\theta_0$ vs. $H_1:\theta=\theta_1$, this test provides the most powerful critical region.
$L_0=\prod f(x_i,\theta_0),\ L_1=\prod f(x_i,\theta_1)$ Reject $H_0$ if $L_0/L_1\leq k$ inside crit.\\
{\bf LRT} Test $H_0:\theta=\theta_0$ vs. $H_1:\theta\neq\theta_0$. max $L_0=\prod f(x_i, MLE\ \theta_0)$ and max $L_1=\prod f(x_i,\theta_{MLE})$. $\Lambda=$max $L_0$/ max $L_1$, $\Lambda \leq k$ crit region. Large $n$, $-2\ln\Lambda\sim\chi^2_1$.\\
p-value $p=\mathbb P($as extreme or more extreme observation under $H_0)$. Reject $H_0$ if $p<\alpha$.\\
Remember, larger samples make it easier to reject $H_0$.\\
{\bf Tests for Mean} $H_0:\mu=\mu_0$ vs. $H_1:\mu\neq\mu_0,\mu>\mu_0,\mu<\mu_0$. Reject $H_0$ if $|Z|>z_{\alpha/2},Z>z_{\alpha}, z<-z_{\alpha}$ with $Z=\frac{\bar x-\mu_0}{\sigma/\sqrt n}$. {\bf t test} If $\sigma$ unknown and normal pop, use $t_{n-1}$ and $s$.\\
{\bf Difference of Means} Test $H_0:\mu_1-\mu_2=\delta$ vs. $H_1: \mu_1-\mu_2\neq\delta,>\delta,<\delta$. Reject $H_0$ if $|Z|>z_{\alpha/2},Z>z_{\alpha},Z<-z_{\alpha}$ with $Z=\frac{\bar x_1-\bar x_2-\delta}{\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}}$. Can use $t_{n_1+n_2-2}$ and $s^2$ as well.\\
If we have paired data, subtract one from the other for "new" data set and do one-sample test.\\
{\bf Tests for Variance} Test $H_0:\sigma^2=\sigma_0^2$ vs. $H_1:\sigma\neq,>,<\sigma_0^2$. Reject $H_0$ if $\chi^2>\chi^2_{\alpha/2,n-1}$ or $\chi^2<\chi^2_{1-\alpha/2,n-1},\chi^2>\chi^2_{\alpha,n-1},\chi^2<\chi^2_{1-\alpha,n-1}$ where $\chi^2=(n-1)s^2/\sigma_0^2$.\\
{\bf Two samples} Test $H_0:\sigma^2_1=\sigma^2_2$ vs $H_1:\sigma_1^2\neq,>,<\sigma_2^2$. Reject $H_0$ if $F>f_{\alpha/2,n_1-1,n_2-1}$ or $F<f_{\alpha/2,n_1-2,n_1-1}, F>f_{\alpha,n_1-1,n_2-1}, F<f_{\alpha/2,n_2-1,n_1-1}$ where $F=s_1^2/s_2^2$.\\
{\bf Test for Props} Using Binom under $H_0$, find p-vlaue. If $n$ is large $Z=\frac{x-n\theta_0}{\sqrt{n\theta_0(1-\theta_0)}}$ is standard normal.\\
{\bf Diff of k props} $\chi^2=\sum_1^k\frac{(x_i-n_i\theta_i)^2}{n_i\theta_i(1-\theta_i)}$ is test stat. Reject $H_0$ that all props are equal in favor of $H_1$ that they aren't all equal if $\chi^2>\chi^2_{\alpha,k}$. Using $\hat\theta=\sum x_i/\sum n_i$ and reduce df by 1.\\
{\bf r x c table RIP}
\begin{tabular}{|c| c| c| c|| c}\hline
&Outcome 1&Outcome 2&Outcome 3\\\hline
Sample 1&$f_{11}$&$f_{12}$&$f_{13}$&$f_{1\cdot}$\\\hline
Sample 2&$f_{21}$&$f_{22}$&$f_{23}$&$f_{2\cdot}$\\\hline
Sample 3&$f_{31}$&$f_{32}$&$f_{33}$&$f_{3\cdot}$\\\hline\hline
&$f_{\cdot 1}$&$f_{\cdot 2}$&$f_{\cdot 3}$&$f$\\\hline
\end{tabular}\\
Test $H_0$ Samples are independent vs. $H_1$ not independent. Reject $H_0$ if $\chi^2=\sum_1^3\sum_1^3\frac{(f_{ij}-e_{ij})^2}{e_{ij}}>\chi^2_{(r-1)(c-1)}$ where $e_{ij}=f_{i\cdot}f_{\cdot j}/f$.\\
{\bf Goodness of Fit} Test whether data comes from dist. Reject if $\chi^2>\sum \frac{(f_i-e_i)^2}{e_i}>\chi^2_{\alpha,m-t-1}$ where $f_i$ observed and $e_i$ expected. Combine columns until expected value $>5$ and use $\hat\theta$ MLE if not given.\\
{\bf Sign Test} Test $\mu=\mu_0$ vs something else. In observed data, values $<\mu_0$ get a $+$ and values $<\mu_0$ get a $-$. Under $H_0$, $X=$ number of $+$ is $Binom(\theta=.5)$ so use p-value.\\
{\bf Wilcoxon Signed Ranked Test}  One sample. ASSUME symmetric about $\mu$. Rank differences $x_i-\mu_0$ irregardless of sign. $T^+$ is sum of ranks for positive diff, $T^-$ similar. $T$ is min of two. Test $H_0:\mu=\mu_0$ vs. $H_1:\mu\neq,>,<\mu_0$ and reject $H_0$ if $T<T_{\alpha},T^-<T_{2\alpha},T^+<T_{2\alpha}$.\\
$\mu_{T^+}=\frac{1}{4}(n)(n+1)$ and $Var(T^+)=\frac{1}{24}(n(n+1)(2n+1))$. Can see that $Z=\frac{T^+-\mu}{\sigma}\approx  N(0,1)$.\\
{\bf Ranked Sum Test, U, Wilcoxon Test} Two Sample. ASSUME cont. pop. Rank all observations from 1 to $n_1+n_2$. Compute $W_1=$ sum of ranks from sample 1, $W_2$ from sample 2. $U_1=W_1-\frac{1}{2}(n_1)(n_1+1)$ and $U_2=W_2-\frac{1}{2}(n_2)(n_2+1)$ and $U$ is min. $U_1+U_2=n_1n_2$.\\ 
Test $H_0:\mu_1=\mu_2$ vs $H_1:\mu_1\neq,>,<\mu_2$. Reject $H_0$ if $U<U_{\alpha},U_2<U_{2\alpha},U_1<U_{2\alpha}$. \\
$\mu_{U_1}=\frac{1}{2}n_1n_2$ and $Var(U_1)=\frac{1}{12}n_1n_2(n_1+n_2+1)$. Then for large $n$, $Z=\frac{U_1-\mu}{\sigma}\approx N(0,1)$.\\
{\bf H Test} We have $k$ samples. Rank all data. Test $H_0$ all $\mu_i$ equal vs. $H_1$ not all equal. Reject $H_0$ if $H>\chi^2_{k-1}$ where $H=\frac{12}{n(n+1)}\sum_1^k\frac{R_i^2}{n_i}-3(n+1)$ where $R_i$ is the sum of ranks.\\
$\mu_{Y|X=x}=E(Y|X=x)=\int y\cdot w(y|X=x)\ dy$ where $w(y|X=x)$ is conditional distribution of $Y$ given $X=x$. Integrate out $x$ for marginal then divide $f(x,y)/h(x)$.\\
If $\mu_{Y|X=x}$ is linear in $x$ then $\mu_{Y|X=x}=\mu_2+\rho\frac{\sigma_2}{\sigma_1}(x-\mu_1)$ where $\mu_1=E(X),\mu_2=E(Y),\sigma_1^2=Var(X),\sigma_2^2=Var(Y), \rho=\frac{Cov(X,Y)}{\sigma_1\sigma_2}$.\\
{\bf Least Squares} Estimate $\hat y=\hat\alpha+\hat\beta x$ are $\hat\alpha=\bar y-\hat\beta\bar x$ and $\hat\beta=S_{xy}/S_{xx}$ were $S_{xy}=\sum x_iy_i-\frac{1}{n}\sum x_i\sum y_i$ and $s_{xx}=\sum x_i^2-\frac{1}{n}(\sum x_i)^2$.\\
{\bf Normal Regression Analysis} $n\hat\sigma^2/\sigma^2\sim\chi^2_{n-2}$. $t=\frac{\hat\beta-\beta}{\hat\sigma}\sqrt{\frac{(n-2)S_{xx}}{n}}\sim t_{n-2}$ with $\hat\beta$ as above. $\hat\sigma^2=\frac{1}{n}(S_{yy}-\hat\beta S_{xy})$ and we want to estimate $\beta$. $\hat\beta\pm t_{\alpha/2,n-2}\hat\sigma\sqrt{\frac{n}{(n-2)S_{xx}}}$ is $1-\alpha$.\\
{\bf Normal Correlation Analysis} If $X,Y$ bivariate normal, $\hat\mu_1=\bar x,\hat\mu_2=\bar y,\hat\sigma_1^2=\frac{1}{n}\sum (x_i-\bar x)^2,\hat\rho=\frac{s_{xy}}{\sqrt{s_{xx}s_{yy}}}$. $\rho$ measures strength of linear relationship. $r=\frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}$.\\
\begin{equation}\notag Z=\frac{\frac{1}{2}\ln\Big(\frac{1+R}{1-R}\Big)-\frac{1}{2}\ln\Big(\frac{1+\rho}{1-\rho}\Big)}{\sqrt{\frac{1}{n-3}}}\sim N(0,1)\end{equation}
Under $H_0$, we will have a value of $\rho$. $r=\hat\rho$.












\end{document}